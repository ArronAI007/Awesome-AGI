{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "giM74oK1rRIH",
        "outputId": "25dca1f4-83d7-45a9-81b1-87508f90281d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 15787, done.\u001b[K\n",
            "remote: Counting objects: 100% (6817/6817), done.\u001b[K\n",
            "remote: Compressing objects: 100% (494/494), done.\u001b[K\n",
            "remote: Total 15787 (delta 6453), reused 6363 (delta 6323), pack-reused 8970\u001b[K\n",
            "Receiving objects: 100% (15787/15787), 222.13 MiB | 16.88 MiB/s, done.\n",
            "Resolving deltas: 100% (11644/11644), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (4.42.4)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.20.0)\n",
            "Requirement already satisfied: accelerate>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.32.1)\n",
            "Requirement already satisfied: peft>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.11.1)\n",
            "Requirement already satisfied: trl>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.9.6)\n",
            "Requirement already satisfied: gradio>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (4.38.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.11.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.1.99)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.20.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.30.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.111.1)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.1.2)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (6.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.3.1+cu121)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.43.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.9.5)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (5.3.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.1.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.27.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.10.6)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (9.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.0.9)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.5.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.0.7)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (11.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.8.4.dev0) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.4.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.4.dev0) (0.19.1)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl>=0.8.6->llamafactory==0.8.4.dev0) (0.8.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.8.4.dev0) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.8.4.dev0) (0.0.4)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.8.4.dev0) (2.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (2.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.8.4.dev0) (2.6.1)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.8.4.dev0) (3.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->llamafactory==0.8.4.dev0) (1.2.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.4.dev0) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.4.dev0) (1.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (0.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.8.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.19.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.1.2)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.8.4.dev0-0.editable-py3-none-any.whl size=20620 sha256=716f4b01d1e77f067f74c9544beb24891dff3362f8b7588fe0b87e74995a1885\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3e95_abx/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.8.4.dev0\n",
            "    Uninstalling llamafactory-0.8.4.dev0:\n",
            "      Successfully uninstalled llamafactory-0.8.4.dev0\n",
            "Successfully installed llamafactory-0.8.4.dev0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/LLaMA-Factory/\n",
        "\n",
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "mO3HDHF7qx1u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU environment"
      ],
      "metadata": {
        "id": "H9RXn_YQnn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Identity Dataset"
      ],
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "ap_fvMBsQHJc",
        "outputId": "951d3153-8f62-4d6e-bc29-bd88b67b6730",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy",
        "outputId": "1d57ffd2-3f05-44e7-831a-5d7766c8add5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-07-22 04:55:35.692669: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-22 04:55:35.692728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-22 04:55:35.694406: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-22 04:55:35.702057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-22 04:55:38.038223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Exception ignored in: <function _xla_gc_callback at 0x7d1c79299120>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://a80977186f1bb458d1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo\",             # use alpaca and identity datasets\n",
        "  template=\"llama3\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=3.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  quantization_bit=4,                     # use 4-bit QLoRA\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q",
        "outputId": "2aed971c-f89a-4154-b6e6-a65b29cf43d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-07-22 04:55:59.397469: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-22 04:55:59.397514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-22 04:55:59.398810: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-22 04:55:59.408409: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-22 04:56:00.585561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/22/2024 04:56:07 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/22/2024 04:56:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-22 04:56:07,705 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-22 04:56:07,706 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-22 04:56:07,706 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-22 04:56:07,706 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-22 04:56:08,338 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/22/2024 04:56:08 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "07/22/2024 04:56:08 - INFO - llamafactory.data.loader - Loading dataset identity.json...\n",
            "Generating train split: 91 examples [00:00, 4670.32 examples/s]\n",
            "Converting format of dataset: 100% 91/91 [00:00<00:00, 7466.68 examples/s]\n",
            "07/22/2024 04:56:08 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...\n",
            "Generating train split: 1000 examples [00:00, 62690.44 examples/s]\n",
            "Converting format of dataset: 100% 500/500 [00:00<00:00, 30003.89 examples/s]\n",
            "Running tokenizer on dataset: 100% 591/591 [00:00<00:00, 1671.44 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-22 04:56:09,608 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-22 04:56:09,609 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "07/22/2024 04:56:09 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
            "07/22/2024 04:56:09 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[WARNING|quantization_config.py:397] 2024-07-22 04:56:09,744 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-22 04:56:09,745 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-22 04:56:09,786 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-22 04:56:09,790 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-22 04:56:34,121 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-22 04:56:34,121 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-22 04:56:34,224 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-22 04:56:34,224 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "07/22/2024 04:56:34 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/22/2024 04:56:34 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/22/2024 04:56:34 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/22/2024 04:56:34 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/22/2024 04:56:34 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,down_proj,v_proj,up_proj,q_proj,o_proj,gate_proj\n",
            "07/22/2024 04:56:34 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:642] 2024-07-22 04:56:34,969 >> Using auto half precision backend\n",
            "07/22/2024 04:56:35 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2128] 2024-07-22 04:56:35,458 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-22 04:56:35,458 >>   Num examples = 591\n",
            "[INFO|trainer.py:2130] 2024-07-22 04:56:35,458 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-22 04:56:35,458 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2134] 2024-07-22 04:56:35,458 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2135] 2024-07-22 04:56:35,459 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2136] 2024-07-22 04:56:35,459 >>   Total optimization steps = 222\n",
            "[INFO|trainer.py:2137] 2024-07-22 04:56:35,463 >>   Number of trainable parameters = 20,971,520\n",
            "{'loss': 1.3718, 'grad_norm': 0.7028204202651978, 'learning_rate': 2.173913043478261e-05, 'epoch': 0.14}\n",
            "{'loss': 1.2243, 'grad_norm': 1.9944065809249878, 'learning_rate': 4.347826086956522e-05, 'epoch': 0.27}\n",
            "{'loss': 1.0245, 'grad_norm': 0.7271386384963989, 'learning_rate': 4.98475042744222e-05, 'epoch': 0.41}\n",
            "{'loss': 1.0828, 'grad_norm': 1.1125611066818237, 'learning_rate': 4.910506156279029e-05, 'epoch': 0.54}\n",
            "{'loss': 1.0133, 'grad_norm': 1.367089867591858, 'learning_rate': 4.7763104379936555e-05, 'epoch': 0.68}\n",
            "{'loss': 1.0057, 'grad_norm': 0.9770417809486389, 'learning_rate': 4.585500840294794e-05, 'epoch': 0.81}\n",
            "{'loss': 1.0705, 'grad_norm': 1.1586514711380005, 'learning_rate': 4.342822968779448e-05, 'epoch': 0.95}\n",
            "{'loss': 0.8445, 'grad_norm': 2.027451515197754, 'learning_rate': 4.054312439471239e-05, 'epoch': 1.08}\n",
            "{'loss': 0.7177, 'grad_norm': 0.48213788866996765, 'learning_rate': 3.727144767643984e-05, 'epoch': 1.22}\n",
            "{'loss': 0.795, 'grad_norm': 1.0213286876678467, 'learning_rate': 3.369456906329956e-05, 'epoch': 1.35}\n",
            "{'loss': 0.8143, 'grad_norm': 1.6476799249649048, 'learning_rate': 2.990144873009946e-05, 'epoch': 1.49}\n",
            "{'loss': 0.7006, 'grad_norm': 0.7878261804580688, 'learning_rate': 2.5986424976906322e-05, 'epoch': 1.62}\n",
            "{'loss': 0.6909, 'grad_norm': 1.4791374206542969, 'learning_rate': 2.2046867951027303e-05, 'epoch': 1.76}\n",
            "{'loss': 0.7302, 'grad_norm': 0.5592633485794067, 'learning_rate': 1.8180757964234924e-05, 'epoch': 1.89}\n",
            "{'loss': 0.6043, 'grad_norm': 0.7901464104652405, 'learning_rate': 1.4484248634655401e-05, 'epoch': 2.03}\n",
            "{'loss': 0.5634, 'grad_norm': 1.0272388458251953, 'learning_rate': 1.1049275460163999e-05, 'epoch': 2.16}\n",
            "{'loss': 0.5479, 'grad_norm': 0.6934621334075928, 'learning_rate': 7.961269300209159e-06, 'epoch': 2.3}\n",
            "{'loss': 0.5527, 'grad_norm': 1.0163666009902954, 'learning_rate': 5.297031633820193e-06, 'epoch': 2.43}\n",
            "{'loss': 0.5397, 'grad_norm': 1.2155324220657349, 'learning_rate': 3.1228244380351602e-06, 'epoch': 2.57}\n",
            "{'loss': 0.4364, 'grad_norm': 0.6431947946548462, 'learning_rate': 1.4927221931831131e-06, 'epoch': 2.7}\n",
            "{'loss': 0.4934, 'grad_norm': 0.8815789818763733, 'learning_rate': 4.472670021254899e-07, 'epoch': 2.84}\n",
            "{'loss': 0.5198, 'grad_norm': 1.0776289701461792, 'learning_rate': 1.2460271845654569e-08, 'epoch': 2.97}\n",
            "100% 222/222 [33:12<00:00,  8.17s/it][INFO|trainer.py:3478] 2024-07-22 05:29:47,923 >> Saving model checkpoint to llama3_lora/checkpoint-222\n",
            "[INFO|configuration_utils.py:733] 2024-07-22 05:29:48,192 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-22 05:29:48,193 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-22 05:29:48,403 >> tokenizer config file saved in llama3_lora/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-22 05:29:48,403 >> Special tokens file saved in llama3_lora/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-22 05:29:49,371 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1993.9081, 'train_samples_per_second': 0.889, 'train_steps_per_second': 0.111, 'train_loss': 0.7851412478867952, 'epoch': 3.0}\n",
            "100% 222/222 [33:13<00:00,  8.98s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-22 05:29:49,374 >> Saving model checkpoint to llama3_lora\n",
            "[INFO|configuration_utils.py:733] 2024-07-22 05:29:49,619 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-22 05:29:49,620 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-22 05:29:50,001 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-22 05:29:50,001 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 16222836GF\n",
            "  train_loss               =     0.7851\n",
            "  train_runtime            = 0:33:13.90\n",
            "  train_samples_per_second =      0.889\n",
            "  train_steps_per_second   =      0.111\n",
            "[INFO|modelcard.py:449] 2024-07-22 05:29:50,185 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9",
        "outputId": "787663f3-fb63-4804-f373-108fc7d17620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-22 05:30:04,680 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-22 05:30:04,682 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-22 05:30:04,684 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-22 05:30:04,687 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-22 05:30:05,135 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/22/2024 05:30:05 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.data.template:Replace eos token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-22 05:30:05,242 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-22 05:30:05,245 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/22/2024 05:30:05 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llamafactory.model.model_utils.quantization:`quantization_bit` will not affect on the PTQ-quantized models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/22/2024 05:30:05 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.model_utils.quantization:Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/22/2024 05:30:05 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.patcher:Using KV cache for faster generation.\n",
            "[WARNING|quantization_config.py:397] 2024-07-22 05:30:05,395 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-22 05:30:05,399 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-22 05:30:05,445 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-22 05:30:05,453 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "[INFO|quantizer_bnb_4bit.py:106] 2024-07-22 05:30:05,854 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "[INFO|modeling_utils.py:4364] 2024-07-22 05:30:09,550 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-22 05:30:09,551 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-22 05:30:09,660 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/e11416454f0acc5f2fc66f75c209b7786eaeb956/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-22 05:30:09,662 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/22/2024 05:30:09 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.model_utils.attention:Using torch SDPA for faster training and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/22/2024 05:30:10 - INFO - llamafactory.model.adapter - Loaded adapter(s): llama3_lora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.adapter:Loaded adapter(s): llama3_lora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/22/2024 05:30:10 - INFO - llamafactory.model.loader - all params: 8,051,232,768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.loader:all params: 8,051,232,768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
            "\n",
            "User: who are you\n",
            "Assistant: I am Llama-3, an AI assistant developed by LLaMA Factory.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-02babdfbf1d2>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nUser: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "kTESHaFvbNTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mcNcHcA4bf4Z",
        "outputId": "ee2777fb-b92a-47c8-f947-5719933be3c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "IMojogHbaOZF",
        "outputId": "e138e43c-e886-4adc-c1c4-012ac1e3b1ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-07-22 05:55:23.101047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-22 05:55:23.101101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-22 05:55:23.102427: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-22 05:55:24.337762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|tokenization_auto.py:675] 2024-07-22 05:55:31,524 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1823, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n",
            "    raise GatedRepoError(message, response) from e\n",
            "huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-669df453-1d87721c5f8fdd5728ea3cae;29ad08ee-52d7-4a97-ab25-cb9be939ec84)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 87, in main\n",
            "    export_model()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in export_model\n",
            "    tokenizer_module = load_tokenizer(model_args)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 846, in from_pretrained\n",
            "    config = AutoConfig.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 965, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 420, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "401 Client Error. (Request ID: Root=1-669df453-1d87721c5f8fdd5728ea3cae;29ad08ee-52d7-4a97-ab25-cb9be939ec84)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\n"
          ]
        }
      ]
    }
  ]
}