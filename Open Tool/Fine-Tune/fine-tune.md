# AGI_Fine_Tuned_Tools

## 大模型微调开源框架

| Tool | Description | Code | Blog | 
| --- | --- | --- | --- |
| LLaMA-Adapter | [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/pdf/2303.16199.pdf) | [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter) |
| LLaMA Efficient Tuning | 支持Baichuan-7B使用Qlora进行Finetune，支持RLHF，支持WebDemo | [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) | [百川大模型【Baichuan-13B】 多卡训练微调记录](https://mp.weixin.qq.com/s/EUZA6Lt-OcI170md9lXH1g) | 
| Firefly | 中文对话式大语言模型(全量微调+QLoRA)，支持微调Baichuan2、CodeLlama、Llma2、Llama、Qwen、Baichuan、ChatGLM2、InternLM、Ziya、Bloom等大模型 | [Firefly](https://github.com/yangjianxin1/Firefly) | [微调百川Baichuan-13B保姆式教程，手把手教你训练百亿大模型](https://mp.weixin.qq.com/s/ZBY6kbogHjbCQvZBzNEqag) |
| LLM-Tuning | 支持ChatGLM、baichuan大模型的LoRA微调 | [LLM-Tuning](https://github.com/beyondguo/LLM-Tuning) | [使用 HC3 数据集来让 baichuan-7B 有对话能力](https://zhuanlan.zhihu.com/p/640973286) |

---

## 大模型微调技术

| Tool | Description | Code | Paper/Blog | 
| --- | --- | --- | --- |
| GaLore | 碾压LoRA！Meta & CMU | 提出高效大模型微调方法：GaLore，内存可减少63.3% |  | [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/pdf/2403.03507v1.pdf) |
| LoRA |  |  |  |
| DoRA |  |  |  |
| P-Tuning |  |  |  |
| P-Tuning V2 |  |  |  |
| AdapterTuning |  |  |  |
| Prompt Tuning |  |  |  |
