{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install necessary libraries**\n",
    "\n",
    "**Huggingface transformers library** - First thing you need to install. It’s a library that allows you to download, train and fine tune pre-trained models\n",
    "\n",
    "**Dataset** - Library will allow you to load a dataset in JSON, CSV, Parquet, text and other formats\n",
    "\n",
    "**TRL** - Library will allow Supervised training of the model. And if you have a structured dataset, you need to implement this type of training\n",
    "\n",
    "**PEFT** - Parameter-Efficient Fine-tuning techniques fine-tunes a small number of (extra) model parameters or weights while freezing most parameters of the pre trained LLMs. This is important because fine tuning entire LLM would require incredible hardware and it be very energy consuming but with PEFT, you can fine tune a giant LLM on a regular consumer GPU. Lora or Low-Rank Adaptation of Large Language Models is a specific method within the broader category of PEFT techniques. It focuses on freezing the pre-trained model weights\n",
    "\n",
    "**bitsandbytes and accelerate** - libraries are going to be used for quantizing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install xformers\n",
    "!pip install -q datasets\n",
    "!pip install -q trl\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install -q bitsandbytes==0.37.2\n",
    "!pip install -q -U accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, pipeline\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Huggingface’s hub\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "repo_id = \"meta-llama/Llama-2-7b-chat-hf\" # Modify to whatever model you want to use\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=True,   # quantize 8bit\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and preprocess the model for training\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # you have to know the target modules, it varies from model to model\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(base_model, config) # Wrap the base model with get_peft_model() to get a trainable PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset from datasets library\n",
    "dataset = load_dataset(\"csv\", data_files = \"you_data_here.csv\") # substitute with whatever file name you have\n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "adam_bits = 8\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"Trainer_output\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    run_name=f\"deb-v2-xl-{adam_bits}bitAdam\",\n",
    "    logging_steps = 20,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = 300,\n",
    "    warmup_ratio = 0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type = \"constant\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    args = training_arguments,\n",
    "    max_seq_length = 512,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adapter and Merge it with the base model\n",
    "trainer.save_model(\"Finetuned_adapter\")\n",
    "adapter_model = model\n",
    "\n",
    "print(\"Lora Adapter saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One thing to keep in mind is that you can’t merge the 8 bit/4 bit base model with Lora (as of right now) so you have to reload the model with full precision.\n",
    "repo_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "use_ram_optimized_load=False\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# Load Lora adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"/content/Finetuned_adapter\",\n",
    "    )\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Merge the model and adapter\n",
    "merged_model.save_pretrained(\"/content/Merged_model\")\n",
    "tokenizer.save_pretrained(\"/content/Merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the merged model to huggingface’s hub\n",
    "# Replace with your hg id \n",
    "merged_model.push_to_hub(\"your_hg_id/name_fine_tuned_model\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
