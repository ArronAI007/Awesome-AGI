## AGI-Paper-List

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 中文大语言模型汇总：医疗、法律、金融、教育、数学微调， 目前已1.1K星 |  | [code](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM) |  | 
| 大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写 | [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) |  |  | 
| 大语言模型综述全新出炉：51页论文带你盘点LLM领域专业化技术 | [Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey](https://arxiv.org/abs/2305.18703) |  | [blog](https://mp.weixin.qq.com/s/0DrowrTIgXsBhj3sYu6Aog) | 
| AIGC综述: 从GAN到ChatGPT的生成式人工智能简史 | [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/abs/2303.04226v1) |  |  | 
| 大模型综述来了！一文带你理清全球AI巨头的大模型进化史 | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/pdf/2304.13712.pdf) | [code](https://github.com/Mooler0410/LLMsPracticalGuide) |  | 
| 复旦大学教授肖仰华：ChatGPT 浪潮下，面向大模型如何做数据治理？ |  |  | [blog](https://mp.weixin.qq.com/s/od24PYvFLUJe4NQxjvsbMw) | 
| 面向决策的基础模型: 问题、方法与机会 | [Foundation Models for Decision Making: Problems, Methods, and Opportunities](https://arxiv.org/abs/2303.04129) |  |  | 
| 较大语言模型上下文学习的方式有所不同 | [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846) |  |  | 
| 通用语音识别大模型已经支持100+语言 |  |  | [blog](https://mp.weixin.qq.com/s/fHr2vL-w4JtYt5utcZrbsw) | 
| 发布5620亿参数多模态模型PaLM-E，机器人操控再上台阶 | [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) |  | [blog](https://palm-e.github.io/)，[twitter](https://twitter.com/DannyDriess/status/1632904675124035585)，[video](https://mp.weixin.qq.com/s/yZt3sEQPzVjnIvqXsNOnPA) | 
| PanGu-Σ: 稀疏异构计算万亿参数语言模型研究参数语言模型 | [PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing](https://arxiv.org/abs/2303.10845) |  |  | 
| 奖励聊天机器人在现实世界中与数以百万计的用户进行互动 | [Rewarding Chatbots for Real-World Engagement with Millions of Users](https://arxiv.org/pdf/2303.06135.pdf) |  |  | 
| 人工智能系统最终是否需要以现实为基础，而不仅仅是从语言中学习？ |  |  | [blog](https://spectrum.ieee.org/ai-hallucination) | 
| 大型语言模型是否需要感官基础来理解意义和理解？ |  |  | [slices](https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view) | 
| ChatGPT是「外星人」，所以才会胡说八道 | [AI Chatbots Don’t Care About Your Social Norms](https://www.noemamag.com/ai-chatbots-dont-care-about-your-social-norms/?utm_source=noematwitter&utm_medium=noemasocial) |  | [blog](https://twitter.com/ylecun/status/1633459264508542978) | 
| AI聊天机器人并不关注用户的社交属性 |  |  | [blog](https://www.noemamag.com/ai-chatbots-dont-care-about-your-social-norms/?utm_source=noematwitter&utm_medium=noemasocial) | 
| LeCun和马库斯齐喷ChatGPT：大语言模型果然是邪路？ |  |  | [blog](https://mp.weixin.qq.com/s/5e0aTSEAym9rF5QxRndLgQ) | 
| ChatGPT无法实现通用人工智能，但ALM技术路线也许可以 |  |  | [blog](https://mp.weixin.qq.com/s/MEdl3zmiYJU1iFsTXmibng) | 
| 「增强语言模型」的综述 | [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842) |  |  | 
| 自回归LLM的缺陷之一，大语言模型必须知道的8个要点 | [Eight Things to Know about Large Language Models](https://cims.nyu.edu/~sbowman/eightthings.pdf) |  |  | 
| 从词模型到世界模型：从自然语言到思维概率语言的转变 | [From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought](https://arxiv.org/abs/2306.12672) | |  | 
| AI进入2.0时代，所有应用都会被重写一遍 |  |  | [blog](https://mp.weixin.qq.com/s/zV8Y9RQnIoExwa1mmarZmA) | 
| 提出ILF（从语言反馈中模仿学习）：利用语言反馈大规模训练语言模型 | [Training Language Models with Language Feedback at Scale](https://arxiv.org/pdf/2303.16755.pdf) |  |  | 
| GPT就是GPT：大模型对劳动力市场影响潜力的早期研究 | [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/pdf/2303.10130.pdf) |  |  | 
| ABC News 专访OpenAI首席执行官萨姆·奥尔特曼：AI风险和重塑社会的问题 |  |  | [blog](https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122) | 
| 最新发布通用人工智能路线图！AGI比想象中来得更快！ |  |  | [blog](https://openai.com/blog/planning-for-agi-and-beyond/) | 
| Sam Altman 担心“潜在的可怕的”人工智能工具以及“未来的人们如何看待我们” |  |  | [blog](https://finance.yahoo.com/news/openai-ceo-sam-altman-frets-165250285.html) | 
| The Age of AI：拾象大模型及OpenAI投资思考 |  |  | [blog](https://mp.weixin.qq.com/s/AxX-Q7njegNTAxMkYFwsfA) | 
| 为什么ChatGPT用强化学习而非监督学习？ | [Scaling TransNormer to 175 Billion Parameters](https://arxiv.org/abs/2307.14995) | [code](https://github.com/OpenNLPLab/TransnormerLLM) | [blog](https://mp.weixin.qq.com/s/4USDakdomupWuwwhex6fMg) | 
| ChatGPT和生成式AI的11大安全趋势 |  |  | [blog](https://mp.weixin.qq.com/s/_RAx3vAx1ykQTJTEEoc37w) | 
| 分析过688篇大模型论文，这篇论文综述了LLM的当前挑战和应用 |  |  | [blog](https://mp.weixin.qq.com/s/drE6lhuhF9CbuAzDOhswTQ) | 
| 张学工教授：AI技术前沿——从ChatGPT到更多突破 |  |  | [blog](https://mp.weixin.qq.com/s/oeZd52BYKU3hhauZZ0eirQ) | 
| 研究大语言模型反映了谁的观点？ | [Whose Opinions Do Language Models Reflect?](https://arxiv.org/pdf/2303.17548.pdf) | [code](https://github.com/tatsu-lab/opinions_qa) |  | 
| 大模型及其公平使用 | [FOUNDATION MODELS AND FAIR USE](https://arxiv.org/pdf/2303.15715.pdf) |  |  | 
| 构建大模型生态系统图，用于跟踪大模型的足迹 |  |  | [blog](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=home) | 
| 斯坦福报告：基础模型的机遇与风险 |  |  | [blog](https://mp.weixin.qq.com/s/iEwvkqMT7KEqmnHk8NVz6w) | 
| 一种新的大语言模型NLG评估框架 | [G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634) |  |  | 
| 低代码LLM: LLM的可视化编程 | [Low-code LLM: Visual Programming over LLMs](https://arxiv.org/abs/2304.08103) |  |  | 
| 微软提出LLMA:大型语言模型的无损加速,可以无损地加速带有引用的大型语言模型 (LLM) 推理 | [Inference with Reference: Lossless Acceleration of Large Language Models](https://arxiv.org/pdf/2304.04487.pdf) |  |  | 
| ART：大型语言模型的自动多步骤推理和工具使用 | [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/pdf/2303.09014.pdf) |  |  | 
| 提出Pythia： 跨越训练和扩展的大型语言模型分析套件 | [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/pdf/2304.01373.pdf) | [code](https://github.com/EleutherAI/pythia) |  | 
| ChatGPT的底层逻辑 |  |  | [blog](https://mp.weixin.qq.com/s/Rv5htsD2x7TmD-E42RL6Vg) | 
| 智慧信息的压缩：模型智能的涌现之道 |  |  | [blog](https://mp.weixin.qq.com/s/hQmvltuMlClBonM6UJmtLg) | 
| 拨动大模型的琴弦｜Delta Tuning 成果登上 Nature子刊封面！ |  |  | [blog](https://mp.weixin.qq.com/s/m3fNselWKQ2m5XnBe79fQQ) | 
| 大型人工智能模型中出现的不可预测的能力 |  |  | [blog](https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316) | 
| 为什么现在的大语言模型（LLM）都是Decoder-only的架构？ |  |  | [blog](https://mp.weixin.qq.com/s/ZsHX-M9pisUvG9vqfzdzTQ) | 
| 大型语言模型的涌现能力 |  |  | [blog](https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/) | 
| 大型语言模型成本分析 |  |  | [blog](https://hub.baai.ac.cn/view/24047) | 
| 超越ChatGPT：大模型的智能极限 |  |  | [blog](https://yaofu.notion.site/e1cd16d1fae84f87aeddf872c838e07c) | 
| Nature：AI模型越大越好吗? |  |  | [blog](https://www.nature.com/articles/d41586-023-00641-w) | 
| 一场关于ChatGPT话语权的深度思考：人类会在大模型中迷失自我吗？ |  |  | [blog](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html)，[blog译文](https://mp.weixin.qq.com/s/RPiIh5cbxzXl5uMo_BVFMg) | 
| 马斯克强调的TruthGPT 是什么 |  |  | [blog](https://mp.weixin.qq.com/s/_nSYK63DvqE7ZJyJz6NeEA) | 
| 对话式AI搜索的技术路线猜想 |  |  | [blog](https://mp.weixin.qq.com/s/AIIu4rRi1WZRQn3oHtuwdg) | 
| AI走过多少路，才迎来了ChatGPT？ |  |  | [blog](https://mp.weixin.qq.com/s/WWc39HtuV-TrbwFybX112Q) | 
| 如何负责任地创建、发布和共享生成式 AI |  |  | [blog](https://www.technologyreview.com/2023/02/27/1069166/how-to-create-release-and-share-generative-ai-responsibly/) | 
| 大模型时代的“Linux”生态，开启人工智能新十年 |  |  | [blog](https://mp.weixin.qq.com/s/sUmA3nSSVfNQFBgSjiSn0g) | 
| 揭秘ChatGPT背后的AI“梦之队”：90后科研“后浪”展示强大创新能力｜智谱研究报告 |  |  | [blog](https://mp.weixin.qq.com/s/sncE01utzu_-r3dLFYU5QA) | 
| In-Context Learning玩法大全 |  |  | [blog](https://mp.weixin.qq.com/s/sC3Xq1QQmtC8Tz84oRRwcw) | 
| 一文理解“上下文学习”----大语言模型突现能力 |  |  | [blog](https://mp.weixin.qq.com/s/0kchPu20nwCKCXk4PZBkOg) | 
| 回应吴军老师 "ChatGPT不算新技术革命" |  |  | [blog](https://mp.weixin.qq.com/s/dZldwGaYnUcDlB4nUpASMg) | 
| Poe向所有开发者推出Poe API，以便广泛获取基于LLM的服务 |  | [code](https://github.com/poe-platform/api-bot-tutorial) |  | 
| 【LLM系列之底座模型对比】LLaMA、Palm、GLM、BLOOM、GPT模型结构对比 |  |  | [blog](https://mp.weixin.qq.com/s/UkifGP2OXxGWeMV7Jm4zWQ) | 
| 大模型实践总结 |  |  | [blog](https://mp.weixin.qq.com/s/FPweLbvDrCnIzb5PETHMLQ) | 
| 【LLM系列之GPT】GPT（Generative Pre-trained Transformer）生成式预训练模型 |  |  | [blog](https://mp.weixin.qq.com/s/1Bpt5MG6mbZCYAXDJmIr3A) | 
| 【LLM系列之Tokenizer】如何科学地训练一个LLM分词器 |  |  | [blog](https://mp.weixin.qq.com/s/4_P2G2Q0YmunQh7DwDas3w) | 
| 大模型词表扩充必备工具SentencePiece |  |  | [blog](https://mp.weixin.qq.com/s/qQMZ1s7lt-LLkQKx7HIDMw) | 
| 大模型知识&推理评估基准 |  |  | [blog](https://mp.weixin.qq.com/s/P0ohd5DpwJOkL8DFVC4qoA) | 
| 万字长文说清大模型在自动驾驶领域的应用 |  |  | [blog](https://mp.weixin.qq.com/s/5tSwRz-fI4ccLPEn2KrgqA) | 
| 一文速览大语言模型在推荐系统中的应用 |  |  | [blog](https://mp.weixin.qq.com/s/RdRLKjzbTWCATmtRMfxW0Q) | 
| NAACL & ACL：大模型的两种知识继承方案 |  |  | [方案一](https://aclanthology.org/2022.naacl-main.288/)，[方案二](https://aclanthology.org/2022.acl-long.151/) | 
| a16Z：大模型应用程序的新兴架构 |  |  | [中文blog](https://hub.baai.ac.cn/view/27506)，[英文blog](https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/) | 
| 如何优雅下载huggingface大模型文件？ |  |  | [blog](https://mp.weixin.qq.com/s/biNtwJRuWuQxiaklyEWVMg) | 
| LLM 时代指南 |  |  | [blog](https://mp.weixin.qq.com/s/4EvcEzMLfZ3VQTFt7rLA2Q) | 
| RetNet：MSRA提出Transformer全新替代大模型基础架构，推理速度8倍提升，内存占用减少70% | [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621) |  | [blog](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247686895&idx=2&sn=9a2763953d209a29e5d0b03e8b75a912&chksm=e8dead9ddfa9248bea848d16358c5a3eabf11cdd13b5aa96033f3ab2b6dc1ee089bedc73c332&token=1541731120&lang=zh_CN#rd)| 
| 大模型微调指南：当GPU资源不足时的有效解决方案 | [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf) |  |  | 
| TaskMatrix.AI | [TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs](https://arxiv.org/pdf/2303.16434.pdf) |  |  | 
| AnnoLLM | [AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators](https://arxiv.org/pdf/2303.16854.pdf) |  |  | 
| 南加州大学:大语言模型统计偏好的挑战和危险 | [Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models](https://arxiv.org/pdf/2304.03738.pdf) |  |  | 
| 卡内基·梅隆大学 语言生成模型可能造成危害：那么我们能做些什么呢？ | [Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey](https://arxiv.org/pdf/2210.07700.pdf) |  |  | 
| 鹏程实验室等最新《大规模多模态预训练模型》全面综述 | [Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey](https://arxiv.org/abs/2302.10035) |  |  | 
| 预训练基础模型综合调研：从 BERT 到 ChatGPT 的历史 | [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/abs/2302.09419) |  |  | 
| 洛桑联邦理工学院提出REFINER框架，用于微调大规模语言模型 | [REFINER: Reasoning Feedback on Intermediate Representations](https://arxiv.org/pdf/2304.01904.pdf) |  |  | 
| LLM-Adapters： 用于大型语言模型的参数高效微调的适配器系列 | [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf) |  |  | 
| 大型语言模型的涌现记忆和可预测记忆 | [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/abs/2304.11158) |  |  | 
| 机器心理学：使用心理学方法研究大型语言模型中的涌现能力和行为 | [Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods](https://arxiv.org/abs/2303.13988v1) |  |  | 
| Chameleon：使用大型语言模型进行即插即用的组合推理 | [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2304.09842) |  |  | 
| 大型语言模型相关文献资源列表 |  | [code](https://github.com/RUCAIBox/LLMSurvey) |  | 
| RRTF：通过反馈提高代码生成的能力 | [PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback](https://arxiv.org/abs/2307.14936.pdf) |  | [blog](https://mp.weixin.qq.com/s/3lgztkBGlfCdHwygDggBbw) | 
| Google发布统计深度学习框架平台：OpenXLA |  |  | [blog](https://github.com/wshzd/ChatGPT-Summary/blob/main/AGI/Google_OpenXLA.md) |  
| AGI的火花一作Sébastien Bubeck演讲万字全文 |  | | [blog](https://mp.weixin.qq.com/s/H1RVdH0fmwM0GjfV3uvd4g) |  
| AGI通用智能发展的思考：是否存在足够通用的处理器？ |  |  | [blog](https://mp.weixin.qq.com/s/16TfOu4qfqlbQHpDgDUM2A) |  
| OpenAGI:当大语言模型遇到领域专家 | [OpenAGI: When LLM Meets Domain Experts](https://arxiv.org/abs/2304.04370) | [code](https://github.com/agiresearch/OpenAGI) |  | 
| 垂直领域大模型的一些思考及开源模型汇总 |  |  | [blog](https://mp.weixin.qq.com/s/HiGkSwbGeo4sPZvQeKCJfQ) | 
| 大模型时代-行业落地的再思考 |  |  | [blog](https://mp.weixin.qq.com/s/wSQSjO_0OmIg2kBZUuXA4Q) | 
| 医疗领域大模型的幻觉问题分析 |  |  | [blog](https://mp.weixin.qq.com/s/1o4u0Em0fFk9YndTaF2I7A) | 
| 基于中文金融知识的 LLaMA 系微调模型的智能问答系统：LLaMA大模型训练微调推理等详细教学 |  |  | [blog](https://mp.weixin.qq.com/s/lrKPUcS9GkSS20-Jda-8bA) | 
| 中文多模态医学大模型智能分析X光片，实现影像诊断，完成医生问诊多轮对话 |  |  | [blog](https://mp.weixin.qq.com/s/Spb_dbsHRyP9EvUaMYgHxw) | 
| 伯克利&微软｜用GPT-4进行可控的文本-图像生成 | [paper](https://arxiv.org/abs/2305.18583) |  |  |

## 代码生成

| Description| Paper | Code | Blog |
| --- | --- | --- | --- | 
| 代码大模型综述：中科院和MSRA调研27个LLMs，并给出5个有趣挑战 | [paper](https://arxiv.org/abs/2212.09420) |  | [blog](https://mp.weixin.qq.com/s/t2SMftox6546E7kvRgQMnA)，[项目主页](https://nl2code.github.io) |
| 北京大学：具有大语言模型的自我规划代码生成 | [paper](https://arxiv.org/pdf/2303.06689.pdf) |  |  |
| 谷歌提出Self-Debugging:教导大型语言模型进行自我调试 | [paper](https://arxiv.org/pdf/2304.05128.pdf) |  |  |
| 通过自我改进实现更好的代码语言模型，显著提高模型生成任务的性能 | [paper](https://arxiv.org/pdf/2304.01228.pdf) |  |  |
| MIT最新研究：利用大预言模型生成Code | [paper](https://arxiv.org/abs/2303.05510) | [code](https://github.com/shunzh/Code-AI-Tree-Search) | [项目网址](https://codeaimcts.github.io/) |
| MathPrompter: 基于大型语言模型的数学推理 | [paper](https://arxiv.org/abs/2303.05398) |  |  |
| MIT最新研究：利用大语言模型生成Code | [paper](https://arxiv.org/abs/2303.05510) | [code](https://github.com/shunzh/Code-AI-Tree-Search) | [官网地址](https://codeaimcts.github.io/) |
| 一键控制10万多个AI模型，HuggingFace给类ChatGPT模型们做了个「APP Store」 |  | [demo](https://huggingface.co/docs/transformers/transformers_agents) | [blog](https://mp.weixin.qq.com/s/8gyTqT1B4C2Da_6dmtaNiw) |

## ChatGPT 应用篇

| Description| Paper | Code |Blog |
| --- | --- | --- | --- | 
| 从 GPT 到 ChatGPT 的演进与应用思考 |  | | [blog](https://mp.weixin.qq.com/s/3Pr82xKpZ7mAWQcxPPB1xA) | 
| 语言模型可以预测公众舆论 | [Language models trained on media diets can predict public opinion](https://arxiv.org/pdf/2303.16779.pdf) |  |  | 
| ChatGPT助力芯片，传统 EDA如何演变成智能EDA |  |  | [blog](https://mp.weixin.qq.com/s/JyveUDEYKLrFolfCFLqhhw) | 
| ChatGPT机器人:设计原则和模型能力 | [ChatGPT for Robotics: Design Principles and Model Abilities](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/) |  |  | 
| 各种环境下的ChatGPT赋能长步机器人控制： 一个案例的应用 | [ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application](https://arxiv.org/pdf/2304.03893.pdf) | [code](https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts) |  | 
| ChatGPT获得了「Wolfram」超能力 |  |  | [blog](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/) | 
| OpenAI开发Plugin将 ChatGPT 连接到互联网 |  |  | [blog](https://techcrunch.com/2023/03/23/openai-connects-chatgpt-to-the-internet/) | 
| ChatAug：利用ChatGPT进行文本数据增强 | [ChatAug: Leveraging ChatGPT for Text Data Augmentation](https://arxiv.org/pdf/2302.13007v1.pdf) |  |  | 
| ChatGPT 是数据隐私的另一个障碍吗 |  |  | [blog](https://www.bizcommunity.com/Article/196/639/236418.html) | 
| 基于ChatGPT的数据增强方法：ChatAug和AugGPT |  |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486140&idx=1&sn=bba4342966c99559938824f2d747d231&chksm=ced54958f9a2c04ec121b8c198d69a5a17c8b3e0a96a0cfcd8d1271bd6097a2cbf66895dd8a9&token=447941009&lang=zh_CN#rd) | 
| Character.AI 在ChatGPT基础上加入个性化、UGC两大武器，有比 ChatGPT 更丰富的使用场景 |  |  | [blog](https://mp.weixin.qq.com/s/U4R8loz1G9PYM_l6IvNF_A) | 
| 让ChatGPT可以**语音交互** |  |  | [blog](https://mp.weixin.qq.com/s/H4XLCQ-kR7T28yywHJL4uA) | 
| “ChatGPT们”的淘金时代 |  |  | [blog](https://mp.weixin.qq.com/s/otdenJh5FJsCgi5ONy9JIQ) | 
| 70 款 ChatGPT 插件评测（含样例分析） |  |  | [blog](https://mp.weixin.qq.com/s/vHwAk63ukRteF1u1myrTlA) | 
| 人大提出WebBrain：NLP新任务，通过网络数据的挖掘生成真实文章 | [WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus](https://arxiv.org/abs/2304.04358) | [code](https://github.com/qhjqhj00/WebBrain) |  | 
| ChatGPT爆火带来思考：医学界或将迎来与AI融合的奇点？ |  |  | [blog](https://mp.weixin.qq.com/s/x8ppg6GVCAeLNpv5uJ7B7g) | 
| 论ChatGPT大语言模型在教育中的机遇与挑战 |  |  | [blog](https://url39.ctfile.com/f/2501739-809898048-6394c7?p=2096) | 
| ChatGPT在投资研究领域的应用初探及原理分析 |  |  | [blog](https://mp.weixin.qq.com/s/LFPeSLeEOTb1-2YJBXclbQ) | 
| OpenAI总裁Greg Brockman转发｜一种编译语言的调试器，利用ChatGPT旨在增强您使用GDB进行调试体验 |  | [code](https://github.com/pgosar/ChatGDB) |  | 
| 不必排队等 OpenAI Plugins，OpenBMB 开源大模型工具学习引擎 |  |  | [blog](https://hub.baai.ac.cn/view/25189) | 
| 分析了ChatGPT技术以及落地应用场景 |  |  | [blog](https://url39.ctfile.com/f/2501739-805099789-098b62?p=2096) | 

## ChatGPT 工具篇

| Description| Paper | Code |Blog |
| --- | --- | --- | --- | 
| ChatGPT 应用汇总及操作手册 |  |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485794&idx=1&sn=6aa0500e3139b67246dd5f96007d1487&chksm=ced54a86f9a2c390d86856181f1fcd09091cf84d67e81535b6d592617f49fe24349779cfa1e5&token=447941009&lang=zh_CN#rd) | 
| ChatGPT提示和技巧速查手册 |  |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485766&idx=1&sn=43ad627e4e183d7a108c3c57ab0e02dc&chksm=ced54aa2f9a2c3b4a2d529e4ed7c2acc7fa32e7465837045d3ec607701e0da2a55c0c557cad2&token=447941009&lang=zh_CN#rd) | 
| 非常全面的ChatGPT、LLM相关资源整理分享 |  | [code](https://github.com/cedrickchee/chatgpt-universe) |  | 
| ChatGPT超全面课程 |  |  | [blog](https://tested-salto-cab.notion.site/The-Ultimate-Chat-GPT-Course-69ed24a317a942d288e740419b1ad6f6) | 
| BloombergGPT: A Large Language Model for Finance | [BloombergGPT: A Large Language Model for Finance](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=b0e4b03ecf5c11edb95839eec3084ddd) |  |  | 
| ChatPDF：一键上传PDF文件即可解读 |  |  | [blog](https://mp.weixin.qq.com/s/S1DUJrNK5_H5krvHotOwHQ)，[试用地址](https://www.chatpdf.com/) | 
| ChatWeb：可爬取网页正文，并根据正文回答问题 |  | [code](https://github.com/SkywalkerDarren/chatWeb) |  | 
| chatgpt_academic：中科院基于 ChatGPT 专属定制的学术研究及日常开发工具 | --- | [code](https://github.com/binary-husky/chatgpt_academic) | [blog](https://hub.baai.ac.cn/view/25298)，[demo](https://huggingface.co/spaces/qingxu98/gpt-academic) | 
| Einstein GPT：SaaS 行业巨头 Salesforce 宣布与 OpenAI 合作，推出 Einstein GPT，这是全球首个用于客户关系管理（CRM）的生成式 AI 产品 |  |  | [Einstein GPT地址](https://www.salesforce.com/products/einstein/overview/?d=cta-body-promo-8)，[试用地址](https://openai.com/waitlist/slack) | 
| HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace](https://arxiv.org/pdf/2303.17580.pdf) |  |  | 
| ImpressionGPT： 利用ChatGPT对放射科报告进行总结的迭代优化框架 | [ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT](https://arxiv.org/abs/2304.08448) |  |  | 
| OpenGpt：创建ChatGPT小应用的AI平台 |  | [code](https://github.com/futantan/OpenGpt) | [官网](https://open-gpt.app/) | 
| TagGPT：腾讯提出零样本多模态标签的大语言模型TagGPT | [TagGPT: Large Language Models are Zero-shot Multimodal Taggers](https://arxiv.org/abs/2304.03022) | [code](https://github.com/TencentARC/TagGPT) |  | 
| Visual ChatGPT: 在视觉模型加持下的ChatGPT，聊天生图全拿捏了。 | [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/pdf/2303.04671.pdf) |  |  | 
| NetGPT：用于网络流量的生成预训练Transformer模型 | [NetGPT: Generative Pretrained Transformer for Network Traffic](https://arxiv.org/pdf/2304.09513.pdf) |  |  | 

## ChatGPT 技术篇

| Description| Paper | Code |Blog |
| --- | --- | --- | --- | 
| ChatGPT_Inference_Cost | --- | --- | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/ChatGPT_Inference_Cost.md) | 
| ChatGPT_Official_API_Learning |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/ChatGPT_Official_API_Learning.md) | 
| ChatGPT_Parameter_is_not_175B |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/ChatGPT_Parameter_is_not_175B.md) | 
| ChatGPT_Road_Map_from_yao.fu |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/ChatGPT_Road_Map_from_yao.fu.md) | 
| Lessons_Learned_from_ChatGPT_Recurrence |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/Lessons_Learned_from_ChatGPT_Recurrence.md) | 
| LLM_Pre-training_Guide（Bloom-175B） |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/LLM_Pre-training_Guide（Bloom-175B）.md) | 
| The_guide_of_training_LLM |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/The_guide_of_training_LLM.md) | 
| 深度拆解GPT-3.5能力起源 |  |  | [原文blog](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)，[译文blog](https://mp.weixin.qq.com/s/ckd6KxeTfdQas_UCsJ7HgQ) | 
| ChatGPT发展历程、原理、技术架构详解和产业未来 |  |  | [blog](https://zhuanlan.zhihu.com/p/590655677) | 
| 让天下没有难训练的大模型，微软亚洲研究院开源TorchScale |  | [code](https://github.com/microsoft/torchscale) |  | 
| 82页PPT ！最新ChatGPT: 提示学习, 指导微调和RLHF |  |  | [blog](https://pan.baidu.com/s/15Bs1u7z1RhCdfiR3oJ_gJQ), [提取码:chat]| 
| 他们提出了包含视觉特征的 Multimodal-CoT，该架构在参数量小于 10 亿的情况下，在 ScienceQA 基准测试中，比 GPT-3.5 高出 16 个百分点 | [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) | [code](https://github.com/amazon-science/mm-cot) |  | 
| Nature ：生成式 AI 的前景与风险 |  |  | [blog](https://mp.weixin.qq.com/s/d6t2xpdvSDCHzO2gG1N6eQ) | 
| 万字长文解读：从Transformer到ChatGPT，通用人工智能曙光初现 |  |  | [blog](https://mp.weixin.qq.com/s/iZyKmWgXUkPv3Phyaw4Ppg) | 
| AI芯片制造商Cerebras发布7个基于GPT的大语言模型，现已开源 |  |  | [官网地址](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)，[GPT地址](https://www.cerebras.net/cerebras-gpt)，[Hugging Face地址](https://huggingface.co/cerebras) | 
| 大模型论文周报丨GPT-4发布，谷歌开放PaLM API，斯坦福7B开源模型Alpaca媲美GPT-3.5 |  |  | [blog](https://mp.weixin.qq.com/s/C6g_H6xfFn59IxnLpbjA1g) | 
| LLaMA模型Meta版泄露，GitHub获8K星 |  |  | [blog](https://mp.weixin.qq.com/s/2M19WSq2YICo-3t5ibQcig) | 
| ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark | [ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark](https://arxiv.org/abs/2303.13648) |  |  | 
| 打造中国版ChatGPT，国内哪家实力最强 |  |  | [blog](https://mp.weixin.qq.com/s/B-n_qz110HmhSP66NKRCiQ) | 
| 复旦大学邱锡鹏教授解读ChatGPT |  |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485810&idx=1&sn=47eb672c688517d6bade2c62c7eae94f&chksm=ced54a96f9a2c380ccacfbb223df52de64f2c410a91e726023a074fc98fb87fcd9f60f5a4957&token=447941009&lang=zh_CN#rd) | 
| 万字长文:可能是全网最晚的ChatGPT技术总结 |  |  | [blog](https://mp.weixin.qq.com/s/LJoxupaKflL793TCwnpyPg) | 
| ChatGPT作为知识库问答系统的问答能力评测 |  |  | [blog](https://mp.weixin.qq.com/s/xul2-SENnqxV8VehozDKHg) | 
| ChatGPT作者John Shulman：我们成功的秘密武器 |  |  | [blog](https://www.talkrl.com/episodes/john-schulman)，[blog译文](https://mp.weixin.qq.com/s/sDeBYMvAwbJr5_tj7Q20-w) | 
| ChatGPT 是数据隐私的另一个障碍吗 |  |  | [blog](https://www.bizcommunity.com/Article/196/639/236418.html) | 
| Hugging Face 每周速递: ChatGPT API 怎么用？我们帮你搭好页面了 |  |  | [blog](https://mp.weixin.qq.com/s/oeXgd78vFV8os2uTGZkFQQ) | 
| 复旦大学教授肖仰华：ChatGPT 浪潮下，面向大模型如何做数据治理？ |  |  | [blog](https://mp.weixin.qq.com/s/od24PYvFLUJe4NQxjvsbMw) | 
| 腾讯在ChatGPT的布局 |  |  | [blog](https://mp.weixin.qq.com/s/rdpGZII3pu3MHr-lFm3GyQ) | 
| 浅析ChatGPT：历史沿革、应用现状及前景展望 |  |  | [blog](https://mp.weixin.qq.com/s/fQ8DmL_M3QMiFX23Tf0z7w) | 
| ChatGPT 背后的“功臣”——人类反馈强化学习RLHF 技术详解 |  |  | [blog](https://mp.weixin.qq.com/s/mZdZS9QNda26Ae0OIhRjFA) | 
| 万字长文解析！复现和使用GPT-3/ChatGPT，你所应该知道的 |  |  | [blog](https://mp.weixin.qq.com/s/ILpbRRNP10Ef1z3lb2CqmA) | 
| 想训练ChatGPT？得先弄明白Reward Model怎么训（附源码） |  |  | [blog](https://mp.weixin.qq.com/s/1v4Uuc1YAZ9MRr1UWMH9xw) | 
| ChatGPT核心技术：强化学习PPO算法 |  |  | [blog](https://mp.weixin.qq.com/s/z4oc9xQmduKMolWxztdHjA) | 
| 解读 ChatGPT 背后的技术重点：RLHF、IFT、CoT、红蓝对抗 |  |  | [blog](https://mp.weixin.qq.com/s/y4ywidZ55BQLgQzJa_Wjbg) | 
| OpenAI ChatGPT Code Interpreter入门 |  |  | [blog](https://www.oneusefulthing.org/p/what-ai-can-do-with-a-toolbox-getting) | 
| 加拿大魁北克大学教授详述：我们该拿ChatGPT怎么办？ |  |  | [blog](https://lemire.me/blog/2023/04/03/what-are-we-going-to-do-about-chatgpt/) | 
| AIGC时代的ChatGPT全面综述 | [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://arxiv.org/abs/2304.06488) |  |  | 
| ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models |  [ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models](https://arxiv.org/pdf/2303.16421.pdf) |  |  | 
| GPT-3 和 GPT-3.5 系列模型的全面分析 | [A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models](https://arxiv.org/abs/2303.10420v1) |  |  | 
| ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks | [CHATGPT OUTPERFORMS CROWD-WORKERS FOR TEXT-ANNOTATION TASKS](https://arxiv.org/pdf/2303.15056.pdf) |  |  | 
| AdaLoRA：自适应预算分配以实现参数有效的微调 | [ADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENT FINE-TUNING](https://arxiv.org/pdf/2303.10512.pdf) | [code](https://github.com/QingruZhang/AdaLoRA) |  | 
| 大型语言模型的语境忠实提示法 | [Context-faithful Prompting for Large Language Models](https://arxiv.org/pdf/2303.11315.pdf) |  |  | 
| ChatGPT问，BLIP-2回答模型：面向丰富的视觉描述的自动提问 | [ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions](https://arxiv.org/pdf/2303.06594.pdf) | [code](https://github.com/Vision-CAIR/ChatCaptioner) |  | 
| ChatGPT真的可以取代知识图谱问答吗？ | [Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of GPT family LLMs' Question Answering Performance](https://arxiv.org/abs/2303.07992)，[paper翻译](https://mp.weixin.qq.com/s/cvBVgxCrreic6U6CU-YB-A) |  |  | 
| Meta & 斯坦福大学推出FlexGen：用单个GPU进行大型语言模型的高吞吐量生成性推理 | [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865.pdf) | [code](https://github.com/FMInference/FlexGen) |  | 
| ChatGPT破圈的「秘密武器」：详解RLHF如何影响人类社会！ | [Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2303.02891) |  | [blog](https://mp.weixin.qq.com/s/DCFhefWGQS5naYwT3o6neg) | 
| 探讨ChatGPT在对抗攻击和分布外泛化下的鲁棒性 | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/pdf/2302.12095.pdf) | [code](https://github.com/microsoft/robustlearn) |  | 
| 复旦清华联合顶刊发文｜ChatGPT：潜力、前景和局限 | [ChatGPT: potential, prospects, and limitations](https://link.springer.com/article/10.1631/FITEE.2300089) |  | [blog](https://mp.weixin.qq.com/s/1D62QuxXFDXWwwRXrB-Ivw) | 
| 引导ChatGPT不要输出有害信息 | [The Capacity for Moral Self-Correction in Large Language Models](https://arxiv.org/pdf/2302.07459.pdf) |  |  | 
| Junnan Li大佬发表最新多模态的杰作BLIP2 | [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | [code](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) | [blog](https://mp.weixin.qq.com/s/xmSy4m7NheY8iComv7grxQ) | 
| Instruction Tuning：无/少样本学习新范式 | [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652) | [code](https://github.com/google-research/flan) |  | 
| GPTScore：一种新的评估语言模型方法 | [GPTScore: Evaluate as You Desire](https://arxiv.org/abs/2302.04166) | [code](https://github.com/jinlanfu/GPTScore) |  | 
| ChatGPT内核：InstructGPT，基于反馈指令的PPO强化学习 |  |  | [blog](https://zhuanlan.zhihu.com/p/589747432)，[video](https://www.bilibili.com/video/BV1hd4y187CR) | 
| Fine-tune-CoT：小模型也能做推理，完美逆袭大模型 | [Large Language Models Are Reasoning Teachers](https://arxiv.org/pdf/2212.10071.pdf) | [code](https://github.com/itsnamgyu/reasoning-teacher) |  | 
| ChatGPT的潜力解锁：自然语言处理中应用、优势、限制和未来方向的全面探索 | [UNLOCKING THE POTENTIAL OF CHATGPT: A COMPREHENSIVE EXPLORATION OF ITS APPLICATIONS, ADVANTAGES, LIMITATIONS, AND FUTURE DIRECTIONS IN NATURAL LANGUAGE PROCESSING](https://arxiv.org/pdf/2304.02017.pdf) |  |  | 
| 阿里巴巴&清华大学|大型语言模型在算术任务中的表现如何？ | [How well do Large Language Models perform in Arithmetic tasks?](https://arxiv.org/pdf/2304.02015.pdf) | [code](https://github.com/GanjinZero/math401-llm) |  | 
| 本科生60行代码教你手搓GPT大模型 |  | [code](https://github.com/jaymody/picoGPT/tree/29e78cc52b58ed2c1c483ffea2eb46ff6bdec785) |  | 

## GPT4

| Description| Paper | Code | Blog |
| --- | --- |--- | --- |
| GPT4_System_Card中文翻译 |  |  |[blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Official/GPT-4_System_Card_zh.md) |
| GPT4_Technical_Report中文翻译 |  |  |[blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Official/GPT4_Technical_Report_zh.md) |
| 【万字长文】GPT-4秘密泄露！所有的信息都在这里！从GPT-4 架构、基础设施、训练数据集、成本、视觉到MoE！ |  |  | [blog](https://mp.weixin.qq.com/s/vgUKe31pykC12sUV5xyLNQ)，[原blog](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure) |
| GPT-4 令人印象深刻但仍在 10 个方面具有缺陷 |  |  | [blog](https://www.nytimes.com/2023/03/14/technology/openai-new-gpt4.html) |
| 多模态大模型GPT-4的新突破 |  |  | [blog](https://hub.baai.ac.cn/view/24852) |
| 重磅发布GPT-4 |  |  | [blog](https://openai.com/research/gpt-4) |
| GPT-4 创造者 Ilya Sutskever 谈 AI 幻觉和 AI 民主 |  |  | [blog](https://www.forbes.com/sites/craigsmith/2023/03/15/gpt-4-creator-ilya-sutskever-on-ai-hallucinations-and-ai-democracy/?sh=7743f01e1218) |
| GPT-4创造者：第二次改变AI浪潮的方向 |  |  | [blog](https://mp.weixin.qq.com/s/rZBEDlxFVsVXoL5YUVU3XQ) |
| 当GPT-4进入北京市2022高考考场能有什么表现？ |  |  | [blog](https://mp.weixin.qq.com/s/N_j01KSuEKuVwCCD69G92g) |
| GPT4技术细节 |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Blog/GPT4_Technical_Detail.md) |
| GPT4技术关键点总结 |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Blog/GPT4_Technical_Summary.md) |
| GPT4和ChatGPT的效果对比 |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT_VS_GPT4/GPT4_VS_ChatGPT（from_nytimes）.md) |
| The Ultimate GPT-4 Guide |  |  | [blog](https://doc.clickup.com/37456139/d/h/13q28b-324/e2a22b0c164b1f9) |
| GPT-4里套娃LLaMA 2！OpenAI创始成员周末爆改「羊驼宝宝」，GitHub一日千星 |  |  | [blog](https://mp.weixin.qq.com/s/Tp4q8VflEZ7o8FgpZfrNgQ) |
| Claude 2 解读 ChatGPT 4 的技术秘密：细节：参数数量、架构、基础设施、训练数据集、成本 |  |  | [blog](https://mp.weixin.qq.com/s/E2KpvldbYrH0NFvxgfsMlw) |
| 用GPT-4进行指令调优 | [INSTRUCTION TUNING WITH GPT-4](https://arxiv.org/pdf/2304.03277.pdf) | [code](https://instruction-tuning-with-gpt-4.github.io/) |  |
| 点燃通用人工智能的火花：GPT-4的早期实验 | [原始paper](https://arxiv.org/pdf/2303.12712.pdf)，[中文版paper](https://event-cdn.baai.ac.cn/file/file-browser/waTXJn85fm3FPyDXpsZ4faGk47trjjYb.pdf) |  |  [blog](https://mp.weixin.qq.com/s/H1RVdH0fmwM0GjfV3uvd4g) |
| GPT4All：用GPT-3.5-Turbo的大规模数据提炼训练一个助理式聊天机器人 | [GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf) | [code](https://github.com/nomic-ai/gpt4all) |  |
| 美国东北大学：可以通过要求GPT4反思“你为什么错了？”来提高30%的性能 | [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366) | [code](https://github.com/noahshinn024/reflexion) |  |
| 对ChatGPT/GPT-4研究的总结以及对大型语言模型未来的展望 | [Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models](https://arxiv.org/pdf/2304.01852.pdf) |  |  |
| 评估日本医疗执照考试的GPT-4和ChatGPT | [Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations](https://arxiv.org/pdf/2303.18027.pdf) |  |  |
| 深入研究LLMs与AutoGPT的结合：揭示出GPT-4惊人的人类决策能力！ | [Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions](https://arxiv.org/pdf/2306.02224.pdf) | [code](https://github.com/younghuman/LLMAgent) | [blog](https://mp.weixin.qq.com/s/Gbz7ZVVdeTq64mj1-__aQA) |

## LLM训练 微调 优化 评估以及部署

**【LLM 学习网站】**【训练、微调、优化和部署大模型最新技术LLM Learning Lab】[[官网](https://lightning.ai/pages/llm-learning-lab/)]

**【LLM 算力评估】**【PEFT | Transformer参数量、计算量、显存占用分析】[[官网](https://mp.weixin.qq.com/s/5zxfwlO-skxZchJ0qtzqAw)]

**【LLM Tokenizer】**【Tokenizer的系统梳理，并手推每个方法的具体实现】[[blog](https://mp.weixin.qq.com/s/W8QaPQFeGO7S6mTZt8iKcg)]

### LLM训练

| Description| Blog |
| --- | --- | 
| 从头预训练大模型实践经验 | [blog](https://mp.weixin.qq.com/s/q8XNFzsm_sm_EocCIks-1w) | 
| DeepSpeed的Tutorials | [主页](https://www.deepspeed.ai)，[DeepSpeed Getting Starte](https://mp.weixin.qq.com/s/xpNQtl7hPs3fy9S7VRbIkg) | 
| 打造LLM界的Web UI：24GB显卡训练百亿大模型 | [blog](https://mp.weixin.qq.com/s/x9oED0Uxc5Wt-eR0Amde7g) | 
| 大模型训练感知量化开山之作：LLM-QAT | [blog](https://mp.weixin.qq.com/s/zKndNym9Q7QJWlmn60HmyQ) | 
| 混合精度训练技术梳理总结 | [blog](https://mp.weixin.qq.com/s/j-f47VPHKAkCwpwEheEgJQ) | 
| LLM大模型训练Trick系列之拒绝采样 | [blog](https://zhuanlan.zhihu.com/p/649731916) | 
| Muti Query Attention 和 Attention with Linear Bias（附源码） | [blog](https://mp.weixin.qq.com/s/GXMwnbWLce9Aq4alEHCHJA)，[paper](https://arxiv.org/pdf/1911.02150.pdf) | 
| 如何使用 Megatron-LM 训练语言模型 | [blog](https://mp.weixin.qq.com/s/QPg6gOWGbQDezTl8OFZU3g) | 

### LLM微调

| Description| Blog |
| --- | --- | 
| PEFT: 在低资源硬件上对十亿规模模型进行参数高效微调 | [blog](https://mp.weixin.qq.com/s/x2mQBE0pfTv8w3Czp8JkDg) | 
| 大语言模型（LLM）微调技术笔记 | [code](https://github.com/ninehills/ninehills.github.io/issues/92) | 
| 大模型LLM-微调经验分享&总结 | [code](https://github.com/liucongg/ChatGLM-Finetuning)，[blog](https://mp.weixin.qq.com/s/pkBvL0k7sZWaW6jMlSSIZA) | 
| LoRA：卷完图像生成领域，卷文本生成领域的东西，到时是个啥？ | [blog](https://mp.weixin.qq.com/s/emLpTAOhr8khO1hTgQhU9w)，[code](https://github.com/microsoft/LoRA) | 
| QLoRA：在单个48GB GPU上对65B参数的大模型进行微调，只需微调12个小时就可以达到97%的ChatGPT水平。同时只用int4就可以保持fp16精度的效果。 | [paper](https://arxiv.org/pdf/2305.14314.pdf) | 
| 华盛顿大学提出全新量化和微调方法，在DB-GPT上享受33B参数的LLM | [blog](https://mp.weixin.qq.com/s/A3flqm2FeOn0WQr5mrD1-Q) | 
| MeZO：高效零阶优化器，单卡A100可训练300亿参数模型 | [paper](https://arxiv.org/abs/2305.17333)，[code](https://github.com/princeton-nlp/MeZO)，[blog](https://mp.weixin.qq.com/s/JteUpY4fEbENQFvReRLPJg) | 
| 人工智能大语言模型微调技术：SFT 监督微调、LoRA 微调方法、P-tuning v2 微调方法、Freeze 监督微调方法 | [blog](https://mp.weixin.qq.com/s/N0Z1Kq0mrVrK-RED_gvJmw) | 
| LLM微调经验分享 | [中文blog](https://mp.weixin.qq.com/s/83sqfeaoSKtMSo_5Sf_doA)，[英文blog](https://twitter.com/xinqiu_bot/status/1679786303716749312) | 
| Firefly项目 | [介绍Firefly项目如何充分高效训练多轮对话大模型](https://mp.weixin.qq.com/s/WG_YCk6DM8nWvLfpw1OmoA)，[源码解析ChatGLM2多轮对话训练方法的不足，以及改进方法](https://mp.weixin.qq.com/s/r-JOLsoIAgZynGIeryU1-Q) | 

### LLM优化

| Description| Blog |
| --- | --- | 
| 伯克利开源LLM推理与服务库：GPU减半、吞吐数十倍猛增 | [中文blog](https://hub.baai.ac.cn/view/27505)，[英文blog](https://vllm.ai/?continueFlag=24b2e01413fd53e24a2779b4a664ca16) | 
| CAME：大模型训练成本降低近一半 | [blog](https://mp.weixin.qq.com/s/iUXu_Pfsop0bq7ktoXTY4A) | 
| 大模型推理性能优化之KV Cache解读 | [blog](https://mp.weixin.qq.com/s/ydjcUOF9iUM581hUTSXPdw) | 
| LLM，压缩即泛化，泛化即智能 | [blog](https://mp.weixin.qq.com/s/tSj9npIPg8IlYr2jbtg-Og) | 
| LLM-Pruner: 剪枝+少量数据+少量训练 = 高效的LLM压缩 | [blog](https://mp.weixin.qq.com/s/feqFfy4n31eztoZfodMieQ) | 
| LLM Accelerator：使用参考文本无损加速大语言模型推理 | [blog](https://mp.weixin.qq.com/s/H1JaQZ9-m2gkZaIwzJTTtg)，[paper](https://arxiv.org/pdf/2304.04487.pdf)，[code](https://github.com/microsoft/LMOps) | 
| LLM 的推理优化技术纵览 | [blog](https://mp.weixin.qq.com/s/Os4Uy8K6z2fVMSa7ihR1dg) | 
| LLM量化之后，能力退化了多少 | [blog](https://mp.weixin.qq.com/s/ri4SS_NCKn4boGZfJtUWUQ) | 
| 邱锡鹏团队提出新优化器LOMO｜650亿参数，8块GPU全参数微调 | [blog](https://mp.weixin.qq.com/s/339iXf2bimusfq6zQmFpWw)，[paper](https://arxiv.org/abs/2306.09782) | 
| 继思维链、思维树后又一思维骨架：让大模型能做并行解码 | [blog](https://mp.weixin.qq.com/s/cyyKEtGe6QBmP8aAU9fmhQ) | 
| FrugalGPT | [paper](https://arxiv.org/pdf/2305.05176.pdf)，[blog](https://www.reddit.com/r/singularity/comments/13dnfd7/frugalgpt_can_match_the_performance_of_the_best/) | 
### LLM评估

| Description| Blog |
| --- | --- | 
| 工程实践！以LLAMA为例的大模型部署方案 | [blog](https://mp.weixin.qq.com/s/zGkkekFqKsnM66uQwfUPcw) | 
| 一文看遍各行业对ChatGPT的专业评估 | [blog](https://mp.weixin.qq.com/s/2JryWW33j9udOpi3dK5X9g) | 
| ChatGPT关于推理、幻觉和交互的多任务、多语言、多通道评估 | [paper](https://arxiv.org/abs/2302.04023) | 
| 如何评价 OpenAI 的超级对话模型 ChatGPT ？ | [paper](https://www.zhihu.com/question/570189639) | 
| 用ChatGPT参加计算机科学考试 | [paper](https://arxiv.org/abs/2303.09461) | 
| C-Eval：构造中文大模型的知识评估基准 | [主页](https://cevalbenchmark.com/)，[paper](https://mp.weixin.qq.com/s/4560jl7ctWmHz3xGVIKkRw)，[code](https://github.com/SJTU-LIT/ceval)，[blog](https://mp.weixin.qq.com/s/4560jl7ctWmHz3xGVIKkRw) | 
| 多模态大模型的幻觉问题与评估 | [blog](https://mp.weixin.qq.com/s/s0z-mAyjAaqvNcaTg2VFEA)，[paper](https://arxiv.org/abs/2305.10355)，[code](https://github.com/RUCAIBox/POPE) | 
| 谷歌提出TrueTeacher：基于大型语言模型的学习事实一致性评价 | [blog](https://mp.weixin.qq.com/s/tcqHXIHxrkiYrYBX8JzIZA)，[paper](https://arxiv.org/pdf/2305.10601.pdf) | 
| 粗看大模型ChatGLM、MOSS、Bloomz在中文垂域评测中的性能表现：医学、法律、心理学、教育等四大类试题下的测试报告介绍 | [paper](https://arxiv.org/pdf/2304.12986.pdf)，[code](https://github.com/Felixgithub2017/MMCU)，[blog](https://mp.weixin.qq.com/s/Hq6bn_4vD559TMQxx806tg) | 
| 评测国内各种对标 ChatGPT 的大语言模型 | [blog](https://mp.weixin.qq.com/s/Oe1Rc0kXjMOD2G_Sqambow)，[code](https://github.com/dongrixinyu/JioNLP/wiki/LLM%E8%AF%84%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86) | 
| OpenLLM大模型排行榜 | [主页](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，[blog](https://mp.weixin.qq.com/s/t1Th8iFOGoyuqqysUiIcXQ)，[最新进展blog](https://zhuanlan.zhihu.com/p/642996275) | 
| 斯坦福发布LLM排行榜AlpacaEval，微软WizardLM登顶开源模型第一 | [blog](https://mp.weixin.qq.com/s/7X8pRaexWJ4c0kVswawU1A)，[主页](https://tatsu-lab.github.io/alpaca_eval)，[code](https://github.com/tatsu-lab/alpaca_eval) | 

### LLM部署

| Description| Blog |
| --- | --- | 
| 工程实践！以LLAMA为例的大模型部署方案 | [blog](https://mp.weixin.qq.com/s/zGkkekFqKsnM66uQwfUPcw) | 
| 大模型部署框架FastLLM解析，支持X86/Arm/CUDA 3种架构的硬件！ | [blog](https://mp.weixin.qq.com/s/j19QdlFvblcABXzB7Vi5wg)，[code](https://github.com/ztxz16/fastllm) | 
| 用 Hugging Face 推理端点部署 LLM | [blog](https://mp.weixin.qq.com/s/ms1ThLcN6uTOFgKm5FqBig) | 
| 【完全指南】如何在本地运行LLM模型：提高模型性能与运行速度 | [blog](https://mp.weixin.qq.com/s/Ijf6MrUdqG0JxiRmF6Wh5w) | 
| LLM 低成本 GPU 部署方案 lmdeploy 开源！ | [blog](https://mp.weixin.qq.com/s/cndjXFr3vJPdN-7aTNqCnQ)，[code](https://github.com/InternLM/lmdeploy) | 
| 使用 BentoML 部署 🤗 Hugging Face 上的模型：DeepFloyd IF 实战 | [中文blog](https://mp.weixin.qq.com/s/GySP9vpzf3cj6vtQAsRRvw)，[英文blog](https://hf.co/blog/deploy-deepfloydif-using-bentoml)，[code](https://github.com/bentoml) | 

## LLM 关键技术与应用

### Prompt Engineering

Some examples of **Prompt Engineering** as follows：

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| OpenAI 应用人工智能研究负责人Lilian Weng新博文：关于提示工程的介绍 | |  | [blog](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/) | 
| Prompt Engineering全面自动化 | | | [blog](https://mp.weixin.qq.com/s/aj8Ls463jpF92ssn6Acwzg) | 
| ChatGPT提示示例集合 | [huggingface](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts) | [ChatGPT提示示例集合](https://github.com/f/awesome-chatgpt-prompts/) | [主页](https://prompts.chat) | 
| ChatGPT Prompt工程：设计、实践与思考 | | | [blog](https://mp.weixin.qq.com/s/a8hjzZ_Rzl6pOU1PRAARJQ) | 
| 指令学习综述｜ChatGPT背后的指令学习是什么 | [Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning](https://arxiv.org/pdf/2303.10475v2.pdf) | | [blog](https://mp.weixin.qq.com/s/BK30JkIlshwkdHRjaRCD2g) | 

**Complete Content**: please refer to [Prompt Engineering](https://github.com/ArronAI007/Awesome-AGI/tree/main/Prompt/README.md)

### LLM DataSets

Some examples of **DataSets** as follows：

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 一篇关于LLM指令微调的综述 | [paper](https://arxiv.org/pdf/2308.10792.pdf) |  | [blog](https://mp.weixin.qq.com/s/7pqBvgF1BWDFxP5hajmBNw) |  
| 智源研究院发布国内首个大规模、可商用中文开源指令数据集COIG：最大规模中文多任务指令集，上新千个中文数据集 | [paper](https://arxiv.org/pdf/2304.07987.pdf) |  | [blog](https://mp.weixin.qq.com/s/PvJa8dPHk6aGEv1G1B3PUw)，[COIG-PC数据下载地址](https://huggingface.co/datasets/BAAI/COIG-PC)，[COIG数据下载地址](https://huggingface.co/datasets/BAAI/COIG) |  
| 总结当前开源可用的Instruct/Prompt Tuning数据 |  |  | [blog](https://mp.weixin.qq.com/s/vDbTJo3F7sy3-NY8xxg8jw) |  
| GPT-4平替版：MiniGPT-4，支持图像理解和对话，现已开源 |  |  | [dataset](https://drive.google.com/file/d/1nJXhoEcy3KTExr17I7BXqY5Y9Lx_-n-9/view) |  
| 多模态C4：一个开放的、10亿规模的、与文本交错的图像语料库 | [paper](https://arxiv.org/abs/2304.06939) | [code](https://github.com/allenai/mmc4) |  |  
| Mind2Web: 首个全面衡量大模型上网能力的数据集 |  |  | [blog](https://mp.weixin.qq.com/s/vge4CJbBfLXFIYYyNC12Hw) |  
| 该数据集是一个由人工生成、人工注释的助理式对话语料库，覆盖了广泛的主题和写作风格，由 161443 条消息组成，分布在 66497 个会话树中，使用 35 种不同的语言。该语料库是全球众包工作的产物，涉及超过 13500 名志愿者。为了证明 OpenAssistant Conversations 数据集的有效性，该研究还提出了一个基于聊天的助手 OpenAssistant，其可以理解任务、与第三方系统交互、动态检索信息。 | [paper](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view ) | [code](https://github.com/LAION-AI/Open-Assistant) | [dataset](https://huggingface.co/datasets/OpenAssistant/oasst1) |  
| 为了让Panda LLM在中文数据集上获得强大的性能，作者使用了强大的指令微调instruction-tuning技术，将LLaMA基础模型在五个开源的中文数据集进行混合训练，其中包括来自各种语言领域的1530万个样本，例如维基百科语料，新闻语料，百科问答语料，社区问答语料，和翻译语料。 |  |  | [blog](https://mp.weixin.qq.com/s/IsWSPAvwgT263wjO7TYTZQ) |  
| RedPajama开源项目｜复制超过1.2万亿个令牌的LLaMA训练数据集 |  | [code](https://github.com/togethercomputer/RedPajama-Data) | [原始blog](https://www.together.xyz/blog/redpajama)，[中文blog](https://hub.baai.ac.cn/view/25485)，[dataset](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) |  

**Complete Content**: please refer to [DataSets](https://github.com/ArronAI007/Awesome-AGI/tree/main/DataSets/README.md)

### LLM RLHF

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 复现RLHF：通过开源项目 trl 搭建一个通过强化学习算法（PPO）来更新语言模型（GPT-2） |  | [code](https://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF) | [blog](https://mp.weixin.qq.com/s/1v4Uuc1YAZ9MRr1UWMH9xw) |  
| 详解大模型RLHF过程（配代码解读） |  |  | [blog](https://mp.weixin.qq.com/s/2M3igy7Ctk2LAdNQLSO5tg) |  
| 想训练ChatGPT？得先弄明白Reward Model怎么训（附源码） |  |  | [blog](https://mp.weixin.qq.com/s/1v4Uuc1YAZ9MRr1UWMH9xw) |  

### LLM 幻觉

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 腾讯AILab等《大型语言模型中的幻觉》，全面阐述检测、解释和减轻幻觉 | [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://www.zhuanzhi.ai/paper/61ebe9c5007cf1373b900452ad52f0ae) | [code](https://github.com/HillZhang1999/llm-hallucination-survey) | [blog](https://mp.weixin.qq.com/s/GrN0FO_HrEk4GMYdJWJCMQ) |  

### LLM 可控性与安全

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 微软提出Control-GPT：用GPT-4实现可控文本到图像生成！ | [paper](https://arxiv.org/abs/2305.18583) |  |  [blog](https://mp.weixin.qq.com/s/U3eWeGOEt9nhW-Xwbuah9w)|  
| AIGC如何安全可控?中山大学等最新《AIGC中对隐私和安全的挑战及其补救措施：探索隐私计算、区块链潜在应用》全面阐述 | [paper](https://www.zhuanzhi.ai/paper/0dd95e1d5aae9eb2e60aabf36a107482) |  | [blog](https://mp.weixin.qq.com/s/V8QjMQSO2tX6PFx_LfLIEA) |  
| ControlVideo: 可控的Training-free的文本生成视频 | [paper](https://arxiv.org/pdf/2305.13077.pdf) | [code](https://github.com/YBYBZhang/ControlVideo) | [blog](https://mp.weixin.qq.com/s/CAH6u-MT3cFM359d5_Xpxg) |  
| 大模型切脑后变身PoisonGPT，虚假信息案例 |  | [code](https://colab.research.google.com/drive/16RPph6SobDLhisNzA5azcP-0uMGGq10R?usp=sharing&ref=blog.mithrilsecurity.io) | [blog](https://hub.baai.ac.cn/view/27736) |  
| ChatGPT羊驼家族全沦陷！CMU博士击破LLM护栏，人类毁灭计划脱口而出 | [paper](https://arxiv.org/abs/2307.15043) | [code](https://github.com/llm-attacks/llm-attacks) | [blog](https://mp.weixin.qq.com/s/298nwP98UdRNybV2Fuo6Wg) |  

### LLM 长文本解决方案

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| Transformer升级之路：一种全局长度外推的新思路 |  |  | [blog](https://mp.weixin.qq.com/s/YJ647EUfzWaJsGoMdgsguA) |  
| ChatGPT能写长篇小说了，ETH提出RecurrentGPT实现交互式超长文本生成 | [paper](https://arxiv.org/abs/2305.13304) | [code](https://github.com/aiwaves-cn/RecurrentGPT) | [blog](https://mp.weixin.qq.com/s/5UVTwSWgoz7uhozMeps3EQ)，[demo1](https://www.aiwaves.org/recurrentgpt (长篇小说写作))，[demo2](https://www.aiwaves.org/interactivefiction (交互式小说)) |  
| 语言大模型100K上下文窗口的秘诀 |  |  | [blog](https://mp.weixin.qq.com/s/_i0eQgYNSLJydv3qOTqr-Q) |  
| RoPE可能是LLM时代的Resnet |  |  | [blog](https://mp.weixin.qq.com/s/BVm1XC7r1yzOiWIrEbWg3A) |  

### LLM 问答

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 基于大语言模型的智能问答系统应该包含哪些环节？ |  | [OpenAI 的审核函数接口 Moderation API](https://platform.openai.com/docs/guides/moderation)  | [blog](https://mp.weixin.qq.com/s/pXEyFHEv1pcqwMNhveneew) |  
| 搭建本地的chatpdf（原理，文档处理，语义搜索等） |  |  | [blog](https://mp.weixin.qq.com/s/aW7r4i54coW26RMsTdAQ5g) |  
| 如何避免大语言模型绕过知识库乱答的情况？LlamaIndex 原理与应用简介 |  |  | [官方blog](https://betterprogramming.pub/llamaindex-how-to-use-index-correctly-6f928b8944c6)，[中文blog](https://mp.weixin.qq.com/s/D6_pUv7hHZHRrKSXqo0u2w) |  
| 使用 Langchain 和 Azure OpenAI 构建一个聊天机器人来查询您的文档 |  |  | [blog](https://mp.weixin.qq.com/s/LeUuq6O5uIJPmrrYYtTaqA) |  
| 一文搞懂LangChain是什么 |  |  | [blog](https://mp.weixin.qq.com/s/vLlS17AYe4lM95KrG5sFyQ) |  

### LLM Agent

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| AutoGPT |  |  |  |  
| BabyAGI |  |  |  |  
| ChatRPA |  |  |  |  
| Generative Agents |  |  |  |  
| GPT-Engineer |  |  |  |  
| HuggingGPT |  |  |  |  
| MetaGPT |  | [code](https://github.com/geekan/MetaGPT) |  | 
| NexusGPT |  |  |  |  
| Toolformer |  |  |  |  

### LLM 文本检测

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 美国麻省大学&谷歌研究院：改写文本可以避开AI生成文本的检测器，但检索则是一种有效的防御 | [paper](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=2cfe8cecc9f211edb95839eec3084ddd) | [code](https://github.com/martiansideofthemoon/ai-detection-paraphrases) |  |  
| 人工智能生成的文本能被可靠地检测出来吗？ | [paper](https://arxiv.org/pdf/2303.11156.pdf) |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486128&idx=3&sn=e5ea32b7d7cb4c8c41f29a9ea15ac3ac&chksm=ced54954f9a2c0425a65761f1766550f6b90857da0106f6fd55f3c6773fbdbd1fc45bbb9369a&token=447941009&lang=zh_CN#rd) |  
| DetectGPT（斯坦福大学）：利用概率曲率检测文本是否大模型生成 | [paper](https://arxiv.org/abs/2301.11305) | [code&data](https://ericmitchell.ai/detectgpt/) | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485713&idx=2&sn=805caf25603cf15dbf71949f85b9d041&chksm=ced54af5f9a2c3e3e0dffd728592fd7ab8f738869e94240daba4fad9f6ac90a2f76a6b458e3f&token=447941009&lang=zh_CN#rd) |  
| Detecting LLM-Generated-Text综述 | [paper](https://github.com/datamllab/The-Science-of-LLM-generated-Text-Detection) |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485747&idx=1&sn=5e5029a70c54c08f6f8c40631962b1e1&chksm=ced54ad7f9a2c3c184ccb123199510bb09470e054fb5cb887e70bac204927b65e296f8921db1&token=447941009&lang=zh_CN#rd) |  
| 一个专为**教育**者打造的全新 AI 检测模型 |  |  | [blog](https://gptzero.substack.com/p/gptzerox) |  
| OpenAI重磅发布官方「ChatGPT检测器」 |  |  | [blog](https://mp.weixin.qq.com/s/EcZE7TgHspf22rPRWhAybw) |  
| 斯坦福最新研究：不要过度依赖GPT生成内容，其检测器可能存在不利于非母语英语写作者的偏见 | [paper](https://arxiv.org/abs/2304.02819) |  |  |  

### LLaMA

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| LLaMA评测 |  |  | [blog](https://mp.weixin.qq.com/s/kImwfWWtXMmEDVOhJZ4dJg) | 



