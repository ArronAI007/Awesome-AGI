# Awesome-AGI

[![Anurag's GitHub stats](https://github-readme-stats.vercel.app/api?username=ArronAI007)](https://github.com/anuraghazra/github-readme-stats)

## æŠ€æœ¯äº¤æµ

æ¬¢è¿åŠ å…¥AIGCæŠ€æœ¯äº¤æµç¾¤ï¼Œä¸AIé¢†åŸŸä¸“å®¶å’Œå„è¡Œå„ä¸šçš„AIGCçˆ±å¥½è€…ä¸€èµ·äº¤æµæŠ€æœ¯ç†è®ºä¸è¡Œä¸šä¿¡æ¯ï¼ä¸ç®¡ä½ æ˜¯å­¦æœ¯ç•Œè¿˜æ˜¯å·¥ä¸šç•Œå®è·µè€…æˆ–çˆ±å¥½è€…ï¼Œéƒ½æ¬¢è¿åŠ å…¥ç¾¤ä½“ï¼

| äº¤æµç¾¤äºŒç»´ç                     | æ‹‰ä½ å…¥ç¾¤(å¤‡æ³¨AIGC-github)  |
| ------------------------------- | :------------------------: |
| ![Arron](https://i.postimg.cc/PqFvY1kW/AIGC-group.jpg) | ![Arron](https://i.postimg.cc/QMqj1DGc/Arron.jpg) |

## Table of Context
- [LLM ä½“éªŒæ•ˆæœ](#LLM-ä½“éªŒæ•ˆæœ)
- [LLM Lists](#LLM-Lists)
  - [baichuan and extensions](#baichuan-and-extensions)
  - [ChatGLM and extensions](#ChatGLM-and-extensions)
  - [LLaMA and extensions](#LLaMA-and-extensions)
- [Universal LLMs](#Universal-LLMs)
  - [Universal LLMs for Text](#Universal-LLMs-for-Text)
  - [Universal LLMs for Code](#Universal-LLMs-for-Code)
  - [Universal LLMs for Picture/Video](#Universal-LLMs-for-Picture/Video)
  - [Universal LLMs for Audio](#Universal-LLMs-for-Audio)
  - [Universal LLMs for Multimodal](#Universal-LLMs-for-Multimodal)
- [Domain LLMs](#Domain-LLMs)
  - [Domain LLMs for Law](#Domain-LLMs-for-Law)
  - [Domain LLMs for Medical](#Domain-LLMs-for-Medical)
  - [Domain LLMs for Finance](#Domain-LLMs-for-Finance)
  - [Domain LLMs for Environment](#Domain-LLMs-for-Environment)
  - [Domain LLMs for Network Security](#Domain-LLMs-for-Network-Security)
  - [Domain LLMs for Education](#Domain-LLMs-for-Education)
  - [Domain LLMs for Traffic](#Domain-LLMs-for-Traffic)
  - [Domain LLMs for Other](#Domain-LLMs-for-Other)
- [LLM Deployment](#LLM-Deployment)
- [LLM Agent](#LLM-Agent)
- [AGI Paper Lists](#AGI-Paper-Lists)

## LLM ä½“éªŒæ•ˆæœ

| Model_A| Model_B | Blog |
| --- | --- | --- |                                                                                                                     
| 360æ™ºè„‘ | è®¯é£æ˜Ÿç« | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486609&idx=2&sn=7fedb8ab37588d43968fdec2d7e5fcdd&chksm=ced54f75f9a2c663b9a2671f2548e2940730735605356cc0ffe72bc737470136a40032c80bfe&token=1282379489&lang=zh_CN#rd)|
| é˜¿é‡Œé€šä¹‰åƒé—® | è®¯é£æ˜Ÿç« | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486534&idx=1&sn=6f36d41b618790cba62e63eb25bb033b&chksm=ced54fa2f9a2c6b4a901528f87a7e74628dfd79d835f4cdea1ee4dea442f339adfd2736b2305&token=1282379489&lang=zh_CN#rd)|
| Bard | Bing_VS_ChatGPT | [å¯¹æ¯”æ•ˆæœ](https://www.theverge.com/2023/3/24/23653377/ai-chatbots-comparison-bard-bing-chatgpt-gpt-4)|
| æ–‡å¿ƒä¸€è¨€ | Bard | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486260&idx=1&sn=a41224fee7ed4cb4a48eb40a420d7479&chksm=ced548d0f9a2c1c6f4930f30447468f9f01bb2af6031368e302b13a6354fc4bca6636e3b297e&token=666852558&lang=zh_CN#rd)|
| æ–‡å¿ƒä¸€è¨€ | Baize-7B | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486317&idx=1&sn=ea3cc745d2991b8c657325392ce68f71&chksm=ced54889f9a2c19f3c2f85d8d7af7fff366027f79d1f4a5b2c650fea1b5dee9efde0b7c992ca&token=1173964254&lang=zh_CN#rd)|
| æ–‡å¿ƒä¸€è¨€ | OpenAssistant | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486413&idx=2&sn=3816e5a4bccceee5e2af868166b18897&chksm=ced54829f9a2c13fb787b7a7e3c2aa0799eb7ff6d124f6847349346146900e05684ceb8cc7f7&token=1282379489&lang=zh_CN#rd)|
| æ–‡å¿ƒä¸€è¨€ | ChatGLM-6B | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486081&idx=2&sn=fd87305419158d66dd4b05b57bee1324&chksm=ced54965f9a2c073ba1badfedbc6610036455cd769a3c8ee3445f7fbff9364b5624091be9914&token=666852558&lang=zh_CN#rd)|
| æ–‡å¿ƒä¸€è¨€ | GPT-4 | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s/l1pTPlohMmiYEMc4x6QKhw)|
| æ–‡å¿ƒä¸€è¨€ | GPT-4å®æµ‹ | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s/uO8N3RpcrYU8rV1RkwBxzQ)|
| æ–‡å¿ƒä¸€è¨€ | è®¯é£æ˜Ÿç« | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486490&idx=1&sn=c8d756f7f26a4e35f8b67ae485efabce&chksm=ced54ffef9a2c6e8d66f8b744d6af524e320d5aec384d142621cee53fd2150f2c7db1fa7596a&token=1282379489&lang=zh_CN#rd)|
| GPT4 | ChatGPT | [å¯¹æ¯”æ•ˆæœ](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485952&idx=2&sn=e54a62e358bf7aee3c007d59600fd452&chksm=ced549e4f9a2c0f2868eb8877c14fbe287a469e63b09774cefcb9edc4c0601016f6d36561973&token=666852558&lang=zh_CN#rd)|
| GPT4 | Claude2 | [å¯¹æ¯”æ•ˆæœ1](https://mp.weixin.qq.com/s/dj2_WlWVpGwYsa8kO-GRFQ)ï¼Œ[å¯¹æ¯”æ•ˆæœ2](https://mp.weixin.qq.com/s/Xo3XXQ5zYPmDxBYivhBYqA)|

## LLM Lists

ä»GPT3åˆ°ChatGPTæ¨¡å‹çš„å‘å±•è·¯çº¿å›¾

![ChatGPT_family](https://i.postimg.cc/GtZmmjG2/chatgpt-3.jpg)

### baichuan and extensions

| Target Model | Source Model | Optimization | Checkpoints | Paper/Blog | Params (B) | Context Length | Code | Tokens | Tokenizer | Vocabulary | Position Embedding | Layer Normalization | Activation Function | Attention |
| --- | --- | --- | --- |--- | --- | --- |--- | --- | --- | --- | --- | --- | --- | --- | 
| baichuan-7b |  | [Model Scope](https://modelscope.cn/models/baichuan-inc/baichuan-7B/summary) | [blog](https://mp.weixin.qq.com/s/qA_E_3dUe1sSOUM87ZgHdQ)ï¼Œ[C-EVAL](https://cevalbenchmark.com/static/leaderboard_zh.html) |  | 7ï¼Œ13ï¼Œ53ï¼ˆç”¨äºæœç´¢ï¼‰ | 4096 | [baichuan-7b Code](https://github.com/baichuan-inc/baichuan-7B)ï¼Œ[baichuan-7b Chat](https://huggingface.co/baichuan-inc/baichuan-7B) |1.2T | BPE | 64000 | | Pre RMS Norm |  SwiGLU |
| baichuan-13b |  |  | [blog](https://mp.weixin.qq.com/s/Px4h2r3VIAFI5vfjXxROxg)ï¼Œ[ç™¾å·å¤§æ¨¡å‹ã€Baichuan-13Bã€‘ å¤šå¡è®­ç»ƒå¾®è°ƒè®°å½•](https://mp.weixin.qq.com/s/EUZA6Lt-OcI170md9lXH1g) | 
| fireballoon/baichuan-vicuna-chinese-7b | baichuan-7b |  |  | | |  | |  |  |  |  |  |  |  | 
| fireballoon/baichuan-vicuna-7b | baichuan-7b |  |  | | |  | |  |  |  |  |  |  |  | 
| firefly-baichuan-7b-qlora-sft | baichuan-7b | [code](https://github.com/baichuan-inc/baichuan-7B) | [blog](https://mp.weixin.qq.com/s/_eTkDGG5DmxyWeiQ6DIxBw)ï¼Œ[Hugging Face model](https://huggingface.co/YeungNLP/firefly-baichuan-7b-qlora-sft)ï¼Œ[Model Scope](https://modelscope.cn/models/baichuan-inc/baichuan-7B/summary)ï¼Œ[C-EVAL](https://cevalbenchmark.com/static/leaderboard_zh.html) | 
| baichuan-13b-Chat |  | [code](https://github.com/percent4/document_qa_with_llm) | [blog](https://mp.weixin.qq.com/s/wStOyHPd8c7V0ug1Qebryw) |  
| Baichuan2 |  |  |  | [Baichuan2æŠ€æœ¯æŠ¥å‘Š](https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf)ï¼Œ[SuperCLUEè¯„æµ‹æ•ˆæœ](https://mp.weixin.qq.com/s/SV7COWNu9uGnpOBzVYCyog) | 7ï¼Œ13 |  | [Baichuan2 Code](https://github.com/baichuan-inc/Baichuan2) | 2.6T |  |  |  |  |  |  | 

### ChatGLM and extensions

| Model/Description| Paper | Code | Blog | Tokens |  Tokenizer | Vocabulary | Position Embedding | Layer Normalization | Activation Function | Attention |
| --- | --- | --- | --- |--- | --- | --- |--- | --- | --- | --- | 
| ChatGLM-6B |  | [code](https://github.com/THUDM/ChatGLM-6B.git) | [blog](https://chatglm.cn/blog)ï¼Œ[ChatGLM-6Bæºç é˜…è¯»](https://mp.weixin.qq.com/s/r7KEJmrpJZmY7KBP4veS6A)ï¼Œ[ChatGLMæ¨¡å‹åº•åº§ç»†èŠ‚åˆ†æ](https://mp.weixin.qq.com/s/oOdD3MYtE6-sNeAmPthqLg) | 1T | SentencePiece | 130528 | | Post Deep Norm | GeLU |
| chatglm+langchain+äº’è”ç½‘ |  | [code](https://github.com/LemonQu-GIT/ChatGLM-6B-Engineering/) | [blog](https://mp.weixin.qq.com/s/lO6SrEuv4-vNbL8B3G-f8g) | 
| ChatGLM_multi_gpu_zero_Tuning |  | [code](https://github.com/CSHaitao/ChatGLM_mutli_gpu_tuning) |  | 
| ChatGLM+Fastapi |  |  | [blog](https://mp.weixin.qq.com/s/5J4UA4ePVZGXJGZsBXeN8Q) | 
| ChatGLM2-6B-32K |  |  | [blog](https://mp.weixin.qq.com/s/Fkm_D26z1jrqA44B82v7Ww) | 1.4T |  | 65024 | | Post RMS Norm | SwiGLU | GQA |
| ChatGLM-6b+langchain |  | [code](https://github.com/yanqiangmiffy/Chinese-LangChain) | [blog](https://mp.weixin.qq.com/s/xAsZZ_LOkr9Nj-JafSbXnA) | 
| one-shotå¾®è°ƒchatglm-6bå®è·µä¿¡æ¯æŠ½å– |  |  | [blog](https://mp.weixin.qq.com/s/l7lCbdJ9XGzLPTb3zKDAzQ) | 
| Falcon |  |  | [blog1](https://mp.weixin.qq.com/s/jbRRjG2ferhFPWsMtCaJyg)ï¼Œ[blog2](https://mp.weixin.qq.com/s/Vy_xWBuZU0AaaPMCIhKIyw) | 1.5T |  | 65024 | | Pre LN | GeLU | MQA |

### LLaMA and extensions

| Target Model | Source Model | Optimization | Checkpoints | Paper/Blog | Params (B) | Context Length | Code | Tokens | Tokenizer | Vocabulary | Position Embedding | Layer Normalization | Activation Function | Attention |
| --- | --- | --- | --- |--- | --- | --- |--- | --- | --- | --- | --- | --- | --- | --- | 
| LLaMA | |  |  | [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)ï¼Œ[blog1](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485822&idx=1&sn=b365d93a0a08769aef77f34069da1422&chksm=ced54a9af9a2c38cd5779284b5e9ae573846153e7dc00961dc163664a657d6a3fa5c8c14c7d2&token=447941009&lang=zh_CN#rd)ï¼Œ[blog2](https://mp.weixin.qq.com/s/fGNuTcYE8QI9_JKS9LcQ7w)ï¼Œ[è¯¦èŠLLaMAå¤§æ¨¡å‹çš„æŠ€æœ¯ç»†èŠ‚](https://mp.weixin.qq.com/s/B9Ue0ihUGAFjT_X__R2u8Q) | 7ï¼Œ13ï¼Œ33ï¼Œ65 | 512 | [LLaMA Code](https://github.com/facebookresearch/llama) | 1T/1.4T | BPE | 32000 | RoPE | Pre RMS Norm | SwiGLU |
| LLaMA 2 |  | | [[åœ¨ Hugging Face ä¸Šç©è½¬LLaMA 2](https://mp.weixin.qq.com/s/UnzhBJjZfPXsaSu8gNnosw)] ï¼Œ[[åœ¨Colabç¬”è®°æœ¬ä¸­å¾®è°ƒè‡ªå·±çš„Llama 2æ¨¡å‹](https://mp.weixin.qq.com/s/pnDJaOUh_xdNdqSBl53Arw)]ï¼Œ[[ä¸‰æ­¥ä¸Šæ‰‹ LLaMA2](https://mp.weixin.qq.com/s/lkRg8-rw57wDNr7FrjOSOQ)]ï¼Œ[[ä½¿ç”¨ Transformers é‡åŒ– Meta AI LLaMA2 ä¸­æ–‡ç‰ˆå¤§æ¨¡å‹](https://mp.weixin.qq.com/s/DEgFNAB4gwWDlQOj7-2CEg)] | [[blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486800&idx=1&sn=9b629ca41b9f6b4feedad94363a17253&chksm=ced54eb4f9a2c7a2a5b20c182981b4323b18509f2ca8f482c2a8cdbb29bf570488bdcd280eb6&token=882149695&lang=zh_CN#rd)]ï¼Œ[[ä¼¯å…‹åˆ©AIåšå£«è¯¦è§£Llama 2çš„æŠ€æœ¯ç»†èŠ‚](https://mp.weixin.qq.com/s/Mee7sMq_bxLpIOOr91li9A)]ï¼Œ[[NLPç¤¾åŒºå¯¹LLaMA2è®ºæ–‡ä¸ŠåŠéƒ¨åˆ†çš„è®¨è®º](https://mp.weixin.qq.com/s/SJNqjSCBX-k80_r3nmTiuA)]ï¼Œ[[NLPä¸­æ–‡ç¤¾åŒºé¡¶å°–ç ”ç©¶å‘˜ä»¬å¯¹LLaMA2è®ºæ–‡ä¸‹åŠéƒ¨åˆ†çš„è®¨è®º](https://mp.weixin.qq.com/s/6k5ML3HtmvBTTCgHBZGycQ)]ï¼Œ[[3ä¸ªæœ€å€¼å¾—äº†è§£llama2å¼€å‘åº“ï¼ŒåŠ©ä½ å¿«é€Ÿæ­å»ºLLMåº”ç”¨](https://mp.weixin.qq.com/s/_3H6Y_NolUuxYxOo8Pl7fg)]ï¼Œ[[ä½¿ç”¨ Docker å¿«é€Ÿä¸Šæ‰‹ä¸­æ–‡ç‰ˆ LLaMA2 å¼€æºå¤§æ¨¡å‹](https://mp.weixin.qq.com/s/9cTNa_oya2Zj9YdDYodCvw)]ï¼Œ[[ Llama 2èµ„æ–™æ±‡æ€»](https://mp.weixin.qq.com/s/-01Dg9ZVfPYM4mZ4iKt8Cw) |  |  |  | 2T |  |  |  |  |SwiGLU | GQA |
| Alpaca | LLaMA 7B |  |  | [Alpaca blog](https://crfm.stanford.edu/2023/03/13/alpaca.html)ï¼Œ[Alpaca homepage](https://crfm.stanford.edu/alpaca) | 7ï¼Œ13 | 512 | [Alpaca Code](https://github.com/tatsu-lab/stanford_alpaca) |
| Alpaca-Lora | LLaMA 7B |  |  |  | 7 |  | [Alpaca-Lora Code](https://github.com/tloen/alpaca-lora) |
| AlpaGasus | Alpaca |  |  | [AlpaGasus: Training A Better Alpaca with Fewer Data](https://arxiv.org/abs/2307.08701)ï¼Œ[blog](https://mp.weixin.qq.com/s/UroGj4rIa2nOw6DookpvCQ) |  |  | [AlpaGasus Code](https://lichang-chen.github.io/AlpaGasus/) |
| Alpaca-CoT | Alpaca |  |  | [å®˜ç½‘](https://sota.jiqizhixin.com/project/alpaca-cot) |  |  |  [Alpaca-CoT Code](https://github.com/PhoebusSi/Alpaca-CoT)|
| Anima | guanaco-33B |  | [Anima-33B](https://huggingface.co/lyogavin/Anima33B) | [Anima Blog](https://zhuanlan.zhihu.com/p/638058537?utm_source=wechat_session&utm_medium=social&s_r=0) |  |  | [Anima Code](https://github.com/lyogavin/Anima) |
| Baize | LLaMA |  | [baize-lora-7B](https://huggingface.co/spaces/project-baize/baize-lora-7B) | [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://arxiv.org/abs/2304.01196)ï¼Œ[blog](https://mp.weixin.qq.com/s/zxElGfclNbBwTuDG4Qrxnw) | 7B | 512 | [Baize Code](https://github.com/project-baize/baize-chatbot) |
| BELLE | Alpaca |  |  |  | 7B |  | [BELLE Code](https://github.com/LianjiaTech/BELLE) |
| BiLLa | LLaMA |  |  | [BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability](https://mp.weixin.qq.com/s/8KDpDC6Fkb_61gFfkcT8TQ) | 7B |  | [BiLLa Code](https://github.com/Neutralzz/BiLLa) |
| CaMA | LLaMA |  |  |  |  |  | [CaMA Code](https://github.com/zjunlp/CaMA) |
| ChatLLaMA | LLaMA | RLHF |  |  |  |  | [ChatLLaMA Code](https://github.com/nebuly-ai/nebuly/tree/main/optimization/chatllama) |
| Chinese-LlaMA2 | Llama-2 | SFT |  |  |  |  | [Chinese-LlaMA2 Code](https://github.com/michael-wzhu/Chinese-LlaMA2) |
| Chinese-Llama-2 | Llama-2 | LoRA/FPFT |  |  |  |  | [Chinese-Llama-2 Code](https://github.com/longyuewangdcu/Chinese-Llama-2) |
| Chinese-Vicuna | LLaMA | LoRA |  |  |  |  | [Chinese-Vicuna Code](https://github.com/Facico/Chinese-Vicuna) |
| ColossalChat | LLaMA | RLHF |  | [blog](https://syncedreview.com/2023/03/29/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline/) |  |  | [ColossalChat Code](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) |
| CAMEL | LLaMA |  |  | [blog](https://starmpcc.github.io/CAMEL/) |  |  | [CAMEL Code](https://github.com/starmpcc/CAMEL) |
| è‰æœ¬ï¼ˆåŸåé©¼ï¼‰ |  LLaMA | SFT |  | [HuaTuo (åé©¼): Tuning LLaMA Model with Chinese Medical Knowledge](https://arxiv.org/pdf/2304.06975v1.pdf)ï¼Œ[blog1](https://mp.weixin.qq.com/s/TYpc_63qDlR6MwscxCKKhA)ï¼Œ[blog2](https://mp.weixin.qq.com/s/iuQANmwCS7AXQRik7HwQPg) |  |  | [è‰æœ¬ï¼ˆåŸåé©¼ï¼‰ Code](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) |
| ExpertLLaMA | LLaMA |  |  | [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688)ï¼Œ[ExpertLLaMA demo](https://huggingface.co/spaces/OFA-Sys/expertllama) | 7B |  | [ExpertLLaMA Code](https://github.com/OFA-Sys/ExpertLLaMA) |
| FreedomGPT | LLaMA |  |  | [FreedomGPT homepage](https://freedomgpt.com/) |  |  | [FreedomGPT Code](https://github.com/ohmplatform/FreedomGPT) |
| GoGPT/GoGPT2 |  |  | [GoGPT2-7B](https://huggingface.co/golaxy/gogpt2-7b) |  |  |  | [GoGPT code](https://github.com/yanqiangmiffy/GoGPT) |
| guanaco | LLaMA |  |  |  |  |  |  |
| Koala | LLaMA-13B |  | [Koala model](https://drive.google.com/drive/folders/10f7wrlAFoPIy-TECHsx9DKIvbQYunCfl) | [ä¸­æ–‡blog](https://hub.baai.ac.cn/view/25284)ï¼Œ[è‹±æ–‡blog](https://bair.berkeley.edu/blog/2023/04/03/koala/) | 13B | 512 | [Koala Chat](https://chat.lmsys.org/?model=koala-13b)ï¼Œ[EasyLM Koala](https://github.com/young-geng/EasyLM)ï¼Œ[training data preprocessing](https://github.com/young-geng/koala_data_pipeline) | ChatGPT Distillation Data:[ShareGPT](https://sharegpt.com/)ï¼Œ[HC3](https://arxiv.org/abs/2301.07597)ï¼› Open Source Data:[OIG](https://laion.ai/blog/oig-dataset/)ï¼Œ[Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)ï¼Œ[Anthropic HH](https://huggingface.co/datasets/Anthropic/hh-rlhf)ï¼Œ[OpenAI WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)ï¼Œ[OpenAI Summarization](https://huggingface.co/datasets/openai/summarize_from_feedback)ï¼›[koala-test-set](https://github.com/arnav-gudibande/koala-test-set) |
| LLaMA-2 & Alpaca-2 | Llama-2ï¼ŒLLaMA&Alpaca | ï¼ˆ1ï¼‰æ‰©å……ä¸­æ–‡è¯è¡¨ï¼Œï¼ˆ2ï¼‰FlashAttention-2ï¼Œï¼ˆ3ï¼‰åŸºäºNTKçš„è‡ªé€‚åº”ä¸Šä¸‹æ–‡æ‰©å±•æŠ€æœ¯ï¼Œï¼ˆ4ï¼‰ç®€åŒ–çš„ä¸­è‹±åŒè¯­ç³»ç»Ÿæç¤ºè¯­ |  | [blog](https://mp.weixin.qq.com/s/sJ_imBdHCD4NibVy58EO2w) |  |  | [LLaMA-2 & Alpaca-2 Code](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) |
| LIMA |  |  |  | [blog](https://mp.weixin.qq.com/s?search_click_id=7213828026277652651-1688375605291-1083947599&__biz=MjM5ODExNDA2MA==&mid=2449961473&idx=2&sn=f080fa7b1b5657db9872724caee56519&chksm=b13c7462864bfd741f0f061b87187f2cde36b68020cfe3402717a6858563311cb642eb340989&rd2werd=1&key=ea1d916ce49bb536ce48f3aba8e329d1e1aa6fdcda4f73580b0a5adbd624721e6a974570fd6ef2823ecfa6c95e2dc09179b51e440e9179f79d0ba01f62cf795d6c697f95bf05a28904f4172b11e1ce873a2d7a0e85c74d509e916176aacb43657fd11a6de7611d65bd4ae82315835aa138a423887a219f2971c6a525679fd805&ascene=65&uin=MTkwNzA5OTA4Mw%3D%3D&devicetype=iMac+MacBookPro13%2C2+OSX+OSX+12.6.7+build(21G651)&version=13080109&nettype=WIFI&lang=zh_CN&countrycode=CN&fontScale=100&exportkey=n_ChQIAhIQ0Z339%2BFUk%2Bp0YpfMQjB%2BhxKDAgIE97dBBAEAAAAAANJyNCKr%2F3UAAAAOpnltbLcz9gKNyK89dVj0q5AacL9r2sPbvlDuJo6SwYSJ2wbfYGvc3EDxuk%2BMQS0vl8RLluMN%2Fuh9u2LxBZTHTiuQct62Bjib68qd1EvB8CgGKMV34B5%2BKHCutInPzdE9Uac6dxp0VYtd%2BJnEwljL8jf7mWZdwTkPdEZl1P0OEb3HFzczXelqDR3h7D2xEVmQuFHGIeVi7iPOHMT0AWhhGLdbrVhCKbPT3%2BX9FPOLjJSql2UD95dTmSzZKqdvOIMGpD5t%2F98jDuMUojr9HUMdvljQ1XkiJVnd%2FbqSsLS3S5t7E%2Ftjmjb9g7IxWkY%3D&acctmode=0&pass_ticket=mJ3t3nBN%2BXhKCYp9bzJSkTl%2B9PwobzvYen%2F5Kv4kpcj1Lig98d0DXbcAyqBW0vaB&wx_header=0) |  |  |  |
| LongLLaMA |  |  | [LongLLaMA-3B](https://huggingface.co/syzymon/long_llama_3b) | [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/pdf/2307.03170.pdf)ï¼Œ[blog](https://mp.weixin.qq.com/s/K8ExTUUXDruZGwr-PA4oFQ) |  |  | [LongLLaMA Code](https://github.com/CStanKonrad/long_llama) |
| OpenBuddy-LLaMA1-30B | LLaMA |  | [ModelScope demo](https://modelscope.cn/models/OpenBuddy/openbuddy-llama-65b-v8-bf16/summary) | [blog](https://mp.weixin.qq.com/s/k-ZWg0Vuud3Atn3aaXBaCQ) | 13ï¼Œ33ï¼Œ65 |  |  |
| Platypus | LLaMA2 |  |  | [Platypus: Quick, Cheap, and Powerful Refinement of LLMs](https://arxiv.org/abs/2308.07317)ï¼Œ[blog](https://mp.weixin.qq.com/s/yzfgITUWaCf3Wcdc6lGCQA) |  |  |  |
| StackLLaMA | LLaMA | SFT/RM/RLHF |  | [ä¸­æ–‡blog](https://hub.baai.ac.cn/view/25341)ï¼Œ[è‹±æ–‡blog](https://huggingface.co/blog/stackllama) |  |  |  |
| Vicuna | Alpaca |  |  | [Vicunaå®˜ç½‘åœ°å€](https://vicuna.lmsys.org/)ï¼Œ[blog](https://hub.baai.ac.cn/view/25328) | 7ï¼Œ13 | 2k | [Vicuna Code](https://github.com/lm-sys/FastChat) |
| Vicuna v1.5 | Llama 2 |  |  | [Vicunaå®˜ç½‘åœ°å€](https://vicuna.lmsys.org/)ï¼Œ[blog](https://hub.baai.ac.cn/view/25328) | 7ï¼Œ13 | 4kï¼Œ16k | [Vicuna v1.5 Code](https://github.com/lm-sys/FastChat) |

## Universal LLMs

### Universal LLMs for Text

Some examples of **Universal LLMs for Text** as followsï¼š

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| Bard | 2023/03 |  | [Bard homepage](http://bard.google.com/)ï¼Œ[theverge blog](https://www.theverge.com/23649897/google-Bard-chatbot-search-engine) | | | | |
| Bloom | 2022/11 | [Bloom](https://huggingface.co/bigscience/bloom) | [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100) | 176 | [2048](https://huggingface.co/bigscience/bloom) |  [OpenRAIL-M v1](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) | [Bloom Code](https://github.com/huggingface/transformers-bloom-inference/tree/main)                                                                                                                     |
| Cerebras-GPT | 2023/03 | [Cerebras-GPT](https://huggingface.co/cerebras)                                           | [Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) ([Paper](https://arxiv.org/abs/2304.03208)) | 0.111 - 13      | [2048](https://huggingface.co/cerebras/Cerebras-GPT-13B#model-details) | Apache 2.0         | [Cerebras-GPT-1.3B](https://github.com/slai-labs/get-beam/tree/main/examples/cerebras-gpt)                            |
| ChatYuan |  | [ChatYuan-large-v1](https://huggingface.co/ClueAI/ChatYuan-large-v1)  | [ChatYuan Blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485655&idx=1&sn=ad80d8a17d4aaab90b17a79b638c712d&chksm=ced54b33f9a2c225ce292b4e3d5725a668d0bfc9fe0be610c847b31b61714ecf75c06dac1cb5&token=447941009&lang=zh_CN#rd) | |  |  | [ChatYuan Code](https://github.com/clue-ai/ChatYuan) |
| ChatDB |  |  | [ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory](https://arxiv.org/abs/2306.03901)ï¼Œ[blog](https://mp.weixin.qq.com/s/o3j1vNLHlJ6qTea219A4Qw)ï¼Œ[ä¸»é¡µ](https://chatdatabase.github.io) |  |  |   | [ChatDB Code](https://github.com/huchenxucs/ChatDB) |
| Claude |  |  | [äº§å“åœ°å€](https://www.anthropic.com/product)ï¼Œ[ç”³è¯·åœ°å€](https://www.anthropic.com/earlyaccess)ï¼Œ[blog](https://mp.weixin.qq.com/s/Wx5q-rEwG4sROvnewGxWrw)ï¼Œ[Claudeæ”¯æŒ100kä¸Šä¸‹æ–‡](https://mp.weixin.qq.com/s/Yu551-z14lpiFGSOfXE2Tw) | 
| Claude 2 |  |  | [Model Card and Evaluations for Claude Models](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf) |
| ColossalAI |  |  | [ColossalAI SFT Blog](https://mp.weixin.qq.com/s/NS4yySeYd7QUYb7CB9V0lA) |  |  |  | [ColossalAI Code](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) | 
| CPM-Bee |  |  [CPM-Bee-10B](https://huggingface.co/openbmb/cpm-bee-10b)| [CPM-Bee Blog](https://mp.weixin.qq.com/s/BO4cDB9KRSODZw3TvZpUAA) |  |  |  | [CPM-Bee Code](https://github.com/OpenBMB/CPM-Bee) |
| Dolly        | 2023/04 | [dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b)                            |  [Hello Dolly: Democratizing the magic of ChatGPT with open models](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)ï¼Œ[Databricks Incä¸»é¡µ](https://www.databricks.com)            | 3, 7, 12     | [2048](https://github.com/databrickslabs/dolly#dolly) | MIT                |                                                                                                                       |
| Dolly2.0 |  |  | [Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)ï¼Œ[ä¸­æ–‡blog](https://hub.baai.ac.cn/view/25434) | 
| DeepSpeed-Chat |  | [code](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat) | [blog](https://hub.baai.ac.cn/view/25414) | 
| DLite | 2023/05 | [dlite-v2-1_5b](https://huggingface.co/aisquared/dlite-v2-1_5b) | [Announcing DLite V2: Lightweight, Open LLMs That Can RunÂ Anywhere](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e) | 0.124 - 1.5 | [1024](https://huggingface.co/aisquared/dlite-v2-1_5b/blob/main/config.json) | Apache 2.0         | [DLite-v2-1.5B](https://github.com/slai-labs/get-beam/tree/main/examples/dlite-v2)                                    |
| FastChat-T5 | 2023/04 | [fastchat-t5-3b-v1.0](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0) | [We are excited to release FastChat-T5: our compact and commercial-friendly chatbot!](https://twitter.com/lmsysorg/status/1652037026705985537?s=20) | 3 | [512](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0/blob/main/config.json) | Apache 2.0 |                                                                                                                       |
| GPT3.5 |  |  | [OpenAI playground](https://platform.openai.com/playground) | 
| MiniGPT-4 |  |  | [MiniGPT-4 Paper](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf)ï¼Œ[ä¸»é¡µ](https://minigpt-4.github.io/)ï¼Œ[video](https://youtu.be/__tftoxpBAw)ï¼Œ[dataset](https://drive.google.com/file/d/1nJXhoEcy3KTExr17I7BXqY5Y9Lx_-n-9/view)ï¼Œ[Demo](https://6b89c70eb5e14dca33.gradio.live/)ï¼Œ[Demo1](https://b2517615b965687635.gradio.live/)ï¼Œ[Demo2](https://c8de8ff74b6a6c6a9b.gradio.live/)ï¼Œ[Demo3](https://0a111504e072685259.gradio.live/)ï¼Œ[Demo4](https://90bc0bac96e6457e8f.gradio.live/) |  |  |  | [MiniGPT-4 Code](https://github.com/Vision-CAIR/MiniGPT-4) |
| MOSS |  |  | [Secrets of RLHF in Large Language Models Part I: PPO](https://arxiv.org/abs/2307.04964)ï¼Œ[MOSS Blog](https://mp.weixin.qq.com/s/BjXtnEEVCQiPOy-_qCNM4g)ï¼Œ[æ•°æ®é›†](https://laion.ai/blog/oig-dataset/) |  |  |  | [MOSS Code](https://openlmlab.github.io/MOSS-RLHF/) |
| OpenAssistant |  |  | [OpenAssistant Conversations - Democratizing Large Language Model Alignment](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view)ï¼Œ[å®˜ç½‘](https://open-assistant.io/chat)ï¼Œ[dataset](https://huggingface.co/datasets/OpenAssistant/oasst1)ï¼Œ[youtube](https://youtu.be/ddG2fM9i4Kk) |  |  |  | [OpenAssistant Code](https://github.com/LAION-AI/Open-Assistant) |
| OpenChatKit |  | | [OpenChatKit Blog](https://mp.weixin.qq.com/s/9Av3nhJLrcYAsBW9vVGjTw)ï¼Œ[OpenChatKit Chat](https://huggingface.co/spaces/togethercomputer/OpenChatKit)  |  |  |  | [OpenChatKit Code](https://github.com/togethercomputer/OpenChatKit) |
| WebCPM |  |  | [WebCPM: Interactive Web Search for Chinese Long-form Question Answering](https://arxiv.org/abs/2305.06849)ï¼Œ[WebCPM Blog](https://mp.weixin.qq.com/s/m4zsF2HDFHSKc23Oq0O98w) |  |  |  | [WebCPM Code](https://github.com/thunlp/WebCPM) |

**Complete Content**: please refer to [Model Survey](https://github.com/ArronAI007/Awesome-AGI/tree/main/Model-Survey/README.md)

---

### Universal LLMs for Code  

Some examples of **Universal LLMs for Code** as followsï¼š

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length                                                                         | Licence | Code                                                                                    |
| --- | --- | --- | --- | --- |----------------------------------------------------------------------------------------| --- |-------------------------------------------------------------------------------------------|
| Baldur |  |  | [Baldur: Whole-Proof Generation and Repair with Large Language Models](https://arxiv.org/abs/2303.04910) |  |  |  |  |
| CodeGeeX |  |  | [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X](https://arxiv.org/pdf/2303.17568.pdf) |  |  |  | [CodeGeeX Code](https://github.com/THUDM/CodeGeeX) |
| CodeGeeX2-6B |  |  | [CodeGeeX2-6B Blog](https://mp.weixin.qq.com/s/roQSCo-7s361P3TmJjjZjA) |  |  |  | [CodeGeeX2-6B Code](https://github.com/THUDM/CodeGeeX2) |
| CodeGen2 | 2023/04 | [codegen2 1B-16B](https://github.com/salesforce/CodeGen2) | [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309) | 1 - 16 | [2048](https://arxiv.org/abs/2305.02309) | [Apache 2.0](https://github.com/salesforce/CodeGen2/blob/main/LICENSE)|                                                                                           |
| CodeGen2.5 | 2023/07 | [CodeGen2.5-7B-multi](https://huggingface.co/Salesforce/codegen25-7b-multi) | [CodeGen2.5: Small, but mighty](https://blog.salesforceairesearch.com/codegen25/) | 7 | [2048](https://huggingface.co/Salesforce/codegen25-7b-multi/blob/main/config.json) | [Apache 2.0](https://huggingface.co/Salesforce/codegen25-7b-multi/blob/main/README.md) | 
| Code Llama  | 2023 | [Inference Code for CodeLlama models]([https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://github.com/facebookresearch/codellama))Â | [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)     | 7 - 34       | [4096](https://scontent-zrh1-1.xx.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=wURKmnWKaloAX-ib5XW&_nc_ht=scontent-zrh1-1.xx&oh=00_AfAN1GB2K_XwIz54PqXTr-dhilI3CfCwdQoaLMyaYEEECg&oe=64F0A68F)  | [Custom](https://github.com/facebookresearch/llama/blob/main/LICENSE) Free if you have under 700M users and you cannot use LLaMA outputs to train other LLMs besides LLaMA and its derivatives   | [HuggingChat](https://huggingface.co/blog/codellama) | 
| Copilot X |  |  | [Copilot X Blog](https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/) |  |  |  | [Copilot X Code](https://github.com/features/copilot/) |

**Complete Content**: please refer to [Model Survey](https://github.com/ArronAI007/Awesome-AGI/tree/main/Model-Survey/README.md)

---

### Universal LLMs for Picture/Video 

Some examples of **Universal LLMs for Picture/Video** as followsï¼š

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length                                                                         | Licence | Code                                                                                    |
| --- | --- | --- | --- | --- |----------------------------------------------------------------------------------------| --- |-------------------------------------------------------------------------------------------|
| Bing Image Creator |  |  | [Bing Image Creator Blog](https://techcrunch.com/2023/03/21/microsoft-brings-openais-dall-e-image-creator-to-the-new-bing/) |  |  |  |  |
| BlenderGPT |  |  |  |  |  |  | [BlenderGPT Code](https://github.com/gd3kr/BlenderGPT)  |
| CHAMPAGNE |  |  | [CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos](https://arxiv.org/pdf/2303.09713.pdf)  |  |  |  | [CHAMPAGNE Code](https://seungjuhan.me/champagne) |
| CodeGen2 | 2023/04 | [codegen2 1B-16B](https://github.com/salesforce/CodeGen2) | [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309) | 1 - 16 | [2048](https://arxiv.org/abs/2305.02309) | [Apache 2.0](https://github.com/salesforce/CodeGen2/blob/main/LICENSE)| 
| Consistency Models |  |  | [Consistency Models](https://arxiv.org/abs/2303.01469)ï¼Œ[Consistency Models Blog](https://hub.baai.ac.cn/view/25445) |  |  |  | [Consistency Models Code](https://github.com/openai/consistency_models) |
| ControlNet |  |  | [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/pdf/2302.05543.pdf)ï¼Œ[ControlNet Blog](https://mp.weixin.qq.com/s/k8rE9GrF97E-0TKJhih9kw)  |  |  |  | [ControlNet Code](https://github.com/lllyasviel/ControlNet) |

**Complete Content**: please refer to [Model Survey](https://github.com/ArronAI007/Awesome-AGI/tree/main/Model-Survey/README.md)

---  

### Universal LLMs for Audio

Some examples of **Universal LLMs for Audio** as followsï¼š

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length                                                                         | Licence | Code                                                                                    |
| --- | --- | --- | --- | --- |----------------------------------------------------------------------------------------| --- |-------------------------------------------------------------------------------------------|
| AudioCraft |  |  | [AudioCraft Blog](https://mp.weixin.qq.com/s/OLLCiMqKHQJxGGR1sPA3qw) |  |  |  | [code](https://github.com/facebookresearch/audiocraft) |
| CodeGen2 | 2023/04 | [codegen2 1B-16B](https://github.com/salesforce/CodeGen2) | [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309) | 1 - 16 | [2048](https://arxiv.org/abs/2305.02309) | [Apache 2.0](https://github.com/salesforce/CodeGen2/blob/main/LICENSE)| 
| Generative Disco |  |  | [Generative Disco: Text-to-Video Generation for Music Visualization](https://arxiv.org/pdf/2304.08551.pdf)ï¼Œ[Generative Disco Blog](https://hub.baai.ac.cn/view/25517) |  |  |  | | 
| MUSICGEN |  |  | [Simple and Controllable Music Generation](https://arxiv.org/pdf/2306.05284.pdf)ï¼Œ[MUSICGEN Blog](https://the-decoder.com/metas-open-source-ai-musicgen-turns-text-and-melody-into-new-songs/) |  |  |  | [demo](https://huggingface.co/spaces/facebook/MusicGen) |
| Make-An-Audio |  |  | [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](https://arxiv.org/abs/2301.12661) |  |  |  | [Make-An-Audio Code](https://text-to-audio.github.io) |
| Voicebox |  |  | [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale](https://research.facebook.com/file/649409006862002/paper_fixed.pdf)ï¼Œ[Voicebox Blog](https://hub.baai.ac.cn/view/27492) | |  |  |  | |

**Complete Content**: please refer to [Model Survey](https://github.com/ArronAI007/Awesome-AGI/tree/main/Model-Survey/README.md)

---

### Universal LLMs for Multimodal 

Some examples of **Universal LLMs for Multimodal** as followsï¼š
 
| Multimodal Model | Language Model | Vision Model |  Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code |
| --- | --- | --- | --- | --- |---|---| --- | --- |
| BLIP-2 |  |  |  | [paper](https://arxiv.org/abs/2301.12597)ï¼Œ[demo](https://huggingface.co/spaces/Salesforce/BLIP2)ï¼Œ[doc](https://huggingface.co/docs/transformers/main/en/model_doc/blip-2) |  |  | | [code](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)ï¼Œ[fine-tuing](https://github.com/salesforce/LAVIS)ï¼Œ[hugging face spaces](https://hf.co/spaces/Salesforce/BLIP2) |
| CodeGen2 | | | [codegen2 1B-16B](https://github.com/salesforce/CodeGen2) | [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309) | 1 - 16 | [2048](https://arxiv.org/abs/2305.02309) | [Apache 2.0](https://github.com/salesforce/CodeGen2/blob/main/LICENSE)| 
| GPT4 |  |  |  | [OpenAI GPT4](https://openai.com/research/gpt-4)ï¼Œ[GPT4_System_Cardä¸­æ–‡ç¿»è¯‘](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Official/GPT-4_System_Card_zh.md)ï¼Œ[GPT4_Technical_Reportä¸­æ–‡ç¿»è¯‘](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Official/GPT4_Technical_Report_zh.md)ï¼Œ[The Ultimate GPT-4 Guide](https://doc.clickup.com/37456139/d/h/13q28b-324/e2a22b0c164b1f9) | 
| LaVIN | LLaMA | ViT-L |  | [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://arxiv.org/pdf/2305.15023.pdf)ï¼Œ[blog](https://mp.weixin.qq.com/s/MRLYk1b7VJ_b6OmJ9mzkdw) | 7ï¼Œ13 |  | [LaVIN Code](https://github.com/luogen1996/LaVIN) |
| LLaVA |  |  |  | [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)ï¼Œ[introduce](https://llava-vl.github.io/) |  |  |  | |
| UniDiffuser |  |  |  | [One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale](https://ml.cs.tsinghua.edu.cn/diffusion/unidiffuser.pdf) |  |  |  | [UniDiffuser Code](https://github.com/thu-ml/unidiffuser) |
| Video-LLaMA |  |  |  | [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858) |  |  |  | |
| VisCPM |  |  |  | [VisCPM Blog](https://mp.weixin.qq.com/s/Fgfbs1vV7RF6kpyk4bfIYw) |  |  |  | [VisCPM Code](https://github.com/OpenBMB/VisCPM) |
| X-LLM |  |  |  | [X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](https://arxiv.org/abs/2305.04160)ï¼Œ[é¡¹ç›®ä¸»é¡µ](https://x-llm.github.io/) |  |  |  | |

**Complete Content**: please refer to [Model Survey](https://github.com/ArronAI007/Awesome-AGI/tree/main/Model-Survey/README.md)

---

## Domain LLMs

### Domain LLMs for Law

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| CaMA |  |  | [CaMA Blog](https://mp.weixin.qq.com/s/FYWmMH2ndN5XfWvwI9dcUA) |  |  |   |  |    
| ChatLaw |  |  | [ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases](https://arxiv.org/pdf/2306.16092.pdf)ï¼Œ[official website](https://www.chatlaw.cloud/) |  |  |   | [ChatLaw Code](https://github.com/PKU-YuanGroup/ChatLaw) |   
| ExpertLLaMA |  |  | [ExpertLLaMA Blog](https://mp.weixin.qq.com/s/FYWmMH2ndN5XfWvwI9dcUA) |  |  |   |  |   
| LaWGPT |  |  | [LaWGPT Blog](https://mp.weixin.qq.com/s/dI839IF0hdBTAfOBUg7Pfw) |  |  |   | [LaWGPT Code](https://github.com/pengxiao-song/LaWGPT) |   
| LAW-GPT |  |  |  |  |  |   | [LAW-GPT Code](https://github.com/LiuHC0428/LAW-GPT) |   
| LexiLaw |  |  |  |  |  |   | [LexiLaw Code](https://github.com/CSHaitao/LexiLaw) |   
| lawyer-llama |  |  |  |  |  |   | [lawyer-llama Code](https://github.com/AndrewZhe/lawyer-llama) |   

---
                                                                                                           
### Domain LLMs for Medical

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| AD-AutoGPT |  |  | [AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology](https://arxiv.org/abs/2306.10095) |  |  |  |  |     
| BianQue |  |  |  |  |  |  | [BianQue Code](https://github.com/scutcyr/BianQue) |  
| BiomedGPT |  |  | [BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks](https://arxiv.org/abs/2305.17100) |  |  |  |  |
| ChatMed |  |  |  |  |  |  | [ChatMed Code](https://github.com/michael-wzhu/ChatMed) |   
| DoctorGLM |  |  |  |  |  |  | [DoctorGLM Code](https://github.com/xionghonglin/DoctorGLM) |   
| Huatuo-Llama-Med-Chinese |  |  |  |  |  |  | [Huatuo-Llama-Med-Chinese Code](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) |   
| Med-PaLM |  |  | [Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)ï¼Œ[Med-PaLM Blog](https://mp.weixin.qq.com/s/Qf4Ts7UKJNzkW1Tfy-b0Zg) |  |  |  |  |   
| MedQA-ChatGLM |  |  | [ä¸»é¡µ](https://www.wangrs.co/MedQA-ChatGLM/) |  |  |  | [MedQA-ChatGLM Code](http://github.com/WangRongsheng/MedQA-ChatGLM) |   
| MedicalGPT-zh |  |  |  |  |  |  | [MedicalGPT-zh Code](https://github.com/MediaBrain-SJTU/MedicalGPT-zh) |   
| Med-ChatGLM |  |  |  |  |  |  | [Med-ChatGLM Code](https://github.com/SCIR-HI/Med-ChatGLM) |   
| PULSE |  | [PULSE](https://huggingface.co/OpenMEDLab/PULSE-7bv5) |  |  |  |  |  |   
| ShenNong-TCM-LLM |  |  |  |  |  |  | [ShenNong-TCM-LLM Code](https://github.com/michael-wzhu/ShenNong-TCM-LLM) |  
| SoulChat |  |  |  |  |  |  | [SoulChat Code](https://github.com/scutcyr/SoulChat) |    

---                                                                                                              

### Domain LLMs for Finance

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| FinGPT |  |  | [FinGPT: Open-Source Financial Large Language Models](https://arxiv.org/pdf/2306.06031v1.pdf)ï¼Œ[FinGPT Blog](https://mp.weixin.qq.com/s/A9euFin675nxGGciiX6rJQ) |  |  |  | [FinGPT Code](https://github.com/ai4finance-foundation/fingpt) |     
| FinTuo |  |  |  |  |  |  | [FinTuo Code](https://github.com/qiyuan-chen/FinTuo-Chinese-Finance-LLM) |    

--- 

### Domain LLMs for Environment

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| NowcastNet |  |  | [Skilful nowcasting of extreme precipitation with NowcastNet](https://www.nature.com/articles/s41586-023-06184-4)ï¼Œ[NowcastNet Blog](https://mp.weixin.qq.com/s/Aygm03CdA0zFNf9F3_JU5A) |  |  |  |  |   

---   

### Domain LLMs for Network Security

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| FraudGPT |  |  | [FraudGPT Blog](https://mp.weixin.qq.com/s/OtLNybsbxDlbVb-cs4Zk8g) |  |  |  |  |      

---

### Domain LLMs for Education

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| EduChat |  |  |  |  |  |  | [EduChat Code](https://github.com/icalk-nlp/EduChat) |  

---     

### Domain LLMs for Traffic

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| TransGPT |  |  | [TransGPT Blog](https://mp.weixin.qq.com/s/WvzyjHqI0lOGIyPlCIFNQg) |  |  |  | [TransGPT Code](https://github.com/DUOMO/TransGPT) |   

---    

### Domain LLMs for Other

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Code                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |-----------------------------------------------------------------------------------------------------------------------|
| Panda LLM |  |  | [Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models](https://arxiv.org/pdf/2305.03025v1.pdf)ï¼Œ[Panda LLM Blog](https://mp.weixin.qq.com/s/IsWSPAvwgT263wjO7TYTZQ) |  |  |  | [Panda LLM Code](https://github.com/dandelionsllm/pandallm) |     
  
---

## LLM Deployment

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| BentoML |  | [BentoML Code](https://github.com/bentoml/BentoML) |  |  
| CLIP-API-service |  |  |  |  
| CTranslate2 |  |  |  |  
| DeepSpeed-MII |  |  |  |  
| LightLLM |  |  |  |  
| LMDeploy |  |  |  |  
| MLC LLM |  |  |  |  
| OneDiffusion |  |  |  |  
| OpenLLM |  |  |  |  
| Ray Serve |  |  |  |  

## LLM Agent

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| AgentGPT |  | [AgentGPT Code](https://github.com/reworkd/AgentGPT) | [AgentGPT Chat](https://agentgpt.reworkd.ai/zh)ï¼Œ[AgentGPT docs](https://docs.reworkd.ai/introduction) |  
| AI Legion |  | [AI Legion Chat](https://github.com/eumemic/ai-legion) |  |  
| AutoGPT |  | [AutoGPT Code](https://github.com/Significant-Gravitas/Auto-GPT) | [AutoGPT docs](https://docs.agpt.co/setup/) ï¼Œ[AutoGPT blog](https://generativeai.pub/complete-guide-to-setup-autogpt-revolutionize-your-task-automation-with-gpt-4-39eda5a85821?gi=ea5c40bac6fd) |  
| BabyAGI |  | [BabyAGI Code](https://github.com/yoheinakajima/babyagi) | [BabyAGI docs](https://babyagi.org/) |  
| CAMEL |  | [CAMEL Code](https://github.com/camel-ai/camel) | [CAMEL Chat](http://agents.camel-ai.org/)ï¼Œ[CAMEL docs](https://www.camel-ai.org/) |  
| Do Anything Machine |  |  | [Do Anything Machine Chat](https://www.doanythingmachine.com/) |  
| Generative Agents | [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442) | [GPTRPG Code](https://github.com/dzoba/gptrpg) |  | 
| Godmode |  |  | [Godmode Chat](https://godmode.space/) |  
| GPT-Engineer |  | [GPT-Engineer Code](https://github.com/AntonOsika/gpt-engineer) |  |   
| HuggingGPT |  | [HuggingGPT Code](https://github.com/microsoft/JARVIS) | [HuggingGPT Chat](https://huggingface.co/spaces/microsoft/HuggingGPT) |  
| MetaGPT |  | [MetaGPT Code](https://github.com/geekan/MetaGPT) |  | 
| NexusGPT |  |  | [NexusGPT Chat](https://nexus.snikpic.io/) |  
| Toolformer | [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2302.04761.pdf) |  | [Toolformer blog](https://www.sensorexpert.com.cn/article/194585.html) |  

## AGI Paper Lists

Some examples of **Paper Lists** as followsï¼š

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹æ±‡æ€»ï¼šåŒ»ç–—ã€æ³•å¾‹ã€é‡‘èã€æ•™è‚²ã€æ•°å­¦å¾®è°ƒï¼Œ ç›®å‰å·²1.1Kæ˜Ÿ |  | [code](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM) |  | 
| å¤§å‹è¯­è¨€æ¨¡å‹ç»¼è¿°å…¨æ–°å‡ºç‚‰ï¼šä»T5åˆ°GPT-4æœ€å…¨ç›˜ç‚¹ï¼Œå›½å†…20ä½™ä½ç ”ç©¶è€…è”åˆæ’°å†™ | [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) |  |  | 
| å¤§è¯­è¨€æ¨¡å‹ç»¼è¿°å…¨æ–°å‡ºç‚‰ï¼š51é¡µè®ºæ–‡å¸¦ä½ ç›˜ç‚¹LLMé¢†åŸŸä¸“ä¸šåŒ–æŠ€æœ¯ | [Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey](https://arxiv.org/abs/2305.18703) |  | [blog](https://mp.weixin.qq.com/s/0DrowrTIgXsBhj3sYu6Aog) | 
| AIGCç»¼è¿°: ä»GANåˆ°ChatGPTçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç®€å² | [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/abs/2303.04226v1) |  |  | 
| å¤§æ¨¡å‹ç»¼è¿°æ¥äº†ï¼ä¸€æ–‡å¸¦ä½ ç†æ¸…å…¨çƒAIå·¨å¤´çš„å¤§æ¨¡å‹è¿›åŒ–å² | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/pdf/2304.13712.pdf) | [code](https://github.com/Mooler0410/LLMsPracticalGuide) |  | 

**Complete Content**: please refer to [Paper Lists](https://github.com/ArronAI007/Awesome-AGI/tree/main/Paper-Lists/README.md)

## æ¬¢è¿å…±åˆ›

ã€ğŸ‘¬ğŸ»ã€‘æ¬¢è¿Star â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ && æäº¤ Pull requests ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»

## å…³äºæˆ‘

**ä¸ªäººä¸»é¡µ**ï¼šwshzd.github.io

**å¾®ä¿¡å…¬ä¼—å·**ï¼š

![å…¬ä¼—å·äºŒç»´ç ](https://i.postimg.cc/g092W4dB/ArronAI.jpg)

## å£°æ˜

ä»¥ä¸Šéƒ¨åˆ†èµ„æ–™æ¥è‡ªç½‘ç»œæ•´ç†ï¼Œä¾›å¤§å®¶å­¦ä¹ å‚è€ƒï¼Œå¦‚æœ‰ä¾µæƒï¼Œéº»çƒ¦è”ç³»æˆ‘åˆ é™¤ï¼ 

WeChatï¼šh18821656387
