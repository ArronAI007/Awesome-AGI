{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "这是一个使用 Unsloth 框架和 TRL (Transformer Reinforcement Learning) 库，通过 GRPO (Group Relative Policy Optimization) 算法对 Qwen 2.5 (3B) 大模型进行微调（Fine-tuning）的 Jupyter Notebook 代码。\n",
        "# 核心功能总结\n",
        "## 整个代码实现了以下主要功能：\n",
        "- 环境配置：安装并配置 Unsloth、vLLM 和 TRL 等高性能训练库。\n",
        "- 模型加载与量化：加载 Qwen 2.5-3B-Instruct 模型，并使用 4-bit 量化（QLoRA）以降低显存占用，同时启用 vLLM 进行快速推理。\n",
        "- 数据集准备：加载 GSM8K（小学数学）数据集，并将其格式化为包含 <reasoning> 和 <answer> XML 标签的 Prompt，旨在训练模型具备“思维链”（Chain-of-Thought）推理能力。\n",
        "- 定义奖励函数 (Reward Functions)：定义了一组规则来评价模型的输出，包括：答案准确性、是否为整数、XML 格式是否规范等。这是强化学习（RL）的核心部分。\n",
        "- GRPO 训练：使用 GRPO 算法进行训练。GRPO 会让模型针对同一个问题生成多个回答，通过对比这些回答的奖励分数来优化策略，而不需要额外的价值模型（Value Model）。\n",
        "- 保存与推理：保存微调后的 LoRA 权重，并演示如何加载权重进行推理测试。"
      ],
      "metadata": {
        "id": "0Xe2p_ZtKqR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 环境安装与设置"
      ],
      "metadata": {
        "id": "A4Oevo-UK-aM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGwKRk7GaMKw"
      },
      "outputs": [],
      "source": [
        "import os, numpy\n",
        "\n",
        "# 设置环境变量，让 Unsloth 在 vLLM 中预留更多显存用于上下文\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n",
        "\n",
        "# 获取当前 numpy 版本以防止依赖冲突\n",
        "numpy_version = f\"numpy=={numpy.__version__}\"\n",
        "\n",
        "# Install dependencies with numpy version preservation\n",
        "# 安装 Unsloth 及其依赖（Unsloth 用于加速训练，vLLM 用于加速推理）\n",
        "!uv pip install unsloth_zoo\n",
        "!uv pip install --upgrade unsloth vllm==0.9.2 {numpy_version} torchvision bitsandbytes xformers\n",
        "!uv pip install triton==3.2.0\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 加载模型与 Tokenizer"
      ],
      "metadata": {
        "id": "ODdBCXKkLwRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 设置最大上下文长度\n",
        "max_seq_length = 1024\n",
        "\n",
        "# 加载预训练模型和分词器\n",
        "# 功能：加载 Qwen2.5-3B-Instruct 模型，使用 4-bit 量化加载以节省显存，开启 fast_inference (vLLM)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,               # 4bit 量化加载\n",
        "    fast_inference = True,             # 启用 vLLM 快速推理引擎\n",
        "    max_lora_rank = 8,                 # LoRA 秩\n",
        "    gpu_memory_utilization = 0.9,      # 显存利用率上限\n",
        ")"
      ],
      "metadata": {
        "id": "MaU2aDHda1Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 配置 LoRA (低秩适应)"
      ],
      "metadata": {
        "id": "lOhXjeAIMfIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 配置 PEFT (Parameter-Efficient Fine-Tuning)\n",
        "# 功能：将模型转换为 LoRA 模式，只训练新增的少量参数，冻结原模型参数\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8,  # LoRA 的秩\n",
        "    # 指定需要应用 LoRA 的模块（注意力层和前馈网络层）\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 8,\n",
        "    use_gradient_checkpointing = \"unsloth\",       # 使用梯度检查点节省显存\n",
        "    random_state = 1234,\n",
        ")"
      ],
      "metadata": {
        "id": "EEPZwPapa1Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 数据集处理与格式化"
      ],
      "metadata": {
        "id": "KU0CJPFtPM_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# 系统提示词，强制模型使用特定的 XML 格式输出推理过程和答案\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# 定义 XML 格式模板\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# 函数：从模型输出中提取 XML 标签内的答案\n",
        "def extract_xml_answer(text):\n",
        "    if \"\" not in text or \"\" not in text:\n",
        "        return \"\"\n",
        "    return text.split(\" \", 1)[-1].split(\" \", 1)[0].strip()\n",
        "\n",
        "# 函数：从 GSM8K 数据集的原始答案字段中提取最终数值（通常在 #### 之后）\n",
        "def extract_hash_answer(text):\n",
        "    return text.split(\"####\")[-1].strip() if \"####\" in text else None\n",
        "\n",
        "# 函数：加载并预处理 GSM8K 数据集\n",
        "# 功能：加载 OpenAI 的 GSM8K 数据集，并将每个样本转化为包含 system prompt 和 user prompt 的对话格式\n",
        "def get_gsm8k_dataset(split = \"train\"):\n",
        "    data = load_dataset(\"openai/gsm8k\", \"main\")[split]\n",
        "    return data.map(\n",
        "        lambda x: {\n",
        "            \"prompt\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": x[\"question\"]},\n",
        "            ],\n",
        "            \"answer\": extract_hash_answer(x[\"answer\"]),           # 提取标准答案用于后续奖励计算\n",
        "        }\n",
        "    )\n",
        "\n",
        "# 加载处理好的数据集\n",
        "dataset = get_gsm8k_dataset()"
      ],
      "metadata": {
        "id": "-5nvyrwqa1LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 定义奖励函数 (Reward Functions)\n",
        "这是 GRPO 的核心，模型生成的每个结果都会经过这些函数打分。"
      ],
      "metadata": {
        "id": "oN55_esRP_6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 奖励函数 1：正确性奖励\n",
        "# 功能：检查模型生成的答案（从 XML 中提取）是否与标准答案完全一致。正确得 2.0 分，否则 0 分。\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    # 打印日志方便调试\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "# 奖励函数 2：整数奖励\n",
        "# 功能：检查提取出的答案是否为数字。是则得 0.5 分。\n",
        "def int_reward_func(completions, **kwargs):\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "# 奖励函数 3：严格格式奖励\n",
        "# 功能：使用正则检查输出是否严格符合 <reasoning>...\\n<answer>... 的格式结构。\n",
        "def strict_format_reward_func(completions, **kwargs):\n",
        "    pattern = r\"^\\n.*?\\n\\n\\n.*?\\n\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "# 奖励函数 4：宽松格式奖励\n",
        "# 功能：检查输出是否至少包含了 XML 标签，允许格式上有少量空白差异。\n",
        "def soft_format_reward_func(completions, **kwargs):\n",
        "    pattern = r\".*?\\s*.*?\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "# 辅助函数：计算 XML 标签的完整性\n",
        "def count_xml(text):\n",
        "    count = 0.0\n",
        "    # 检查各个标签是否存在，每存在一个加分，如果格式混乱（如多余换行）则扣分\n",
        "    if text.count(\"\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n\\n\")[-1])*0.001       # 惩罚项\n",
        "    if text.count(\"\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n\")[-1]) - 1)*0.001   # 惩罚项\n",
        "    return count\n",
        "\n",
        "# 奖励函数 5：XML 计数奖励\n",
        "# 功能：基于 XML 标签的完整性和位置给予分数。\n",
        "def xmlcount_reward_func(completions, **kwargs):\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ],
      "metadata": {
        "id": "4Nv6tdqua1O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 配置与启动 GRPO 训练"
      ],
      "metadata": {
        "id": "TJhM2DHrQrrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# 配置训练参数\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True,                  # 使用 vLLM 生成样本（极快）\n",
        "    learning_rate = 5e-6,             # 学习率\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_8bit\",             # 使用 8-bit 优化器节省显存\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    num_generations = 4,              # GRPO 核心：每个 prompt 生成 4 个回答进行对比\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 200,\n",
        "    max_steps = 250,                  # 训练总步数\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ],
      "metadata": {
        "id": "6uBssgXta1Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化 GRPO 训练器\n",
        "# 功能：将模型、奖励函数列表和训练配置结合\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "rX97HKsna1Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始训练\n",
        "# 功能：模型开始根据 prompt 生成多个回答，根据奖励函数的反馈更新 LoRA 权重，使模型更倾向于生成高分回答（格式正确且答案正确）。\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "WS1Q-OwVa1Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 保存与推理测试"
      ],
      "metadata": {
        "id": "1om2iZRRRZXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存训练好的 LoRA 适配器\n",
        "model.save_lora(\"grpo_saved_lora\")"
      ],
      "metadata": {
        "id": "qhPj4SKKcVOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 推理部分 ---\n",
        "from vllm import SamplingParams\n",
        "\n",
        "# 测试用的查询\n",
        "query = \"How many r's are in strawberry?\"\n",
        "\n",
        "# 构建聊天模板\n",
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : query},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "# 设置采样参数\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "\n",
        "# 生成回答（不加载 LoRA 或 加载 LoRA）\n",
        "# 这里演示了如何使用 model.fast_generate 进行快速推理\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,                    # 这里设为 None 表示用基础模型，若要用训练后的模型需加载 LoRA\n",
        ")[0].outputs[0].text\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "cD4eWsYDcVR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 构建聊天模板\n",
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : query},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "\n",
        "# 再次生成，这次加载刚才保存的 LoRA 权重\n",
        "# 功能：验证经过 GRPO 训练后的模型表现\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "JJuZf7rKcVXS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}