{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目地址：https://github.com/Baichuan-inc/Baichuan-13B\n",
    "\n",
    "预训练模型：https://huggingface.co/baichuan-inc/Baichuan-13B-Base\n",
    "\n",
    "对话模型：https://huggingface.co/baichuan-inc/Baichuan-13B-Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、推理和部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 GPU部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装环境\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一：python方式\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True) # device_map='auto'，会使用所有可用显卡。如需指定使用的设备，可以使用类似 export CUDA_VISIBLE_DEVICES=0,1（使用了0、1号显卡）的方式控制。\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\")\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"世界上第二高的山峰是哪座\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n",
    "# 乔戈里峰。世界第二高峰———乔戈里峰西方登山者称其为k2峰，海拔高度是8611米，位于喀喇昆仑山脉的中巴边境上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二：命令行工具方式\n",
    "python cli_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法三：网页 demo 方式\n",
    "# 依靠streamlit运行以下命令，会在本地启动一个 web 服务，把控制台给出的地址放入浏览器即可访问。\n",
    "streamlit run web_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 量化部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baichuan-13B 支持 int8 和 int4 量化，用户只需在推理代码中简单修改两行即可实现。请注意，如果是为了节省显存而进行量化，应加载原始精度模型到 CPU 后再开始量化；避免在from_pretrained时添加device_map='auto'或者其它会导致把原始精度模型直接加载到 GPU 的行为的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int8 量化\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model = model.quantize(8).cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int4 量化\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model = model.quantize(4).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 CPU 部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用CPU进行推理大概需要 60GB 内存。\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", torch_dtype=torch.float32, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"What are the three primary colors?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The three primary colors are red, blue, and yellow.\"\n",
    "    },\n",
    "    ....\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调工具\n",
    " LLaMA Efficient Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 全量微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed --num_gpus=8 src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path baichuan-inc/Baichuan-13B-Base \\\n",
    "    --do_train \\\n",
    "    --dataset alpaca_gpt4_en,alpaca_gpt4_zh \\\n",
    "    --finetuning_type full \\\n",
    "    --output_dir path_to_your_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 4 \\ \n",
    "    --per_device_eval_batch_size 4 \\ \n",
    "    --gradient_accumulation_steps 8 \\ \n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --max_grad_norm 0.5 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --dev_ratio 0.01 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --deepspeed deepspeed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_speed.json 配置示例：\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"zero_allow_untested_optimizer\": true,\n",
    "  \"fp16\": {\n",
    "    \"enabled\": \"auto\",\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16, \n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },  \n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"allgather_partitions\": true,\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"overlap_comm\": false,\n",
    "    \"reduce_scatter\": true,\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"contiguous_gradients\" : true\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 LoRA微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path baichuan-inc/Baichuan-13B-Base \\\n",
    "    --do_train \\\n",
    "    --dataset alpaca_gpt4_en,alpaca_gpt4_zh \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_rank 8 \\ \n",
    "    --lora_target W_pack \\\n",
    "    --output_dir path_to_your_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 4 \\ \n",
    "    --per_device_eval_batch_size 4 \\ \n",
    "    --gradient_accumulation_steps 8 \\ \n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --max_grad_norm 0.5 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --dev_ratio 0.01 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
