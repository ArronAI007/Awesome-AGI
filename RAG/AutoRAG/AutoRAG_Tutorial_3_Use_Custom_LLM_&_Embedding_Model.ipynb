{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO1alhGea227"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and setting AutoRAG"
      ],
      "metadata": {
        "id": "ghjhB71eizfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "apt-get remove python3-blinker\n",
        "pip install blinker==1.8.2"
      ],
      "metadata": {
        "id": "cAKvH5_EGj3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq ipykernel==5.5.6 ipywidgets-bokeh==1.0.2"
      ],
      "metadata": {
        "id": "FSVcZZ9fGjrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq AutoRAG>=0.3.7 datasets chromadb==0.5.5"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tbJg0cJxa_2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "wdMrkdoriwn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom LLM & embedding model\n",
        "\n",
        "You can use most of the LLM and embedding models from LlamaIndex in AutoRAG.\n",
        "However, you have to add some snippet of code to use it.\n",
        "It is not that hard.\n",
        "Let's find out how can add your own LLM model or embedding model in AutoRAG.\n",
        "\n",
        "## AutoRAG default supporting models\n",
        "\n",
        "Since you can use any LlamaIndex models in AutoRAG, but AutoRAG supports some models as default without your own configuration.\n",
        "\n",
        "### Default supported LLMs\n",
        "\n",
        "| LLM Model Type | llm parameter  |\n",
        "|:--------------:|:--------------:|\n",
        "|     OpenAI     |     openai     |\n",
        "| HuggingFaceLLM | huggingfacellm |\n",
        "|   OpenAILike   |   openailike   |\n",
        "|     Ollama     |     ollama     |\n",
        "\n",
        "\n",
        "### Default supported Embedding Models\n",
        "\n",
        "\n",
        "|                                           Embedding Model Type                                            |       embedding_model parameter       |\n",
        "|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------:|\n",
        "|                             Default openai embedding (text-embedding-ada-002)                             |                openai                 |\n",
        "|                              openai large embedding (text-embedding-3-large)                              |         openai_embed_3_large          |\n",
        "|                              openai small embedding (text-embedding-3-small)                              |         openai_embed_3_small          |\n",
        "|                  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)                  |      huggingface_baai_bge_small       |\n",
        "|               [cointegrated/rubert-tiny2](https://huggingface.co/cointegrated/rubert-tiny2)               | huggingface_cointegrated_rubert_tiny2 |\n",
        "| [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) |     huggingface_all_mpnet_base_v2     |\n",
        "|                             [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                             |          huggingface_bge_m3           |\n",
        "\n"
      ],
      "metadata": {
        "id": "EqhTvRnOi2H-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure custom LLM model\n",
        "\n",
        "Beside default LLM model, you can use other LLM model in LlamaIndex.\n",
        "You can check supported LlamaIndex LLM list at [here](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/).\n",
        "\n",
        "In this tutorial, we will use Upstage Model at AutoRAG. So, for using it, you must prepare Upstage API key at [here](https://console.upstage.ai/home).\n",
        "\n",
        "And, you have to add your Upstage api key at secrets. Check out left side and go to 'Secrets' tab. At there, press 'add new secret' and set name to UPSTAGE_API_KEY. And set value to your openai api key. Be sure to press toggle for notebook access!\n",
        "\n",
        "You can check out the usage of the Upstage model in LlamaIndex at [here](https://docs.llamaindex.ai/en/stable/examples/llm/upstage/)."
      ],
      "metadata": {
        "id": "Spx8P3lIlbpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "upstage_api_key = userdata.get('UPSTAGE_API_KEY')\n",
        "assert bool(upstage_api_key), \"You have to set OPENAI_API_KEY at colab secrets.\"\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = upstage_api_key"
      ],
      "metadata": {
        "id": "axSmdtAOiyYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After configuring UPSTAGE_API_KEY, let's add Upstage LLM class to AutoRAG.\n",
        "\n",
        "For adding this, you have to install LlamaIndex Upstage extension first."
      ],
      "metadata": {
        "id": "EpNidQV5paeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-llms-upstage"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UZs2A67VptFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autorag\n",
        "\n",
        "from llama_index.llms.upstage import Upstage\n",
        "\n",
        "autorag.generator_models['upstage'] = Upstage"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F-EQOp_apVYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure custom embedding model\n",
        "\n",
        "After LLM, let's add Upstage embedding model to AutoRAG. It is simple also."
      ],
      "metadata": {
        "id": "YrGLhjeGp2JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-upstage"
      ],
      "metadata": {
        "collapsed": true,
        "id": "txkp1EiFqUt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autorag\n",
        "from autorag import LazyInit\n",
        "from llama_index.embeddings.upstage import UpstageEmbedding\n",
        "\n",
        "autorag.embedding_models['upstage'] = LazyInit(UpstageEmbedding, model_name=\"solar-embedding-1-large\")"
      ],
      "metadata": {
        "id": "juPYzbTopxUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's use custom LLM & embedding model!\n",
        "\n",
        "Now configuration is over.\n",
        "For using custom models in AutoRAG, you can't use cli command.\n",
        "Instead, you can use python code easily.\n",
        "It is not hard.\n",
        "\n",
        "If you skip the first tutorial of AutoRAG, see [here](https://colab.research.google.com/drive/19OEQXO_pHN6gnn2WdfPd4hjnS-4GurVd?usp=sharing)."
      ],
      "metadata": {
        "id": "KFwQVrE0ql0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/content/eli5_data')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1L0yEGOwqkgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "def load_eli5_dataset(save_path):\n",
        "    # set file path\n",
        "    file_path = \"MarkrAI/eli5_sample_autorag\"\n",
        "\n",
        "    # load dataset\n",
        "    corpus_dataset = load_dataset(file_path, \"corpus\")['train'].to_pandas()\n",
        "    qa_train_dataset = load_dataset(file_path, \"qa\")['train'].to_pandas()\n",
        "    qa_test_dataset = load_dataset(file_path, \"qa\")['test'].to_pandas()\n",
        "\n",
        "    # save data\n",
        "    if os.path.exists(os.path.join(save_path, \"corpus.parquet\")) is True:\n",
        "        raise ValueError(\"corpus.parquet already exists\")\n",
        "    if os.path.exists(os.path.join(save_path, \"qa.parquet\")) is True:\n",
        "        raise ValueError(\"qa.parquet already exists\")\n",
        "    corpus_dataset.to_parquet(os.path.join(save_path, \"corpus.parquet\"))\n",
        "    qa_train_dataset.to_parquet(os.path.join(save_path, \"qa_train.parquet\"))\n",
        "    qa_test_dataset.to_parquet(os.path.join(save_path, \"qa_test.parquet\"))\n",
        "\n",
        "load_eli5_dataset(\"/content/eli5_data\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Iu5yOTgpq_Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "qa_df = pd.read_parquet('/content/eli5_data/qa_train.parquet')\n",
        "sample_qa_df = qa_df.sample(20, random_state=42) # In this sample code, we will only optimize pipeline with 20 samples just for testing.\n",
        "sample_qa_df.reset_index(drop=True, inplace=True)\n",
        "sample_qa_df.to_parquet('/content/eli5_data/qa_sample.parquet')"
      ],
      "metadata": {
        "id": "SA-nFZ76rDLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "from autorag.utils.util import to_list\n",
        "# We drop unused corpus dataframe for faster inference.\n",
        "corpus_df = pd.read_parquet('/content/eli5_data/corpus.parquet')\n",
        "target_retrieval_gts = list(chain.from_iterable(to_list(sample_qa_df[\"retrieval_gt\"].tolist())))\n",
        "target_retrieval_gts = list(chain.from_iterable(target_retrieval_gts))\n",
        "sample_corpus_df = corpus_df[corpus_df[\"doc_id\"].isin(target_retrieval_gts)]\n",
        "sample_corpus_df.reset_index(drop=True, inplace=True)\n",
        "sample_corpus_df.to_parquet('/content/eli5_data/corpus_sample.parquet')"
      ],
      "metadata": {
        "id": "IE-e9QtY0l3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sample_corpus_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Z1h1E21P3B",
        "outputId": "beeea4f9-8f01-4de6-b4b5-2afe271a9628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last thing for using custom models, you can just write in YAML file now.\n",
        "You can find `upstage` at `embedding_model` and `llm` setting at `generator`."
      ],
      "metadata": {
        "id": "jLyvtJbOrLBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "vectordb:\n",
        "  - name: chroma_upstage\n",
        "    db_type: chroma\n",
        "    client_type: persistent\n",
        "    collection_name: upstage\n",
        "    embedding_model: upstage\n",
        "    path: ${PROJECT_DIR}/resources/chroma\n",
        "node_lines:\n",
        "- node_line_name: retrieve_node_line\n",
        "  nodes:\n",
        "    - node_type: retrieval\n",
        "      strategy:\n",
        "        metrics: [retrieval_f1, retrieval_recall, retrieval_ndcg, retrieval_mrr]\n",
        "      top_k: 3\n",
        "      modules:\n",
        "        - module_type: vectordb\n",
        "          vectordb: chroma_upstage\n",
        "        - module_type: bm25\n",
        "        - module_type: hybrid_rrf\n",
        "          weight_range: (4,80)\n",
        "- node_line_name: post_retrieve_node_line\n",
        "  nodes:\n",
        "    - node_type: prompt_maker\n",
        "      strategy:\n",
        "        metrics:\n",
        "          - metric_name: meteor\n",
        "          - metric_name: rouge\n",
        "          - metric_name: sem_score\n",
        "            embedding_model: upstage # Use upstage embedding model\n",
        "      modules:\n",
        "        - module_type: fstring\n",
        "          prompt: \"Read the passages and answer the given question. \\n Question: {query} \\n Passage: {retrieved_contents} \\n Answer : \"\n",
        "    - node_type: generator\n",
        "      strategy:\n",
        "        metrics:\n",
        "          - metric_name: meteor\n",
        "          - metric_name: rouge\n",
        "          - metric_name: sem_score\n",
        "            embedding_model: upstage # Use upstage embedding model\n",
        "      modules:\n",
        "        - module_type: llama_index_llm\n",
        "          llm: upstage  # Use upstage LLM\n",
        "          batch: 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK8AZVC-rH1-",
        "outputId": "7015c237-f82b-4788-f8a0-c6976735c73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/project_dir"
      ],
      "metadata": {
        "id": "7dHXUDsF2dOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make project folder\n",
        "import os\n",
        "os.makedirs('/content/project_dir')"
      ],
      "metadata": {
        "id": "65GpC_ourkK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autorag.evaluator import Evaluator\n",
        "evaluator = Evaluator(qa_data_path='/content/eli5_data/qa_sample.parquet', corpus_data_path='/content/eli5_data/corpus_sample.parquet',\n",
        "                      project_dir='/content/project_dir')"
      ],
      "metadata": {
        "id": "xWecEZXQrlsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.start_trial('/content/config.yaml')"
      ],
      "metadata": {
        "id": "fyFzSVxarnGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Tip] Run Local Models\n",
        "\n",
        "To run local models, we recommend to use vllm for faster inference.\n",
        "`vllm` module is optimized for AutoRAG system.\n",
        "To learn how to run it, you can go to [here](https://docs.auto-rag.com/nodes/generator/vllm.html) for checking documentation."
      ],
      "metadata": {
        "id": "H2BdKtwy0dzu"
      }
    }
  ]
}