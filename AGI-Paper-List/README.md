## AGI-Paper-List

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 中文大语言模型汇总：医疗、法律、金融、教育、数学微调， 目前已1.1K星 |  | [code](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM) |  | 
| 大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写 | [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) |  |  | 
| 大语言模型综述全新出炉：51页论文带你盘点LLM领域专业化技术 | [Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey](https://arxiv.org/abs/2305.18703) |  | [blog](https://mp.weixin.qq.com/s/0DrowrTIgXsBhj3sYu6Aog) | 
| AIGC综述: 从GAN到ChatGPT的生成式人工智能简史 | [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/abs/2303.04226v1) |  |  | 
| 大模型综述来了！一文带你理清全球AI巨头的大模型进化史 | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/pdf/2304.13712.pdf) | [code](https://github.com/Mooler0410/LLMsPracticalGuide) |  | 
| 复旦大学教授肖仰华：ChatGPT 浪潮下，面向大模型如何做数据治理？ |  |  | [blog](https://mp.weixin.qq.com/s/od24PYvFLUJe4NQxjvsbMw) | 
| 面向决策的基础模型: 问题、方法与机会 | [Foundation Models for Decision Making: Problems, Methods, and Opportunities](https://arxiv.org/abs/2303.04129) |  |  | 
| 较大语言模型上下文学习的方式有所不同 | [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846) |  |  | 
| 通用语音识别大模型已经支持100+语言 |  |  | [blog](https://mp.weixin.qq.com/s/fHr2vL-w4JtYt5utcZrbsw) | 
| 发布5620亿参数多模态模型PaLM-E，机器人操控再上台阶 | [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) |  | [blog](https://palm-e.github.io/)，[twitter](https://twitter.com/DannyDriess/status/1632904675124035585)，[video](https://mp.weixin.qq.com/s/yZt3sEQPzVjnIvqXsNOnPA) | 
| PanGu-Σ: 稀疏异构计算万亿参数语言模型研究参数语言模型 | [PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing](https://arxiv.org/abs/2303.10845) |  |  | 
| 奖励聊天机器人在现实世界中与数以百万计的用户进行互动 | [Rewarding Chatbots for Real-World Engagement with Millions of Users](https://arxiv.org/pdf/2303.06135.pdf) |  |  | 
| 人工智能系统最终是否需要以现实为基础，而不仅仅是从语言中学习？ |  |  | [blog](https://spectrum.ieee.org/ai-hallucination) | 
| 大型语言模型是否需要感官基础来理解意义和理解？ |  |  | [slices](https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view) | 
| ChatGPT是「外星人」，所以才会胡说八道 | [AI Chatbots Don’t Care About Your Social Norms](https://www.noemamag.com/ai-chatbots-dont-care-about-your-social-norms/?utm_source=noematwitter&utm_medium=noemasocial) |  | [blog](https://twitter.com/ylecun/status/1633459264508542978) | 
| AI聊天机器人并不关注用户的社交属性 |  |  | [blog](https://www.noemamag.com/ai-chatbots-dont-care-about-your-social-norms/?utm_source=noematwitter&utm_medium=noemasocial) | 
| LeCun和马库斯齐喷ChatGPT：大语言模型果然是邪路？ |  |  | [blog](https://mp.weixin.qq.com/s/5e0aTSEAym9rF5QxRndLgQ) | 
| ChatGPT无法实现通用人工智能，但ALM技术路线也许可以 |  |  | [blog](https://mp.weixin.qq.com/s/MEdl3zmiYJU1iFsTXmibng) | 
| 「增强语言模型」的综述 | [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842) |  |  | 
| 自回归LLM的缺陷之一，大语言模型必须知道的8个要点 | [Eight Things to Know about Large Language Models](https://cims.nyu.edu/~sbowman/eightthings.pdf) |  |  | 
| 从词模型到世界模型：从自然语言到思维概率语言的转变 | [From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought](https://arxiv.org/abs/2306.12672) | |  | 
| AI进入2.0时代，所有应用都会被重写一遍 |  |  | [blog](https://mp.weixin.qq.com/s/zV8Y9RQnIoExwa1mmarZmA) | 
| 提出ILF（从语言反馈中模仿学习）：利用语言反馈大规模训练语言模型 | [Training Language Models with Language Feedback at Scale](https://arxiv.org/pdf/2303.16755.pdf) |  |  | 
| GPT就是GPT：大模型对劳动力市场影响潜力的早期研究 | [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/pdf/2303.10130.pdf) |  |  | 
| ABC News 专访OpenAI首席执行官萨姆·奥尔特曼：AI风险和重塑社会的问题 |  |  | [blog](https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122) | 
| 最新发布通用人工智能路线图！AGI比想象中来得更快！ |  |  | [blog](https://openai.com/blog/planning-for-agi-and-beyond/) | 
| Sam Altman 担心“潜在的可怕的”人工智能工具以及“未来的人们如何看待我们” |  |  | [blog](https://finance.yahoo.com/news/openai-ceo-sam-altman-frets-165250285.html) | 
| The Age of AI：拾象大模型及OpenAI投资思考 |  |  | [blog](https://mp.weixin.qq.com/s/AxX-Q7njegNTAxMkYFwsfA) | 
| 为什么ChatGPT用强化学习而非监督学习？ | [Scaling TransNormer to 175 Billion Parameters](https://arxiv.org/abs/2307.14995) | [code](https://github.com/OpenNLPLab/TransnormerLLM) | [blog](https://mp.weixin.qq.com/s/4USDakdomupWuwwhex6fMg) | 
| ChatGPT和生成式AI的11大安全趋势 |  |  | [blog](https://mp.weixin.qq.com/s/_RAx3vAx1ykQTJTEEoc37w) | 
| 分析过688篇大模型论文，这篇论文综述了LLM的当前挑战和应用 |  |  | [blog](https://mp.weixin.qq.com/s/drE6lhuhF9CbuAzDOhswTQ) | 
| 张学工教授：AI技术前沿——从ChatGPT到更多突破 |  |  | [blog](https://mp.weixin.qq.com/s/oeZd52BYKU3hhauZZ0eirQ) | 
| 研究大语言模型反映了谁的观点？ | [Whose Opinions Do Language Models Reflect?](https://arxiv.org/pdf/2303.17548.pdf) | [code](https://github.com/tatsu-lab/opinions_qa) |  | 
| 大模型及其公平使用 | [FOUNDATION MODELS AND FAIR USE](https://arxiv.org/pdf/2303.15715.pdf) |  |  | 
| 构建大模型生态系统图，用于跟踪大模型的足迹 |  |  | [blog](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=home) | 
| 斯坦福报告：基础模型的机遇与风险 |  |  | [blog](https://mp.weixin.qq.com/s/iEwvkqMT7KEqmnHk8NVz6w) | 
| 一种新的大语言模型NLG评估框架 | [G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634) |  |  | 
| 低代码LLM: LLM的可视化编程 | [Low-code LLM: Visual Programming over LLMs](https://arxiv.org/abs/2304.08103) |  |  | 
| 微软提出LLMA:大型语言模型的无损加速,可以无损地加速带有引用的大型语言模型 (LLM) 推理 | [Inference with Reference: Lossless Acceleration of Large Language Models](https://arxiv.org/pdf/2304.04487.pdf) |  |  | 
| ART：大型语言模型的自动多步骤推理和工具使用 | [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/pdf/2303.09014.pdf) |  |  | 
| 提出Pythia： 跨越训练和扩展的大型语言模型分析套件 | [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/pdf/2304.01373.pdf) | [code](https://github.com/EleutherAI/pythia) |  | 
| ChatGPT的底层逻辑 |  |  | [blog](https://mp.weixin.qq.com/s/Rv5htsD2x7TmD-E42RL6Vg) | 
| 智慧信息的压缩：模型智能的涌现之道 |  |  | [blog](https://mp.weixin.qq.com/s/hQmvltuMlClBonM6UJmtLg) | 
| 拨动大模型的琴弦｜Delta Tuning 成果登上 Nature子刊封面！ |  |  | [blog](https://mp.weixin.qq.com/s/m3fNselWKQ2m5XnBe79fQQ) | 
| 大型人工智能模型中出现的不可预测的能力 |  |  | [blog](https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316) | 
| 为什么现在的大语言模型（LLM）都是Decoder-only的架构？ |  |  | [blog](https://mp.weixin.qq.com/s/ZsHX-M9pisUvG9vqfzdzTQ) | 
| 大型语言模型的涌现能力 |  |  | [blog](https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/) | 
| 大型语言模型成本分析 |  |  | [blog](https://hub.baai.ac.cn/view/24047) | 
| 超越ChatGPT：大模型的智能极限 |  |  | [blog](https://yaofu.notion.site/e1cd16d1fae84f87aeddf872c838e07c) | 
| Nature：AI模型越大越好吗? |  |  | [blog](https://www.nature.com/articles/d41586-023-00641-w) | 
| 一场关于ChatGPT话语权的深度思考：人类会在大模型中迷失自我吗？ |  |  | [blog](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html)，[blog译文](https://mp.weixin.qq.com/s/RPiIh5cbxzXl5uMo_BVFMg) | 
| 马斯克强调的TruthGPT 是什么 |  |  | [blog](https://mp.weixin.qq.com/s/_nSYK63DvqE7ZJyJz6NeEA) | 
| 对话式AI搜索的技术路线猜想 |  |  | [blog](https://mp.weixin.qq.com/s/AIIu4rRi1WZRQn3oHtuwdg) | 
| AI走过多少路，才迎来了ChatGPT？ |  |  | [blog](https://mp.weixin.qq.com/s/WWc39HtuV-TrbwFybX112Q) | 
| 如何负责任地创建、发布和共享生成式 AI |  |  | [blog](https://www.technologyreview.com/2023/02/27/1069166/how-to-create-release-and-share-generative-ai-responsibly/) | 
| 大模型时代的“Linux”生态，开启人工智能新十年 |  |  | [blog](https://mp.weixin.qq.com/s/sUmA3nSSVfNQFBgSjiSn0g) | 
| 揭秘ChatGPT背后的AI“梦之队”：90后科研“后浪”展示强大创新能力｜智谱研究报告 |  |  | [blog](https://mp.weixin.qq.com/s/sncE01utzu_-r3dLFYU5QA) | 
| In-Context Learning玩法大全 |  |  | [blog](https://mp.weixin.qq.com/s/sC3Xq1QQmtC8Tz84oRRwcw) | 
| 一文理解“上下文学习”----大语言模型突现能力 |  |  | [blog](https://mp.weixin.qq.com/s/0kchPu20nwCKCXk4PZBkOg) | 
| 回应吴军老师 "ChatGPT不算新技术革命" |  |  | [blog](https://mp.weixin.qq.com/s/dZldwGaYnUcDlB4nUpASMg) | 
| Poe向所有开发者推出Poe API，以便广泛获取基于LLM的服务 |  | [code](https://github.com/poe-platform/api-bot-tutorial) |  | 
| 【LLM系列之底座模型对比】LLaMA、Palm、GLM、BLOOM、GPT模型结构对比 |  |  | [blog](https://mp.weixin.qq.com/s/UkifGP2OXxGWeMV7Jm4zWQ) | 
| 大模型实践总结 |  |  | [blog](https://mp.weixin.qq.com/s/FPweLbvDrCnIzb5PETHMLQ) | 
| 【LLM系列之GPT】GPT（Generative Pre-trained Transformer）生成式预训练模型 |  |  | [blog](https://mp.weixin.qq.com/s/1Bpt5MG6mbZCYAXDJmIr3A) | 
| 【LLM系列之Tokenizer】如何科学地训练一个LLM分词器 |  |  | [blog](https://mp.weixin.qq.com/s/4_P2G2Q0YmunQh7DwDas3w) | 
| 大模型词表扩充必备工具SentencePiece |  |  | [blog](https://mp.weixin.qq.com/s/qQMZ1s7lt-LLkQKx7HIDMw) | 
| 大模型知识&推理评估基准 |  |  | [blog](https://mp.weixin.qq.com/s/P0ohd5DpwJOkL8DFVC4qoA) | 
| 万字长文说清大模型在自动驾驶领域的应用 |  |  | [blog](https://mp.weixin.qq.com/s/5tSwRz-fI4ccLPEn2KrgqA) | 
| 一文速览大语言模型在推荐系统中的应用 |  |  | [blog](https://mp.weixin.qq.com/s/RdRLKjzbTWCATmtRMfxW0Q) | 
| NAACL & ACL：大模型的两种知识继承方案 |  |  | [方案一](https://aclanthology.org/2022.naacl-main.288/)，[方案二](https://aclanthology.org/2022.acl-long.151/) | 
| a16Z：大模型应用程序的新兴架构 |  |  | [中文blog](https://hub.baai.ac.cn/view/27506)，[英文blog](https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/) | 
| 如何优雅下载huggingface大模型文件？ |  |  | [blog](https://mp.weixin.qq.com/s/biNtwJRuWuQxiaklyEWVMg) | 
| LLM 时代指南 |  |  | [blog](https://mp.weixin.qq.com/s/4EvcEzMLfZ3VQTFt7rLA2Q) | 
| RetNet：MSRA提出Transformer全新替代大模型基础架构，推理速度8倍提升，内存占用减少70% | [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621) |  | [blog](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247686895&idx=2&sn=9a2763953d209a29e5d0b03e8b75a912&chksm=e8dead9ddfa9248bea848d16358c5a3eabf11cdd13b5aa96033f3ab2b6dc1ee089bedc73c332&token=1541731120&lang=zh_CN#rd)| 
| 大模型微调指南：当GPU资源不足时的有效解决方案 | [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf) |  |  | 
| TaskMatrix.AI | [TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs](https://arxiv.org/pdf/2303.16434.pdf) |  |  | 
| AnnoLLM | [AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators](https://arxiv.org/pdf/2303.16854.pdf) |  |  | 
| 南加州大学:大语言模型统计偏好的挑战和危险 | [Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models](https://arxiv.org/pdf/2304.03738.pdf) |  |  | 
| 卡内基·梅隆大学 语言生成模型可能造成危害：那么我们能做些什么呢？ | [Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey](https://arxiv.org/pdf/2210.07700.pdf) |  |  | 
| 鹏程实验室等最新《大规模多模态预训练模型》全面综述 | [Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey](https://arxiv.org/abs/2302.10035) |  |  | 
| 预训练基础模型综合调研：从 BERT 到 ChatGPT 的历史 | [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/abs/2302.09419) |  |  | 
| 洛桑联邦理工学院提出REFINER框架，用于微调大规模语言模型 | [REFINER: Reasoning Feedback on Intermediate Representations](https://arxiv.org/pdf/2304.01904.pdf) |  |  | 
| LLM-Adapters： 用于大型语言模型的参数高效微调的适配器系列 | [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf) |  |  | 
| 大型语言模型的涌现记忆和可预测记忆 | [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/abs/2304.11158) |  |  | 
| 机器心理学：使用心理学方法研究大型语言模型中的涌现能力和行为 | [Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods](https://arxiv.org/abs/2303.13988v1) |  |  | 
| Chameleon：使用大型语言模型进行即插即用的组合推理 | [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2304.09842) |  |  | 
| 大型语言模型相关文献资源列表 |  | [code](https://github.com/RUCAIBox/LLMSurvey) |  | 
| RRTF：通过反馈提高代码生成的能力 | [PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback](https://arxiv.org/abs/2307.14936.pdf) |  | [blog](https://mp.weixin.qq.com/s/3lgztkBGlfCdHwygDggBbw) | 
| Google发布统计深度学习框架平台：OpenXLA |  |  | [blog](https://github.com/wshzd/ChatGPT-Summary/blob/main/AGI/Google_OpenXLA.md) |  
| AGI的火花一作Sébastien Bubeck演讲万字全文 |  | | [blog](https://mp.weixin.qq.com/s/H1RVdH0fmwM0GjfV3uvd4g) |  
| AGI通用智能发展的思考：是否存在足够通用的处理器？ |  |  | [blog](https://mp.weixin.qq.com/s/16TfOu4qfqlbQHpDgDUM2A) |  
| OpenAGI:当大语言模型遇到领域专家 | [OpenAGI: When LLM Meets Domain Experts](https://arxiv.org/abs/2304.04370) | [code](https://github.com/agiresearch/OpenAGI) |  | 
| 垂直领域大模型的一些思考及开源模型汇总 |  |  | [blog](https://mp.weixin.qq.com/s/HiGkSwbGeo4sPZvQeKCJfQ) | 
| 大模型时代-行业落地的再思考 |  |  | [blog](https://mp.weixin.qq.com/s/wSQSjO_0OmIg2kBZUuXA4Q) | 
| 医疗领域大模型的幻觉问题分析 |  |  | [blog](https://mp.weixin.qq.com/s/1o4u0Em0fFk9YndTaF2I7A) | 
| 基于中文金融知识的 LLaMA 系微调模型的智能问答系统：LLaMA大模型训练微调推理等详细教学 |  |  | [blog](https://mp.weixin.qq.com/s/lrKPUcS9GkSS20-Jda-8bA) | 
| 中文多模态医学大模型智能分析X光片，实现影像诊断，完成医生问诊多轮对话 |  |  | [blog](https://mp.weixin.qq.com/s/Spb_dbsHRyP9EvUaMYgHxw) | 
| 伯克利&微软｜用GPT-4进行可控的文本-图像生成 | [paper](https://arxiv.org/abs/2305.18583) |  |  |

## 代码生成

| Description| Paper | Code | Blog |
| --- | --- | --- | --- | 
| 代码大模型综述：中科院和MSRA调研27个LLMs，并给出5个有趣挑战 | [paper](https://arxiv.org/abs/2212.09420) |  | [blog](https://mp.weixin.qq.com/s/t2SMftox6546E7kvRgQMnA)，[项目主页](https://nl2code.github.io) |
| 北京大学：具有大语言模型的自我规划代码生成 | [paper](https://arxiv.org/pdf/2303.06689.pdf) |  |  |
| 谷歌提出Self-Debugging:教导大型语言模型进行自我调试 | [paper](https://arxiv.org/pdf/2304.05128.pdf) |  |  |
| 通过自我改进实现更好的代码语言模型，显著提高模型生成任务的性能 | [paper](https://arxiv.org/pdf/2304.01228.pdf) |  |  |
| MIT最新研究：利用大预言模型生成Code | [paper](https://arxiv.org/abs/2303.05510) | [code](https://github.com/shunzh/Code-AI-Tree-Search) | [项目网址](https://codeaimcts.github.io/) |
| MathPrompter: 基于大型语言模型的数学推理 | [paper](https://arxiv.org/abs/2303.05398) |  |  |
| MIT最新研究：利用大语言模型生成Code | [paper](https://arxiv.org/abs/2303.05510) | [code](https://github.com/shunzh/Code-AI-Tree-Search) | [官网地址](https://codeaimcts.github.io/) |
| 一键控制10万多个AI模型，HuggingFace给类ChatGPT模型们做了个「APP Store」 |  | [demo](https://huggingface.co/docs/transformers/transformers_agents) | [blog](https://mp.weixin.qq.com/s/8gyTqT1B4C2Da_6dmtaNiw) |

## ChatGPT 应用篇

| Description| Paper | Code |Blog |
| --- | --- | --- | --- | 
| 从 GPT 到 ChatGPT 的演进与应用思考 |  | | [blog](https://mp.weixin.qq.com/s/3Pr82xKpZ7mAWQcxPPB1xA) | 
| 语言模型可以预测公众舆论 | [Language models trained on media diets can predict public opinion](https://arxiv.org/pdf/2303.16779.pdf) |  |  | 
| ChatGPT助力芯片，传统 EDA如何演变成智能EDA |  |  | [blog](https://mp.weixin.qq.com/s/JyveUDEYKLrFolfCFLqhhw) | 
| ChatGPT机器人:设计原则和模型能力 | [ChatGPT for Robotics: Design Principles and Model Abilities](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/) |  |  | 
| 各种环境下的ChatGPT赋能长步机器人控制： 一个案例的应用 | [ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application](https://arxiv.org/pdf/2304.03893.pdf) | [code](https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts) |  | 
| ChatGPT获得了「Wolfram」超能力 |  |  | [blog](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/) | 
| OpenAI开发Plugin将 ChatGPT 连接到互联网 |  |  | [blog](https://techcrunch.com/2023/03/23/openai-connects-chatgpt-to-the-internet/) | 
| ChatAug：利用ChatGPT进行文本数据增强 | [ChatAug: Leveraging ChatGPT for Text Data Augmentation](https://arxiv.org/pdf/2302.13007v1.pdf) |  |  | 
| ChatGPT 是数据隐私的另一个障碍吗 |  |  | [blog](https://www.bizcommunity.com/Article/196/639/236418.html) | 
| 基于ChatGPT的数据增强方法：ChatAug和AugGPT |  |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486140&idx=1&sn=bba4342966c99559938824f2d747d231&chksm=ced54958f9a2c04ec121b8c198d69a5a17c8b3e0a96a0cfcd8d1271bd6097a2cbf66895dd8a9&token=447941009&lang=zh_CN#rd) | 
| Character.AI 在ChatGPT基础上加入个性化、UGC两大武器，有比 ChatGPT 更丰富的使用场景 |  |  | [blog](https://mp.weixin.qq.com/s/U4R8loz1G9PYM_l6IvNF_A) | 
| 让ChatGPT可以**语音交互** |  |  | [blog](https://mp.weixin.qq.com/s/H4XLCQ-kR7T28yywHJL4uA) | 
| “ChatGPT们”的淘金时代 |  |  | [blog](https://mp.weixin.qq.com/s/otdenJh5FJsCgi5ONy9JIQ) | 
| 70 款 ChatGPT 插件评测（含样例分析） |  |  | [blog](https://mp.weixin.qq.com/s/vHwAk63ukRteF1u1myrTlA) | 
| 人大提出WebBrain：NLP新任务，通过网络数据的挖掘生成真实文章 | [WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus](https://arxiv.org/abs/2304.04358) | [code](https://github.com/qhjqhj00/WebBrain) |  | 
| ChatGPT爆火带来思考：医学界或将迎来与AI融合的奇点？ |  |  | [blog](https://mp.weixin.qq.com/s/x8ppg6GVCAeLNpv5uJ7B7g) | 
| 论ChatGPT大语言模型在教育中的机遇与挑战 |  |  | [blog](https://url39.ctfile.com/f/2501739-809898048-6394c7?p=2096) | 
| ChatGPT在投资研究领域的应用初探及原理分析 |  |  | [blog](https://mp.weixin.qq.com/s/LFPeSLeEOTb1-2YJBXclbQ) | 
| OpenAI总裁Greg Brockman转发｜一种编译语言的调试器，利用ChatGPT旨在增强您使用GDB进行调试体验 |  | [code](https://github.com/pgosar/ChatGDB) |  | 
| 不必排队等 OpenAI Plugins，OpenBMB 开源大模型工具学习引擎 |  |  | [blog](https://hub.baai.ac.cn/view/25189) | 
| 分析了ChatGPT技术以及落地应用场景 |  |  | [blog](https://url39.ctfile.com/f/2501739-805099789-098b62?p=2096) | 

## ChatGPT 工具篇

| Description| Paper | Code |Blog |
| --- | --- | --- | --- | 
| ChatGPT 应用汇总及操作手册 |  |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485794&idx=1&sn=6aa0500e3139b67246dd5f96007d1487&chksm=ced54a86f9a2c390d86856181f1fcd09091cf84d67e81535b6d592617f49fe24349779cfa1e5&token=447941009&lang=zh_CN#rd) | 
| ChatGPT提示和技巧速查手册 |  |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485766&idx=1&sn=43ad627e4e183d7a108c3c57ab0e02dc&chksm=ced54aa2f9a2c3b4a2d529e4ed7c2acc7fa32e7465837045d3ec607701e0da2a55c0c557cad2&token=447941009&lang=zh_CN#rd) | 
| 非常全面的ChatGPT、LLM相关资源整理分享 |  | [code](https://github.com/cedrickchee/chatgpt-universe) |  | 
| ChatGPT超全面课程 |  |  | [blog](https://tested-salto-cab.notion.site/The-Ultimate-Chat-GPT-Course-69ed24a317a942d288e740419b1ad6f6) | 
| BloombergGPT: A Large Language Model for Finance | [BloombergGPT: A Large Language Model for Finance](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=b0e4b03ecf5c11edb95839eec3084ddd) |  |  | 
| ChatPDF：一键上传PDF文件即可解读 |  |  | [blog](https://mp.weixin.qq.com/s/S1DUJrNK5_H5krvHotOwHQ)，[试用地址](https://www.chatpdf.com/) | 
| ChatWeb：可爬取网页正文，并根据正文回答问题 |  | [code](https://github.com/SkywalkerDarren/chatWeb) |  | 
| chatgpt_academic：中科院基于 ChatGPT 专属定制的学术研究及日常开发工具 | --- | [code](https://github.com/binary-husky/chatgpt_academic) | [blog](https://hub.baai.ac.cn/view/25298)，[demo](https://huggingface.co/spaces/qingxu98/gpt-academic) | 
| Einstein GPT：SaaS 行业巨头 Salesforce 宣布与 OpenAI 合作，推出 Einstein GPT，这是全球首个用于客户关系管理（CRM）的生成式 AI 产品 |  |  | [Einstein GPT地址](https://www.salesforce.com/products/einstein/overview/?d=cta-body-promo-8)，[试用地址](https://openai.com/waitlist/slack) | 
| HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace](https://arxiv.org/pdf/2303.17580.pdf) |  |  | 
| ImpressionGPT： 利用ChatGPT对放射科报告进行总结的迭代优化框架 | [ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT](https://arxiv.org/abs/2304.08448) |  |  | 
| OpenGpt：创建ChatGPT小应用的AI平台 |  | [code](https://github.com/futantan/OpenGpt) | [官网](https://open-gpt.app/) | 
| TagGPT：腾讯提出零样本多模态标签的大语言模型TagGPT | [TagGPT: Large Language Models are Zero-shot Multimodal Taggers](https://arxiv.org/abs/2304.03022) | [code](https://github.com/TencentARC/TagGPT) |  | 
| Visual ChatGPT: 在视觉模型加持下的ChatGPT，聊天生图全拿捏了。 | [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/pdf/2303.04671.pdf) |  |  | 
| NetGPT：用于网络流量的生成预训练Transformer模型 | [NetGPT: Generative Pretrained Transformer for Network Traffic](https://arxiv.org/pdf/2304.09513.pdf) |  |  | 

## ChatGPT 技术篇

| Description| Paper | Code |Blog |
| --- | --- | --- | --- | 
| ChatGPT_Inference_Cost | --- | --- | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/ChatGPT_Inference_Cost.md) | 
| ChatGPT_Official_API_Learning |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/ChatGPT_Official_API_Learning.md) | 
| ChatGPT_Parameter_is_not_175B |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/ChatGPT_Parameter_is_not_175B.md) | 
| ChatGPT_Road_Map_from_yao.fu |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/ChatGPT_Road_Map_from_yao.fu.md) | 
| Lessons_Learned_from_ChatGPT_Recurrence |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/Lessons_Learned_from_ChatGPT_Recurrence.md) | 
| LLM_Pre-training_Guide（Bloom-175B） |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/LLM_Pre-training_Guide（Bloom-175B）.md) | 
| The_guide_of_training_LLM |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT/Blog/ChatGPT_Technology/The_guide_of_training_LLM.md) | 
| 深度拆解GPT-3.5能力起源 |  |  | [原文blog](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)，[译文blog](https://mp.weixin.qq.com/s/ckd6KxeTfdQas_UCsJ7HgQ) | 
| ChatGPT发展历程、原理、技术架构详解和产业未来 |  |  | [blog](https://zhuanlan.zhihu.com/p/590655677) | 
| 让天下没有难训练的大模型，微软亚洲研究院开源TorchScale |  | [code](https://github.com/microsoft/torchscale) |  | 
| 82页PPT ！最新ChatGPT: 提示学习, 指导微调和RLHF |  |  | [blog](https://pan.baidu.com/s/15Bs1u7z1RhCdfiR3oJ_gJQ), [提取码:chat]| 
| 他们提出了包含视觉特征的 Multimodal-CoT，该架构在参数量小于 10 亿的情况下，在 ScienceQA 基准测试中，比 GPT-3.5 高出 16 个百分点 | [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) | [code](https://github.com/amazon-science/mm-cot) |  | 
| Nature ：生成式 AI 的前景与风险 |  |  | [blog](https://mp.weixin.qq.com/s/d6t2xpdvSDCHzO2gG1N6eQ) | 
| 万字长文解读：从Transformer到ChatGPT，通用人工智能曙光初现 |  |  | [blog](https://mp.weixin.qq.com/s/iZyKmWgXUkPv3Phyaw4Ppg) | 
| AI芯片制造商Cerebras发布7个基于GPT的大语言模型，现已开源 |  |  | [官网地址](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)，[GPT地址](https://www.cerebras.net/cerebras-gpt)，[Hugging Face地址](https://huggingface.co/cerebras) | 
| 大模型论文周报丨GPT-4发布，谷歌开放PaLM API，斯坦福7B开源模型Alpaca媲美GPT-3.5 |  |  | [blog](https://mp.weixin.qq.com/s/C6g_H6xfFn59IxnLpbjA1g) | 
| LLaMA模型Meta版泄露，GitHub获8K星 |  |  | [blog](https://mp.weixin.qq.com/s/2M19WSq2YICo-3t5ibQcig) | 
| ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark | [ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark](https://arxiv.org/abs/2303.13648) |  |  | 
| 打造中国版ChatGPT，国内哪家实力最强 |  |  | [blog](https://mp.weixin.qq.com/s/B-n_qz110HmhSP66NKRCiQ) | 
| 复旦大学邱锡鹏教授解读ChatGPT |  |  | [blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485810&idx=1&sn=47eb672c688517d6bade2c62c7eae94f&chksm=ced54a96f9a2c380ccacfbb223df52de64f2c410a91e726023a074fc98fb87fcd9f60f5a4957&token=447941009&lang=zh_CN#rd) | 
| 万字长文:可能是全网最晚的ChatGPT技术总结 |  |  | [blog](https://mp.weixin.qq.com/s/LJoxupaKflL793TCwnpyPg) | 
| ChatGPT作为知识库问答系统的问答能力评测 |  |  | [blog](https://mp.weixin.qq.com/s/xul2-SENnqxV8VehozDKHg) | 
| ChatGPT作者John Shulman：我们成功的秘密武器 |  |  | [blog](https://www.talkrl.com/episodes/john-schulman)，[blog译文](https://mp.weixin.qq.com/s/sDeBYMvAwbJr5_tj7Q20-w) | 
| ChatGPT 是数据隐私的另一个障碍吗 |  |  | [blog](https://www.bizcommunity.com/Article/196/639/236418.html) | 
| Hugging Face 每周速递: ChatGPT API 怎么用？我们帮你搭好页面了 |  |  | [blog](https://mp.weixin.qq.com/s/oeXgd78vFV8os2uTGZkFQQ) | 
| 复旦大学教授肖仰华：ChatGPT 浪潮下，面向大模型如何做数据治理？ |  |  | [blog](https://mp.weixin.qq.com/s/od24PYvFLUJe4NQxjvsbMw) | 
| 腾讯在ChatGPT的布局 |  |  | [blog](https://mp.weixin.qq.com/s/rdpGZII3pu3MHr-lFm3GyQ) | 
| 浅析ChatGPT：历史沿革、应用现状及前景展望 |  |  | [blog](https://mp.weixin.qq.com/s/fQ8DmL_M3QMiFX23Tf0z7w) | 
| ChatGPT 背后的“功臣”——人类反馈强化学习RLHF 技术详解 |  |  | [blog](https://mp.weixin.qq.com/s/mZdZS9QNda26Ae0OIhRjFA) | 
| 万字长文解析！复现和使用GPT-3/ChatGPT，你所应该知道的 |  |  | [blog](https://mp.weixin.qq.com/s/ILpbRRNP10Ef1z3lb2CqmA) | 
| 想训练ChatGPT？得先弄明白Reward Model怎么训（附源码） |  |  | [blog](https://mp.weixin.qq.com/s/1v4Uuc1YAZ9MRr1UWMH9xw) | 
| ChatGPT核心技术：强化学习PPO算法 |  |  | [blog](https://mp.weixin.qq.com/s/z4oc9xQmduKMolWxztdHjA) | 
| 解读 ChatGPT 背后的技术重点：RLHF、IFT、CoT、红蓝对抗 |  |  | [blog](https://mp.weixin.qq.com/s/y4ywidZ55BQLgQzJa_Wjbg) | 
| OpenAI ChatGPT Code Interpreter入门 |  |  | [blog](https://www.oneusefulthing.org/p/what-ai-can-do-with-a-toolbox-getting) | 
| 加拿大魁北克大学教授详述：我们该拿ChatGPT怎么办？ |  |  | [blog](https://lemire.me/blog/2023/04/03/what-are-we-going-to-do-about-chatgpt/) | 
| AIGC时代的ChatGPT全面综述 | [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://arxiv.org/abs/2304.06488) |  |  | 
| ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models |  [ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models](https://arxiv.org/pdf/2303.16421.pdf) |  |  | 
| GPT-3 和 GPT-3.5 系列模型的全面分析 | [A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models](https://arxiv.org/abs/2303.10420v1) |  |  | 
| ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks | [CHATGPT OUTPERFORMS CROWD-WORKERS FOR TEXT-ANNOTATION TASKS](https://arxiv.org/pdf/2303.15056.pdf) |  |  | 
| AdaLoRA：自适应预算分配以实现参数有效的微调 | [ADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENT FINE-TUNING](https://arxiv.org/pdf/2303.10512.pdf) | [code](https://github.com/QingruZhang/AdaLoRA) |  | 
| 大型语言模型的语境忠实提示法 | [Context-faithful Prompting for Large Language Models](https://arxiv.org/pdf/2303.11315.pdf) |  |  | 
| ChatGPT问，BLIP-2回答模型：面向丰富的视觉描述的自动提问 | [ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions](https://arxiv.org/pdf/2303.06594.pdf) | [code](https://github.com/Vision-CAIR/ChatCaptioner) |  | 
| ChatGPT真的可以取代知识图谱问答吗？ | [Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of GPT family LLMs' Question Answering Performance](https://arxiv.org/abs/2303.07992)，[paper翻译](https://mp.weixin.qq.com/s/cvBVgxCrreic6U6CU-YB-A) |  |  | 
| Meta & 斯坦福大学推出FlexGen：用单个GPU进行大型语言模型的高吞吐量生成性推理 | [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865.pdf) | [code](https://github.com/FMInference/FlexGen) |  | 
| ChatGPT破圈的「秘密武器」：详解RLHF如何影响人类社会！ | [Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2303.02891) |  | [blog](https://mp.weixin.qq.com/s/DCFhefWGQS5naYwT3o6neg) | 
| 探讨ChatGPT在对抗攻击和分布外泛化下的鲁棒性 | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/pdf/2302.12095.pdf) | [code](https://github.com/microsoft/robustlearn) |  | 
| 复旦清华联合顶刊发文｜ChatGPT：潜力、前景和局限 | [ChatGPT: potential, prospects, and limitations](https://link.springer.com/article/10.1631/FITEE.2300089) |  | [blog](https://mp.weixin.qq.com/s/1D62QuxXFDXWwwRXrB-Ivw) | 
| 引导ChatGPT不要输出有害信息 | [The Capacity for Moral Self-Correction in Large Language Models](https://arxiv.org/pdf/2302.07459.pdf) |  |  | 
| Junnan Li大佬发表最新多模态的杰作BLIP2 | [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | [code](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) | [blog](https://mp.weixin.qq.com/s/xmSy4m7NheY8iComv7grxQ) | 
| Instruction Tuning：无/少样本学习新范式 | [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652) | [code](https://github.com/google-research/flan) |  | 
| GPTScore：一种新的评估语言模型方法 | [GPTScore: Evaluate as You Desire](https://arxiv.org/abs/2302.04166) | [code](https://github.com/jinlanfu/GPTScore) |  | 
| ChatGPT内核：InstructGPT，基于反馈指令的PPO强化学习 |  |  | [blog](https://zhuanlan.zhihu.com/p/589747432)，[video](https://www.bilibili.com/video/BV1hd4y187CR) | 
| Fine-tune-CoT：小模型也能做推理，完美逆袭大模型 | [Large Language Models Are Reasoning Teachers](https://arxiv.org/pdf/2212.10071.pdf) | [code](https://github.com/itsnamgyu/reasoning-teacher) |  | 
| ChatGPT的潜力解锁：自然语言处理中应用、优势、限制和未来方向的全面探索 | [UNLOCKING THE POTENTIAL OF CHATGPT: A COMPREHENSIVE EXPLORATION OF ITS APPLICATIONS, ADVANTAGES, LIMITATIONS, AND FUTURE DIRECTIONS IN NATURAL LANGUAGE PROCESSING](https://arxiv.org/pdf/2304.02017.pdf) |  |  | 
| 阿里巴巴&清华大学|大型语言模型在算术任务中的表现如何？ | [How well do Large Language Models perform in Arithmetic tasks?](https://arxiv.org/pdf/2304.02015.pdf) | [code](https://github.com/GanjinZero/math401-llm) |  | 
| 本科生60行代码教你手搓GPT大模型 |  | [code](https://github.com/jaymody/picoGPT/tree/29e78cc52b58ed2c1c483ffea2eb46ff6bdec785) |  | 

## GPT4

| Description| Paper | Code | Blog |
| --- | --- |--- | --- |
| GPT4_System_Card中文翻译 |  |  |[blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Official/GPT-4_System_Card_zh.md) |
| GPT4_Technical_Report中文翻译 |  |  |[blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Official/GPT4_Technical_Report_zh.md) |
| 【万字长文】GPT-4秘密泄露！所有的信息都在这里！从GPT-4 架构、基础设施、训练数据集、成本、视觉到MoE！ |  |  | [blog](https://mp.weixin.qq.com/s/vgUKe31pykC12sUV5xyLNQ)，[原blog](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure) |
| GPT-4 令人印象深刻但仍在 10 个方面具有缺陷 |  |  | [blog](https://www.nytimes.com/2023/03/14/technology/openai-new-gpt4.html) |
| 多模态大模型GPT-4的新突破 |  |  | [blog](https://hub.baai.ac.cn/view/24852) |
| 重磅发布GPT-4 |  |  | [blog](https://openai.com/research/gpt-4) |
| GPT-4 创造者 Ilya Sutskever 谈 AI 幻觉和 AI 民主 |  |  | [blog](https://www.forbes.com/sites/craigsmith/2023/03/15/gpt-4-creator-ilya-sutskever-on-ai-hallucinations-and-ai-democracy/?sh=7743f01e1218) |
| GPT-4创造者：第二次改变AI浪潮的方向 |  |  | [blog](https://mp.weixin.qq.com/s/rZBEDlxFVsVXoL5YUVU3XQ) |
| 当GPT-4进入北京市2022高考考场能有什么表现？ |  |  | [blog](https://mp.weixin.qq.com/s/N_j01KSuEKuVwCCD69G92g) |
| GPT4技术细节 |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Blog/GPT4_Technical_Detail.md) |
| GPT4技术关键点总结 |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/GPT4/Blog/GPT4_Technical_Summary.md) |
| GPT4和ChatGPT的效果对比 |  |  | [blog](https://github.com/ArronAI007/ChatGPT-Summary/blob/main/ChatGPT_VS_GPT4/GPT4_VS_ChatGPT（from_nytimes）.md) |
| The Ultimate GPT-4 Guide |  |  | [blog](https://doc.clickup.com/37456139/d/h/13q28b-324/e2a22b0c164b1f9) |
| GPT-4里套娃LLaMA 2！OpenAI创始成员周末爆改「羊驼宝宝」，GitHub一日千星 |  |  | [blog](https://mp.weixin.qq.com/s/Tp4q8VflEZ7o8FgpZfrNgQ) |
| Claude 2 解读 ChatGPT 4 的技术秘密：细节：参数数量、架构、基础设施、训练数据集、成本 |  |  | [blog](https://mp.weixin.qq.com/s/E2KpvldbYrH0NFvxgfsMlw) |
| 用GPT-4进行指令调优 | [INSTRUCTION TUNING WITH GPT-4](https://arxiv.org/pdf/2304.03277.pdf) | [code](https://instruction-tuning-with-gpt-4.github.io/) |  |
| 点燃通用人工智能的火花：GPT-4的早期实验 | [原始paper](https://arxiv.org/pdf/2303.12712.pdf)，[中文版paper](https://event-cdn.baai.ac.cn/file/file-browser/waTXJn85fm3FPyDXpsZ4faGk47trjjYb.pdf) |  |  [blog](https://mp.weixin.qq.com/s/H1RVdH0fmwM0GjfV3uvd4g) |
| GPT4All：用GPT-3.5-Turbo的大规模数据提炼训练一个助理式聊天机器人 | [GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf) | [code](https://github.com/nomic-ai/gpt4all) |  |
| 美国东北大学：可以通过要求GPT4反思“你为什么错了？”来提高30%的性能 | [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366) | [code](https://github.com/noahshinn024/reflexion) |  |
| 对ChatGPT/GPT-4研究的总结以及对大型语言模型未来的展望 | [Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models](https://arxiv.org/pdf/2304.01852.pdf) |  |  |
| 评估日本医疗执照考试的GPT-4和ChatGPT | [Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations](https://arxiv.org/pdf/2303.18027.pdf) |  |  |
| 深入研究LLMs与AutoGPT的结合：揭示出GPT-4惊人的人类决策能力！ | [Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions](https://arxiv.org/pdf/2306.02224.pdf) | [code](https://github.com/younghuman/LLMAgent) | [blog](https://mp.weixin.qq.com/s/Gbz7ZVVdeTq64mj1-__aQA) |






