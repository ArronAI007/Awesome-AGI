## AGI-Paper-List

| Description| Paper | Code | Blog |
| --- | --- | --- | --- |  
| 中文大语言模型汇总：医疗、法律、金融、教育、数学微调， 目前已1.1K星 |  | [code](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM) |  | 
| 大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写 | [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) |  |  | 
| 大语言模型综述全新出炉：51页论文带你盘点LLM领域专业化技术 | [Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey](https://arxiv.org/abs/2305.18703) |  | [blog](https://mp.weixin.qq.com/s/0DrowrTIgXsBhj3sYu6Aog) | 
| AIGC综述: 从GAN到ChatGPT的生成式人工智能简史 | [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/abs/2303.04226v1) |  |  | 
| 大模型综述来了！一文带你理清全球AI巨头的大模型进化史 | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/pdf/2304.13712.pdf) | [code](https://github.com/Mooler0410/LLMsPracticalGuide) |  | 
| 复旦大学教授肖仰华：ChatGPT 浪潮下，面向大模型如何做数据治理？ |  |  | [blog](https://mp.weixin.qq.com/s/od24PYvFLUJe4NQxjvsbMw) | 
| 面向决策的基础模型: 问题、方法与机会 | [Foundation Models for Decision Making: Problems, Methods, and Opportunities](https://arxiv.org/abs/2303.04129) |  |  | 
| 较大语言模型上下文学习的方式有所不同 | [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846) |  |  | 
| 通用语音识别大模型已经支持100+语言 |  |  | [blog](https://mp.weixin.qq.com/s/fHr2vL-w4JtYt5utcZrbsw) | 
| 发布5620亿参数多模态模型PaLM-E，机器人操控再上台阶 | [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) |  | [blog](https://palm-e.github.io/)，[twitter](https://twitter.com/DannyDriess/status/1632904675124035585)，[video](https://mp.weixin.qq.com/s/yZt3sEQPzVjnIvqXsNOnPA) | 
| PanGu-Σ: 稀疏异构计算万亿参数语言模型研究参数语言模型 | [PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing](https://arxiv.org/abs/2303.10845) |  |  | 
| 奖励聊天机器人在现实世界中与数以百万计的用户进行互动 | [Rewarding Chatbots for Real-World Engagement with Millions of Users](https://arxiv.org/pdf/2303.06135.pdf) |  |  | 
| 人工智能系统最终是否需要以现实为基础，而不仅仅是从语言中学习？ |  |  | [blog](https://spectrum.ieee.org/ai-hallucination) | 
| 大型语言模型是否需要感官基础来理解意义和理解？ |  |  | [slices](https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view) | 
| ChatGPT是「外星人」，所以才会胡说八道 | [AI Chatbots Don’t Care About Your Social Norms](https://www.noemamag.com/ai-chatbots-dont-care-about-your-social-norms/?utm_source=noematwitter&utm_medium=noemasocial) |  | [blog](https://twitter.com/ylecun/status/1633459264508542978) | 
| AI聊天机器人并不关注用户的社交属性 |  |  | [blog](https://www.noemamag.com/ai-chatbots-dont-care-about-your-social-norms/?utm_source=noematwitter&utm_medium=noemasocial) | 
| LeCun和马库斯齐喷ChatGPT：大语言模型果然是邪路？ |  |  | [blog](https://mp.weixin.qq.com/s/5e0aTSEAym9rF5QxRndLgQ) | 
| ChatGPT无法实现通用人工智能，但ALM技术路线也许可以 |  |  | [blog](https://mp.weixin.qq.com/s/MEdl3zmiYJU1iFsTXmibng) | 
| 「增强语言模型」的综述 | [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842) |  |  | 
| 自回归LLM的缺陷之一，大语言模型必须知道的8个要点 | [Eight Things to Know about Large Language Models](https://cims.nyu.edu/~sbowman/eightthings.pdf) |  |  | 
| 从词模型到世界模型：从自然语言到思维概率语言的转变 | [From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought](https://arxiv.org/abs/2306.12672) | |  | 
| AI进入2.0时代，所有应用都会被重写一遍 |  |  | [blog](https://mp.weixin.qq.com/s/zV8Y9RQnIoExwa1mmarZmA) | 
| 提出ILF（从语言反馈中模仿学习）：利用语言反馈大规模训练语言模型 | [Training Language Models with Language Feedback at Scale](https://arxiv.org/pdf/2303.16755.pdf) |  |  | 
| GPT就是GPT：大模型对劳动力市场影响潜力的早期研究 | [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/pdf/2303.10130.pdf) |  |  | 
| ABC News 专访OpenAI首席执行官萨姆·奥尔特曼：AI风险和重塑社会的问题 |  |  | [blog](https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122) | 
| 最新发布通用人工智能路线图！AGI比想象中来得更快！ |  |  | [blog](https://openai.com/blog/planning-for-agi-and-beyond/) | 
| Sam Altman 担心“潜在的可怕的”人工智能工具以及“未来的人们如何看待我们” |  |  | [blog](https://finance.yahoo.com/news/openai-ceo-sam-altman-frets-165250285.html) | 
| The Age of AI：拾象大模型及OpenAI投资思考 |  |  | [blog](https://mp.weixin.qq.com/s/AxX-Q7njegNTAxMkYFwsfA) | 
| 为什么ChatGPT用强化学习而非监督学习？ | [Scaling TransNormer to 175 Billion Parameters](https://arxiv.org/abs/2307.14995) | [code](https://github.com/OpenNLPLab/TransnormerLLM) | [blog](https://mp.weixin.qq.com/s/4USDakdomupWuwwhex6fMg) | 
| ChatGPT和生成式AI的11大安全趋势 |  |  | [blog](https://mp.weixin.qq.com/s/_RAx3vAx1ykQTJTEEoc37w) | 
| 分析过688篇大模型论文，这篇论文综述了LLM的当前挑战和应用 |  |  | [blog](https://mp.weixin.qq.com/s/drE6lhuhF9CbuAzDOhswTQ) | 
| 张学工教授：AI技术前沿——从ChatGPT到更多突破 |  |  | [blog](https://mp.weixin.qq.com/s/oeZd52BYKU3hhauZZ0eirQ) | 
| 研究大语言模型反映了谁的观点？ | [Whose Opinions Do Language Models Reflect?](https://arxiv.org/pdf/2303.17548.pdf) | [code](https://github.com/tatsu-lab/opinions_qa) |  | 
| 大模型及其公平使用 | [FOUNDATION MODELS AND FAIR USE](https://arxiv.org/pdf/2303.15715.pdf) |  |  | 
| 构建大模型生态系统图，用于跟踪大模型的足迹 |  |  | [blog](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=home) | 
| 斯坦福报告：基础模型的机遇与风险 |  |  | [blog](https://mp.weixin.qq.com/s/iEwvkqMT7KEqmnHk8NVz6w) | 
| 一种新的大语言模型NLG评估框架 | [G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634) |  |  | 
| 低代码LLM: LLM的可视化编程 | [Low-code LLM: Visual Programming over LLMs](https://arxiv.org/abs/2304.08103) |  |  | 
| 微软提出LLMA:大型语言模型的无损加速,可以无损地加速带有引用的大型语言模型 (LLM) 推理 | [Inference with Reference: Lossless Acceleration of Large Language Models](https://arxiv.org/pdf/2304.04487.pdf) |  |  | 
| ART：大型语言模型的自动多步骤推理和工具使用 | [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/pdf/2303.09014.pdf) |  |  | 
| 提出Pythia： 跨越训练和扩展的大型语言模型分析套件 | [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/pdf/2304.01373.pdf) | [code](https://github.com/EleutherAI/pythia) |  | 
| ChatGPT的底层逻辑 |  |  | [blog](https://mp.weixin.qq.com/s/Rv5htsD2x7TmD-E42RL6Vg) | 
| 智慧信息的压缩：模型智能的涌现之道 |  |  | [blog](https://mp.weixin.qq.com/s/hQmvltuMlClBonM6UJmtLg) | 
| 拨动大模型的琴弦｜Delta Tuning 成果登上 Nature子刊封面！ |  |  | [blog](https://mp.weixin.qq.com/s/m3fNselWKQ2m5XnBe79fQQ) | 
| 大型人工智能模型中出现的不可预测的能力 |  |  | [blog](https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316) | 
| 为什么现在的大语言模型（LLM）都是Decoder-only的架构？ |  |  | [blog](https://mp.weixin.qq.com/s/ZsHX-M9pisUvG9vqfzdzTQ) | 
| 大型语言模型的涌现能力 |  |  | [blog](https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/) | 
| 大型语言模型成本分析 |  |  | [blog](https://hub.baai.ac.cn/view/24047) | 
| 超越ChatGPT：大模型的智能极限 |  |  | [blog](https://yaofu.notion.site/e1cd16d1fae84f87aeddf872c838e07c) | 
| Nature：AI模型越大越好吗? |  |  | [blog](https://www.nature.com/articles/d41586-023-00641-w) | 
| 一场关于ChatGPT话语权的深度思考：人类会在大模型中迷失自我吗？ |  |  | [blog](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html)，[blog译文](https://mp.weixin.qq.com/s/RPiIh5cbxzXl5uMo_BVFMg) | 
| 马斯克强调的TruthGPT 是什么 |  |  | [blog](https://mp.weixin.qq.com/s/_nSYK63DvqE7ZJyJz6NeEA) | 
| 对话式AI搜索的技术路线猜想 |  |  | [blog](https://mp.weixin.qq.com/s/AIIu4rRi1WZRQn3oHtuwdg) | 
| AI走过多少路，才迎来了ChatGPT？ |  |  | [blog](https://mp.weixin.qq.com/s/WWc39HtuV-TrbwFybX112Q) | 
| 如何负责任地创建、发布和共享生成式 AI |  |  | [blog](https://www.technologyreview.com/2023/02/27/1069166/how-to-create-release-and-share-generative-ai-responsibly/) | 
| 大模型时代的“Linux”生态，开启人工智能新十年 |  |  | [blog](https://mp.weixin.qq.com/s/sUmA3nSSVfNQFBgSjiSn0g) | 
| 揭秘ChatGPT背后的AI“梦之队”：90后科研“后浪”展示强大创新能力｜智谱研究报告 |  |  | [blog](https://mp.weixin.qq.com/s/sncE01utzu_-r3dLFYU5QA) | 
| In-Context Learning玩法大全 |  |  | [blog](https://mp.weixin.qq.com/s/sC3Xq1QQmtC8Tz84oRRwcw) | 
| 一文理解“上下文学习”----大语言模型突现能力 |  |  | [blog](https://mp.weixin.qq.com/s/0kchPu20nwCKCXk4PZBkOg) | 
| 回应吴军老师 "ChatGPT不算新技术革命" |  |  | [blog](https://mp.weixin.qq.com/s/dZldwGaYnUcDlB4nUpASMg) | 
| Poe向所有开发者推出Poe API，以便广泛获取基于LLM的服务 |  | [code](https://github.com/poe-platform/api-bot-tutorial) |  | 
| 【LLM系列之底座模型对比】LLaMA、Palm、GLM、BLOOM、GPT模型结构对比 |  |  | [blog](https://mp.weixin.qq.com/s/UkifGP2OXxGWeMV7Jm4zWQ) | 
| 大模型实践总结 |  |  | [blog](https://mp.weixin.qq.com/s/FPweLbvDrCnIzb5PETHMLQ) | 
| 【LLM系列之GPT】GPT（Generative Pre-trained Transformer）生成式预训练模型 |  |  | [blog](https://mp.weixin.qq.com/s/1Bpt5MG6mbZCYAXDJmIr3A) | 
| 【LLM系列之Tokenizer】如何科学地训练一个LLM分词器 |  |  | [blog](https://mp.weixin.qq.com/s/4_P2G2Q0YmunQh7DwDas3w) | 
| 大模型词表扩充必备工具SentencePiece |  |  | [blog](https://mp.weixin.qq.com/s/qQMZ1s7lt-LLkQKx7HIDMw) | 
| 大模型知识&推理评估基准 |  |  | [blog](https://mp.weixin.qq.com/s/P0ohd5DpwJOkL8DFVC4qoA) | 
| 万字长文说清大模型在自动驾驶领域的应用 |  |  | [blog](https://mp.weixin.qq.com/s/5tSwRz-fI4ccLPEn2KrgqA) | 
| 一文速览大语言模型在推荐系统中的应用 |  |  | [blog](https://mp.weixin.qq.com/s/RdRLKjzbTWCATmtRMfxW0Q) | 
| NAACL & ACL：大模型的两种知识继承方案 |  |  | [方案一](https://aclanthology.org/2022.naacl-main.288/)，[方案二](https://aclanthology.org/2022.acl-long.151/) | 
| a16Z：大模型应用程序的新兴架构 |  |  | [中文blog](https://hub.baai.ac.cn/view/27506)，[英文blog](https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/) | 
| 如何优雅下载huggingface大模型文件？ |  |  | [blog](https://mp.weixin.qq.com/s/biNtwJRuWuQxiaklyEWVMg) | 
| LLM 时代指南 |  |  | [blog](https://mp.weixin.qq.com/s/4EvcEzMLfZ3VQTFt7rLA2Q) | 
| RetNet：MSRA提出Transformer全新替代大模型基础架构，推理速度8倍提升，内存占用减少70% | [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621) |  | [blog](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247686895&idx=2&sn=9a2763953d209a29e5d0b03e8b75a912&chksm=e8dead9ddfa9248bea848d16358c5a3eabf11cdd13b5aa96033f3ab2b6dc1ee089bedc73c332&token=1541731120&lang=zh_CN#rd)| 
| 大模型微调指南：当GPU资源不足时的有效解决方案 | [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf) |  |  | 
| TaskMatrix.AI | [TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs](https://arxiv.org/pdf/2303.16434.pdf) |  |  | 
| AnnoLLM | [AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators](https://arxiv.org/pdf/2303.16854.pdf) |  |  | 
| 南加州大学:大语言模型统计偏好的挑战和危险 | [Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models](https://arxiv.org/pdf/2304.03738.pdf) |  |  | 
| 卡内基·梅隆大学 语言生成模型可能造成危害：那么我们能做些什么呢？ | [Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey](https://arxiv.org/pdf/2210.07700.pdf) |  |  | 
| 鹏程实验室等最新《大规模多模态预训练模型》全面综述 | [Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey](https://arxiv.org/abs/2302.10035) |  |  | 
| 预训练基础模型综合调研：从 BERT 到 ChatGPT 的历史 | [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/abs/2302.09419) |  |  | 
| 洛桑联邦理工学院提出REFINER框架，用于微调大规模语言模型 | [REFINER: Reasoning Feedback on Intermediate Representations](https://arxiv.org/pdf/2304.01904.pdf) |  |  | 
| LLM-Adapters： 用于大型语言模型的参数高效微调的适配器系列 | [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf) |  |  | 
| 大型语言模型的涌现记忆和可预测记忆 | [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/abs/2304.11158) |  |  | 
| 机器心理学：使用心理学方法研究大型语言模型中的涌现能力和行为 | [Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods](https://arxiv.org/abs/2303.13988v1) |  |  | 
| Chameleon：使用大型语言模型进行即插即用的组合推理 | [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2304.09842) |  |  | 
| 大型语言模型相关文献资源列表 |  | [code](https://github.com/RUCAIBox/LLMSurvey) |  | 
| RRTF：通过反馈提高代码生成的能力 | [PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback](https://arxiv.org/abs/2307.14936.pdf) |  | [blog](https://mp.weixin.qq.com/s/3lgztkBGlfCdHwygDggBbw) | 
| Google发布统计深度学习框架平台：OpenXLA |  |  | [blog](https://github.com/wshzd/ChatGPT-Summary/blob/main/AGI/Google_OpenXLA.md) |  
| AGI的火花一作Sébastien Bubeck演讲万字全文 |  | | [blog](https://mp.weixin.qq.com/s/H1RVdH0fmwM0GjfV3uvd4g) |  
| AGI通用智能发展的思考：是否存在足够通用的处理器？ |  |  | [blog](https://mp.weixin.qq.com/s/16TfOu4qfqlbQHpDgDUM2A) |  
| OpenAGI:当大语言模型遇到领域专家 | [OpenAGI: When LLM Meets Domain Experts](https://arxiv.org/abs/2304.04370) | [code](https://github.com/agiresearch/OpenAGI) |  |  

