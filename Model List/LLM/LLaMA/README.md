# LLaMA大模型

Llama开源家族：从Llama-1到Llama-3发展路线图

![Llama_family](https://i.postimg.cc/tg5SD6JN/download-image.webp)

| Target Model | Source Model | Optimization | Checkpoints | Paper/Blog | Params (B) | Context Length | Code | Tokens | Tokenizer | Vocab size | Position Embedding | Layer Normalization | Activation Function | Attention |
| --- | --- | --- | --- |--- | --- | --- |--- | --- | --- | --- | --- | --- | --- | --- | 
| LLaMA |  |  |  | [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)，[blog1](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247485822&idx=1&sn=b365d93a0a08769aef77f34069da1422&chksm=ced54a9af9a2c38cd5779284b5e9ae573846153e7dc00961dc163664a657d6a3fa5c8c14c7d2&token=447941009&lang=zh_CN#rd)，[blog2](https://mp.weixin.qq.com/s/fGNuTcYE8QI9_JKS9LcQ7w)，[详聊LLaMA大模型的技术细节](https://mp.weixin.qq.com/s/B9Ue0ihUGAFjT_X__R2u8Q) | 7，13，33，65 | 2048 | [LLaMA Code](https://github.com/facebookresearch/llama) | 1T/1.4T | BPE | 32k | RoPE | Pre RMS Norm | SwiGLU | MHA |
| LLaMA 2 |  |  | [[在 Hugging Face 上玩转LLaMA 2](https://mp.weixin.qq.com/s/UnzhBJjZfPXsaSu8gNnosw)] ，[[在Colab笔记本中微调自己的Llama 2模型](https://mp.weixin.qq.com/s/pnDJaOUh_xdNdqSBl53Arw)]，[[三步上手 LLaMA2](https://mp.weixin.qq.com/s/lkRg8-rw57wDNr7FrjOSOQ)]，[[使用 Transformers 量化 Meta AI LLaMA2 中文版大模型](https://mp.weixin.qq.com/s/DEgFNAB4gwWDlQOj7-2CEg)] | [[blog](https://mp.weixin.qq.com/s?__biz=Mzg3NDIyMzI0Mw==&mid=2247486800&idx=1&sn=9b629ca41b9f6b4feedad94363a17253&chksm=ced54eb4f9a2c7a2a5b20c182981b4323b18509f2ca8f482c2a8cdbb29bf570488bdcd280eb6&token=882149695&lang=zh_CN#rd)]，[[伯克利AI博士详解Llama 2的技术细节](https://mp.weixin.qq.com/s/Mee7sMq_bxLpIOOr91li9A)]，[[NLP社区对LLaMA2论文上半部分的讨论](https://mp.weixin.qq.com/s/SJNqjSCBX-k80_r3nmTiuA)]，[[NLP中文社区顶尖研究员们对LLaMA2论文下半部分的讨论](https://mp.weixin.qq.com/s/6k5ML3HtmvBTTCgHBZGycQ)]，[[3个最值得了解llama2开发库，助你快速搭建LLM应用](https://mp.weixin.qq.com/s/_3H6Y_NolUuxYxOo8Pl7fg)]，[[使用 Docker 快速上手中文版 LLaMA2 开源大模型](https://mp.weixin.qq.com/s/9cTNa_oya2Zj9YdDYodCvw)]，[[ Llama 2资料汇总](https://mp.weixin.qq.com/s/-01Dg9ZVfPYM4mZ4iKt8Cw) | 7，13，34（未开源），70 | 4096 | [LLaMA2 Code](https://github.com/facebookresearch/llama-recipes) | 2T | BPE |  |  |  | SwiGLU | GQA（仅限34、70） |
| Llama-2 Chat | Llama-2 | SFT/RLHF |  |  |  |  |  |  | BPE |  |  |  |  |  | 
| Llama 3 |  |  |  |  | 8、70、400（训练中） | 8k | tiktoken | 15T |  | 128k | RoPE | RMSNorm | SwiGLU | GQA | 
| Airoboros | LLaMA |  |  |  | 13B |  |  |  |  |  |  |  |  |  | 
| Alpaca | LLaMA 7B |  |  | [Alpaca blog](https://crfm.stanford.edu/2023/03/13/alpaca.html)，[Alpaca homepage](https://crfm.stanford.edu/alpaca) | 7，13 | 2048 | [Alpaca Code](https://github.com/tatsu-lab/stanford_alpaca) |
| Alpaca-Lora | LLaMA 7B |  |  |  | 7 |  | [Alpaca-Lora Code](https://github.com/tloen/alpaca-lora) |
| AlpaGasus | Alpaca |  |  | [AlpaGasus: Training A Better Alpaca with Fewer Data](https://arxiv.org/abs/2307.08701)，[blog](https://mp.weixin.qq.com/s/UroGj4rIa2nOw6DookpvCQ) |  |  | [AlpaGasus Code](https://lichang-chen.github.io/AlpaGasus/) |
| Alpaca-CoT | Alpaca |  |  | [官网](https://sota.jiqizhixin.com/project/alpaca-cot) |  |  |  [Alpaca-CoT Code](https://github.com/PhoebusSi/Alpaca-CoT)|
| Anima | guanaco-33B |  | [Anima-33B](https://huggingface.co/lyogavin/Anima33B) | [Anima Blog](https://zhuanlan.zhihu.com/p/638058537?utm_source=wechat_session&utm_medium=social&s_r=0) |  |  | [Anima Code](https://github.com/lyogavin/Anima) |
| Baize | LLaMA |  | [baize-lora-7B](https://huggingface.co/spaces/project-baize/baize-lora-7B) | [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://arxiv.org/abs/2304.01196)，[blog](https://mp.weixin.qq.com/s/zxElGfclNbBwTuDG4Qrxnw) | 7B | 512 | [Baize Code](https://github.com/project-baize/baize-chatbot) |
| BELLE | Alpaca |  |  |  | 7B |  | [BELLE Code](https://github.com/LianjiaTech/BELLE) |
| BiLLa | LLaMA |  |  | [BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability](https://mp.weixin.qq.com/s/8KDpDC6Fkb_61gFfkcT8TQ) | 7B |  | [BiLLa Code](https://github.com/Neutralzz/BiLLa) |
| CaMA | LLaMA |  |  |  |  |  | [CaMA Code](https://github.com/zjunlp/CaMA) |
| CAMEL | LLaMA |  |  | [blog](https://starmpcc.github.io/CAMEL/) |  |  | [CAMEL Code](https://github.com/starmpcc/CAMEL) |
| ChatLLaMA | LLaMA | RLHF |  |  |  |  | [ChatLLaMA Code](https://github.com/nebuly-ai/nebuly/tree/main/optimization/chatllama) |
| Chinese-LlaMA2 | Llama-2 | SFT |  |  |  |  | [Chinese-LlaMA2 Code](https://github.com/michael-wzhu/Chinese-LlaMA2) |
| Chinese-Llama-2 | Llama-2 | LoRA/FPFT |  |  |  |  | [Chinese-Llama-2 Code](https://github.com/longyuewangdcu/Chinese-Llama-2) |
| Chinese-Vicuna | LLaMA | LoRA |  |  |  |  | [Chinese-Vicuna Code](https://github.com/Facico/Chinese-Vicuna) |
| Chinese-LLaMA-Alpaca | LLaMA | 扩词表 |  | [EFFICIENT AND EFFECTIVE TEXT ENCODING FOR CHINESE LL AMA AND ALPACA](https://arxiv.org/pdf/2304.08177v1.pdf) |  |  | [Chinese-LLaMA-Alpaca Code](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |  |  |  |  |  |  |  | 
| ColossalChat | LLaMA | RLHF |  | [blog](https://syncedreview.com/2023/03/29/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline/) |  |  | [ColossalChat Code](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) |
| Code-Llama | Llama-2 | Code |  |  | 7、13、34、70 |  |  |  |  |  |  |  |  |  | 
| 草本（原华驼） |  LLaMA | SFT |  | [HuaTuo (华驼): Tuning LLaMA Model with Chinese Medical Knowledge](https://arxiv.org/pdf/2304.06975v1.pdf)，[blog1](https://mp.weixin.qq.com/s/TYpc_63qDlR6MwscxCKKhA)，[blog2](https://mp.weixin.qq.com/s/iuQANmwCS7AXQRik7HwQPg) |  |  | [草本（原华驼） Code](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) |
| ExpertLLaMA | LLaMA |  |  | [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688)，[ExpertLLaMA demo](https://huggingface.co/spaces/OFA-Sys/expertllama) | 7B |  | [ExpertLLaMA Code](https://github.com/OFA-Sys/ExpertLLaMA) |
| FreedomGPT | LLaMA |  |  | [FreedomGPT homepage](https://freedomgpt.com/) |  |  | [FreedomGPT Code](https://github.com/ohmplatform/FreedomGPT) |
| GoGPT/GoGPT2 |  |  | [GoGPT2-7B](https://huggingface.co/golaxy/gogpt2-7b) |  |  |  | [GoGPT code](https://github.com/yanqiangmiffy/GoGPT) |
| GPT-4-LLM | LLaMA |  |  |  | 7B |  |  |
| Guanaco | LLaMA |  |  |  |  |  |  |
| Koala | LLaMA-13B |  | [Koala model](https://drive.google.com/drive/folders/10f7wrlAFoPIy-TECHsx9DKIvbQYunCfl) | [中文blog](https://hub.baai.ac.cn/view/25284)，[英文blog](https://bair.berkeley.edu/blog/2023/04/03/koala/) | 13B | 512 | [Koala Chat](https://chat.lmsys.org/?model=koala-13b)，[EasyLM Koala](https://github.com/young-geng/EasyLM)，[training data preprocessing](https://github.com/young-geng/koala_data_pipeline) | ChatGPT Distillation Data:[ShareGPT](https://sharegpt.com/)，[HC3](https://arxiv.org/abs/2301.07597)； Open Source Data:[OIG](https://laion.ai/blog/oig-dataset/)，[Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)，[Anthropic HH](https://huggingface.co/datasets/Anthropic/hh-rlhf)，[OpenAI WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)，[OpenAI Summarization](https://huggingface.co/datasets/openai/summarize_from_feedback)；[koala-test-set](https://github.com/arnav-gudibande/koala-test-set) |
| LLaMA-2 & Alpaca-2 | Llama-2，LLaMA&Alpaca | （1）扩充中文词表，（2）FlashAttention-2，（3）基于NTK的自适应上下文扩展技术，（4）简化的中英双语系统提示语 |  | [blog](https://mp.weixin.qq.com/s/sJ_imBdHCD4NibVy58EO2w) |  |  | [LLaMA-2 & Alpaca-2 Code](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) |
| LIMA | LLaMA-65B |  |  | [blog](https://mp.weixin.qq.com/s?search_click_id=7213828026277652651-1688375605291-1083947599&__biz=MjM5ODExNDA2MA==&mid=2449961473&idx=2&sn=f080fa7b1b5657db9872724caee56519&chksm=b13c7462864bfd741f0f061b87187f2cde36b68020cfe3402717a6858563311cb642eb340989&rd2werd=1&key=ea1d916ce49bb536ce48f3aba8e329d1e1aa6fdcda4f73580b0a5adbd624721e6a974570fd6ef2823ecfa6c95e2dc09179b51e440e9179f79d0ba01f62cf795d6c697f95bf05a28904f4172b11e1ce873a2d7a0e85c74d509e916176aacb43657fd11a6de7611d65bd4ae82315835aa138a423887a219f2971c6a525679fd805&ascene=65&uin=MTkwNzA5OTA4Mw%3D%3D&devicetype=iMac+MacBookPro13%2C2+OSX+OSX+12.6.7+build(21G651)&version=13080109&nettype=WIFI&lang=zh_CN&countrycode=CN&fontScale=100&exportkey=n_ChQIAhIQ0Z339%2BFUk%2Bp0YpfMQjB%2BhxKDAgIE97dBBAEAAAAAANJyNCKr%2F3UAAAAOpnltbLcz9gKNyK89dVj0q5AacL9r2sPbvlDuJo6SwYSJ2wbfYGvc3EDxuk%2BMQS0vl8RLluMN%2Fuh9u2LxBZTHTiuQct62Bjib68qd1EvB8CgGKMV34B5%2BKHCutInPzdE9Uac6dxp0VYtd%2BJnEwljL8jf7mWZdwTkPdEZl1P0OEb3HFzczXelqDR3h7D2xEVmQuFHGIeVi7iPOHMT0AWhhGLdbrVhCKbPT3%2BX9FPOLjJSql2UD95dTmSzZKqdvOIMGpD5t%2F98jDuMUojr9HUMdvljQ1XkiJVnd%2FbqSsLS3S5t7E%2Ftjmjb9g7IxWkY%3D&acctmode=0&pass_ticket=mJ3t3nBN%2BXhKCYp9bzJSkTl%2B9PwobzvYen%2F5Kv4kpcj1Lig98d0DXbcAyqBW0vaB&wx_header=0) |  |  |  |
| LongLLaMA |  |  | [LongLLaMA-3B](https://huggingface.co/syzymon/long_llama_3b) | [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/pdf/2307.03170.pdf)，[blog](https://mp.weixin.qq.com/s/K8ExTUUXDruZGwr-PA4oFQ) |  |  | [LongLLaMA Code](https://github.com/CStanKonrad/long_llama) |
| MetaMath | LLaMA2 | SFT | [MetaMath](https://huggingface.co/meta-math) | [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models](https://arxiv.org/abs/2309.12284)，[MetaMath主页](https://meta-math.github.io/) | --- | --- | [MetaMath Code](https://github.com/meta-math/MetaMath) | --- | --- | --- | --- | --- | --- | --- | 
| YuLan-Chat | LLaMA-13B |  |  |  |  |  |  |  |  |  |  |  |  |  |
| OpenBuddy-LLaMA1-30B | LLaMA |  | [ModelScope demo](https://modelscope.cn/models/OpenBuddy/openbuddy-llama-65b-v8-bf16/summary) | [blog](https://mp.weixin.qq.com/s/k-ZWg0Vuud3Atn3aaXBaCQ) | 13，33，65 |  |  |
| Platypus | LLaMA2 |  |  | [Platypus: Quick, Cheap, and Powerful Refinement of LLMs](https://arxiv.org/abs/2308.07317)，[blog](https://mp.weixin.qq.com/s/yzfgITUWaCf3Wcdc6lGCQA) |  |  |  |
| StackLLaMA | LLaMA | SFT/RM/RLHF |  | [中文blog](https://hub.baai.ac.cn/view/25341)，[英文blog](https://huggingface.co/blog/stackllama) |  |  |  |
| UltraLM | LLaMA |  |  |  | 13B |  |  |  |  |  |  |  |  |  | 
| Vicuna | Alpaca |  |  | [Vicuna官网地址](https://vicuna.lmsys.org/)，[blog](https://hub.baai.ac.cn/view/25328) | 7，13 | 2k | [Vicuna Code](https://github.com/lm-sys/FastChat) |
| Vicuna v1.5 | Llama 2 |  |  | [Vicuna官网地址](https://vicuna.lmsys.org/)，[blog](https://hub.baai.ac.cn/view/25328) | 7，13 | 4k，16k | [Vicuna v1.5 Code](https://github.com/lm-sys/FastChat) |
| WizardLM | LLaMA |  |  | |  |  | |  |  |  |  |  |  |  | 

