# ChatGLM大模型

| Model/Description| Paper | Code | Blog | Tokens |  Tokenizer | Vocab size | Position Embedding | Layer Normalization | Activation Function | Attention |
| --- | --- | --- | --- |--- | --- | --- |--- | --- | --- | --- | 
| ChatGLM-6B |  | [code](https://github.com/THUDM/ChatGLM-6B.git) | [blog](https://chatglm.cn/blog)，[ChatGLM-6B源码阅读](https://mp.weixin.qq.com/s/r7KEJmrpJZmY7KBP4veS6A)，[ChatGLM模型底座细节分析](https://mp.weixin.qq.com/s/oOdD3MYtE6-sNeAmPthqLg) | 1T | SentencePiece | 130528 | | Post Deep Norm | GeLU |
| chatglm+langchain+互联网 |  | [code](https://github.com/LemonQu-GIT/ChatGLM-6B-Engineering/) | [blog](https://mp.weixin.qq.com/s/lO6SrEuv4-vNbL8B3G-f8g) | 
| ChatGLM_multi_gpu_zero_Tuning |  | [code](https://github.com/CSHaitao/ChatGLM_mutli_gpu_tuning) |  | 
| ChatGLM+Fastapi |  |  | [blog](https://mp.weixin.qq.com/s/5J4UA4ePVZGXJGZsBXeN8Q) | 
| ChatGLM2-6B-32K |  |  | [blog](https://mp.weixin.qq.com/s/Fkm_D26z1jrqA44B82v7Ww) | 1.4T |  | 65024 | | Post RMS Norm | SwiGLU | GQA |
| ChatGLM-6b+langchain |  | [code](https://github.com/yanqiangmiffy/Chinese-LangChain) | [blog](https://mp.weixin.qq.com/s/xAsZZ_LOkr9Nj-JafSbXnA) | 
| one-shot微调chatglm-6b实践信息抽取 |  |  | [blog](https://mp.weixin.qq.com/s/l7lCbdJ9XGzLPTb3zKDAzQ) | 
| Falcon |  |  | [blog1](https://mp.weixin.qq.com/s/jbRRjG2ferhFPWsMtCaJyg)，[blog2](https://mp.weixin.qq.com/s/Vy_xWBuZU0AaaPMCIhKIyw) | 1.5T |  | 65024 | | Pre LN | GeLU | MQA |

